{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ieee-fraud-detection']\n",
      "/kaggle/input/ieee-fraud-detection/train_identity.csv\n",
      "/kaggle/input/ieee-fraud-detection/train_transaction.csv\n",
      "/kaggle/input/ieee-fraud-detection/test_transaction.csv\n",
      "/kaggle/input/ieee-fraud-detection/test_identity.csv\n",
      "/kaggle/input/ieee-fraud-detection/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Preprocessing, modelling and evaluating\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import gc\n",
    "print(os.listdir(\"../input\"))\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "## Utility functions:\n",
    "* Memory reduction (the original dataset is gigantic!)\n",
    "* PCA (dimension reduction - original # cols > 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to reduce the DF size\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def PCA_change(df, cols, n_components, prefix='PCA_', seed=123):\n",
    "    pca = PCA(n_components=n_components, random_state=seed)\n",
    "\n",
    "    principalComponents = pca.fit_transform(df[cols])\n",
    "\n",
    "    principalDf = pd.DataFrame(principalComponents)\n",
    "\n",
    "    df.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "    principalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n",
    "\n",
    "    df = pd.concat([df, principalDf], axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load transaction data (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv', index_col='TransactionID')\n",
    "df_test_trans = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv', index_col='TransactionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>...</th>\n",
       "      <th>V330</th>\n",
       "      <th>V331</th>\n",
       "      <th>V332</th>\n",
       "      <th>V333</th>\n",
       "      <th>V334</th>\n",
       "      <th>V335</th>\n",
       "      <th>V336</th>\n",
       "      <th>V337</th>\n",
       "      <th>V338</th>\n",
       "      <th>V339</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2987000</th>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987001</th>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987002</th>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987003</th>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987004</th>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 393 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               isFraud  TransactionDT  TransactionAmt ProductCD  card1  card2  \\\n",
       "TransactionID                                                                   \n",
       "2987000              0          86400            68.5         W  13926    NaN   \n",
       "2987001              0          86401            29.0         W   2755  404.0   \n",
       "2987002              0          86469            59.0         W   4663  490.0   \n",
       "2987003              0          86499            50.0         W  18132  567.0   \n",
       "2987004              0          86506            50.0         H   4497  514.0   \n",
       "\n",
       "               card3       card4  card5   card6  ...  V330  V331  V332  V333  \\\n",
       "TransactionID                                    ...                           \n",
       "2987000        150.0    discover  142.0  credit  ...   NaN   NaN   NaN   NaN   \n",
       "2987001        150.0  mastercard  102.0  credit  ...   NaN   NaN   NaN   NaN   \n",
       "2987002        150.0        visa  166.0   debit  ...   NaN   NaN   NaN   NaN   \n",
       "2987003        150.0  mastercard  117.0   debit  ...   NaN   NaN   NaN   NaN   \n",
       "2987004        150.0  mastercard  102.0  credit  ...   0.0   0.0   0.0   0.0   \n",
       "\n",
       "              V334 V335  V336  V337  V338  V339  \n",
       "TransactionID                                    \n",
       "2987000        NaN  NaN   NaN   NaN   NaN   NaN  \n",
       "2987001        NaN  NaN   NaN   NaN   NaN   NaN  \n",
       "2987002        NaN  NaN   NaN   NaN   NaN   NaN  \n",
       "2987003        NaN  NaN   NaN   NaN   NaN   NaN  \n",
       "2987004        0.0  0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 393 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 544.60 Mb (69.3% reduction)\n",
      "Mem. usage decreased to 474.52 Mb (68.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "### Reduce memory usage of dataset\n",
    "df_trans = reduce_mem_usage(df_trans)\n",
    "df_test_trans = reduce_mem_usage(df_test_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_trans['isFraud'] = 'test'\n",
    "df = pd.concat([df_trans, df_test_trans], axis=0, sort=False )\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransactionID int64 0 1097231\n",
      "isFraud object 0 3\n",
      "TransactionDT int32 0 1068035\n",
      "TransactionAmt float16 0 9131\n",
      "ProductCD object 0 5\n",
      "card1 int16 0 17091\n",
      "card2 float16 17587 501\n",
      "card3 float16 4567 133\n",
      "card4 object 4663 4\n",
      "card5 float16 8806 138\n",
      "card6 object 4578 4\n",
      "addr1 float16 131315 441\n",
      "addr2 float16 131315 93\n",
      "dist1 float16 643488 2479\n",
      "dist2 float16 1023168 2212\n",
      "P_emaildomain object 163648 60\n",
      "R_emaildomain object 824070 60\n",
      "C1 float16 3 1964\n",
      "C2 float16 3 1706\n",
      "C3 float16 3 32\n",
      "C4 float16 3 1495\n",
      "C5 float16 3 353\n",
      "C6 float16 3 1537\n",
      "C7 float16 3 1301\n",
      "C8 float16 3 1296\n",
      "C9 float16 3 360\n",
      "C10 float16 3 1278\n",
      "C11 float16 3 1677\n",
      "C12 float16 3 1373\n",
      "C13 float16 4748 1558\n",
      "C14 float16 3 1150\n",
      "D1 float16 7300 642\n",
      "D2 float16 515566 642\n",
      "D3 float16 466020 888\n",
      "D4 float16 245773 1069\n",
      "D5 float16 534216 961\n",
      "D6 float16 899261 1076\n",
      "D7 float16 998181 905\n",
      "D8 float16 947967 5828\n",
      "D9 float16 947967 24\n",
      "D10 float16 88567 1075\n",
      "D11 float16 455805 887\n",
      "D12 float16 963260 868\n",
      "D13 float16 911895 827\n",
      "D14 float16 919850 1036\n",
      "D15 float16 101182 1091\n",
      "M1 object 447739 2\n",
      "M2 object 447739 2\n",
      "M3 object 447739 2\n",
      "M4 object 519189 3\n",
      "M5 object 660114 2\n",
      "M6 object 328299 2\n",
      "M7 object 581283 2\n",
      "M8 object 581256 2\n",
      "M9 object 581256 2\n",
      "V1 float16 455805 2\n",
      "V2 float16 455805 12\n",
      "V3 float16 455805 12\n",
      "V4 float16 455805 10\n",
      "V5 float16 455805 11\n",
      "V6 float16 455805 14\n",
      "V7 float16 455805 14\n",
      "V8 float16 455805 12\n",
      "V9 float16 455805 12\n",
      "V10 float16 455805 6\n",
      "V11 float16 455805 8\n",
      "V12 float16 88662 5\n",
      "V13 float16 88662 7\n",
      "V14 float16 88662 2\n",
      "V15 float16 88662 13\n",
      "V16 float16 88662 23\n",
      "V17 float16 88662 16\n",
      "V18 float16 88662 16\n",
      "V19 float16 88662 13\n",
      "V20 float16 88662 23\n",
      "V21 float16 88662 6\n",
      "V22 float16 88662 9\n",
      "V23 float16 88662 16\n",
      "V24 float16 88662 26\n",
      "V25 float16 88662 13\n",
      "V26 float16 88662 23\n",
      "V27 float16 88662 8\n",
      "V28 float16 88662 8\n",
      "V29 float16 88662 6\n",
      "V30 float16 88662 9\n",
      "V31 float16 88662 13\n",
      "V32 float16 88662 23\n",
      "V33 float16 88662 13\n",
      "V34 float16 88662 23\n",
      "V35 float16 245823 5\n",
      "V36 float16 245823 7\n",
      "V37 float16 245823 55\n",
      "V38 float16 245823 55\n",
      "V39 float16 245823 22\n",
      "V40 float16 245823 25\n",
      "V41 float16 245823 2\n",
      "V42 float16 245823 9\n",
      "V43 float16 245823 12\n",
      "V44 float16 245823 49\n",
      "V45 float16 245823 70\n",
      "V46 float16 245823 9\n",
      "V47 float16 245823 13\n",
      "V48 float16 245823 6\n",
      "V49 float16 245823 8\n",
      "V50 float16 245823 8\n",
      "V51 float16 245823 9\n",
      "V52 float16 245823 12\n",
      "V53 float16 89995 6\n",
      "V54 float16 89995 8\n",
      "V55 float16 89995 50\n",
      "V56 float16 89995 52\n",
      "V57 float16 89995 7\n",
      "V58 float16 89995 11\n",
      "V59 float16 89995 17\n",
      "V60 float16 89995 18\n",
      "V61 float16 89995 7\n",
      "V62 float16 89995 11\n",
      "V63 float16 89995 9\n",
      "V64 float16 89995 11\n",
      "V65 float16 89995 2\n",
      "V66 float16 89995 9\n",
      "V67 float16 89995 11\n",
      "V68 float16 89995 8\n",
      "V69 float16 89995 6\n",
      "V70 float16 89995 9\n",
      "V71 float16 89995 7\n",
      "V72 float16 89995 11\n",
      "V73 float16 89995 9\n",
      "V74 float16 89995 11\n",
      "V75 float16 101245 6\n",
      "V76 float16 101245 8\n",
      "V77 float16 101245 81\n",
      "V78 float16 101245 81\n",
      "V79 float16 101245 8\n",
      "V80 float16 101245 20\n",
      "V81 float16 101245 20\n",
      "V82 float16 101245 8\n",
      "V83 float16 101245 8\n",
      "V84 float16 101245 11\n",
      "V85 float16 101245 11\n",
      "V86 float16 101245 81\n",
      "V87 float16 101245 81\n",
      "V88 float16 101245 2\n",
      "V89 float16 101245 8\n",
      "V90 float16 101245 6\n",
      "V91 float16 101245 9\n",
      "V92 float16 101245 8\n",
      "V93 float16 101245 8\n",
      "V94 float16 101245 3\n",
      "V95 float16 314 881\n",
      "V96 float16 314 1410\n",
      "V97 float16 314 976\n",
      "V98 float16 314 13\n",
      "V99 float16 314 89\n",
      "V100 float16 314 31\n",
      "V101 float16 314 870\n",
      "V102 float16 314 1285\n",
      "V103 float16 314 928\n",
      "V104 float16 314 60\n",
      "V105 float16 314 100\n",
      "V106 float16 314 81\n",
      "V107 float16 314 2\n",
      "V108 float16 314 9\n",
      "V109 float16 314 9\n",
      "V110 float16 314 9\n",
      "V111 float16 314 10\n",
      "V112 float16 314 10\n",
      "V113 float16 314 10\n",
      "V114 float16 314 10\n",
      "V115 float16 314 10\n",
      "V116 float16 314 10\n",
      "V117 float16 314 4\n",
      "V118 float16 314 4\n",
      "V119 float16 314 4\n",
      "V120 float16 314 5\n",
      "V121 float16 314 5\n",
      "V122 float16 314 5\n",
      "V123 float16 314 14\n",
      "V124 float16 314 14\n",
      "V125 float16 314 14\n",
      "V126 float32 314 15135\n",
      "V127 float32 314 33546\n",
      "V128 float32 314 20510\n",
      "V129 float16 314 2088\n",
      "V130 float32 314 12344\n",
      "V131 float32 314 5137\n",
      "V132 float32 314 9303\n",
      "V133 float32 314 13668\n",
      "V134 float32 314 11314\n",
      "V135 float32 314 6275\n",
      "V136 float32 314 8207\n",
      "V137 float32 314 7138\n",
      "V138 float16 939501 23\n",
      "V139 float16 939501 34\n",
      "V140 float16 939501 60\n",
      "V141 float16 939501 7\n",
      "V142 float16 939501 12\n",
      "V143 float16 939225 870\n",
      "V144 float16 939225 86\n",
      "V145 float16 939225 298\n",
      "V146 float16 939501 25\n",
      "V147 float16 939501 27\n",
      "V148 float16 939501 21\n",
      "V149 float16 939501 21\n",
      "V150 float16 939225 2634\n",
      "V151 float16 939225 58\n",
      "V152 float16 939225 44\n",
      "V153 float16 939501 19\n",
      "V154 float16 939501 19\n",
      "V155 float16 939501 25\n",
      "V156 float16 939501 25\n",
      "V157 float16 939501 25\n",
      "V158 float16 939501 25\n",
      "V159 float32 939225 8430\n",
      "V160 float32 939225 17182\n",
      "V161 float16 939501 105\n",
      "V162 float16 939501 289\n",
      "V163 float16 939501 153\n",
      "V164 float32 939225 2750\n",
      "V165 float32 939225 3799\n",
      "V166 float32 939225 2261\n",
      "V167 float16 820866 873\n",
      "V168 float16 820866 965\n",
      "V169 float16 821037 40\n",
      "V170 float16 821037 64\n",
      "V171 float16 821037 69\n",
      "V172 float16 820866 38\n",
      "V173 float16 820866 10\n",
      "V174 float16 821037 9\n",
      "V175 float16 821037 23\n",
      "V176 float16 820866 240\n",
      "V177 float16 820866 862\n",
      "V178 float16 820866 1236\n",
      "V179 float16 820866 921\n",
      "V180 float16 821037 180\n",
      "V181 float16 820866 86\n",
      "V182 float16 820866 126\n",
      "V183 float16 820866 107\n",
      "V184 float16 821037 17\n",
      "V185 float16 821037 32\n",
      "V186 float16 820866 40\n",
      "V187 float16 820866 215\n",
      "V188 float16 821037 45\n",
      "V189 float16 821037 45\n",
      "V190 float16 820866 225\n",
      "V191 float16 820866 22\n",
      "V192 float16 820866 45\n",
      "V193 float16 820866 38\n",
      "V194 float16 821037 8\n",
      "V195 float16 821037 19\n",
      "V196 float16 820866 42\n",
      "V197 float16 821037 15\n",
      "V198 float16 821037 29\n",
      "V199 float16 820866 225\n",
      "V200 float16 821037 48\n",
      "V201 float16 821037 56\n",
      "V202 float32 820866 18618\n",
      "V203 float32 820866 26290\n",
      "V204 float32 820866 22355\n",
      "V205 float16 820866 2870\n",
      "V206 float16 820866 2392\n",
      "V207 float32 820866 5305\n",
      "V208 float16 821037 2977\n",
      "V209 float16 821037 3820\n",
      "V210 float16 821037 3245\n",
      "V211 float32 820866 13031\n",
      "V212 float32 820866 15639\n",
      "V213 float32 820866 14582\n",
      "V214 float32 820866 3816\n",
      "V215 float32 820866 4664\n",
      "V216 float32 820866 4195\n",
      "V217 float16 840073 304\n",
      "V218 float16 840073 401\n",
      "V219 float16 840073 379\n",
      "V220 float16 818499 44\n",
      "V221 float16 818499 85\n",
      "V222 float16 818499 293\n",
      "V223 float16 840073 17\n",
      "V224 float16 840073 91\n",
      "V225 float16 840073 37\n",
      "V226 float16 840073 113\n",
      "V227 float16 818499 68\n",
      "V228 float16 840073 240\n",
      "V229 float16 840073 263\n",
      "V230 float16 840073 263\n",
      "V231 float16 840073 294\n",
      "V232 float16 840073 338\n",
      "V233 float16 840073 333\n",
      "V234 float16 818499 308\n",
      "V235 float16 840073 24\n",
      "V236 float16 840073 55\n",
      "V237 float16 840073 40\n",
      "V238 float16 818499 24\n",
      "V239 float16 818499 24\n",
      "V240 float16 840073 7\n",
      "V241 float16 840073 6\n",
      "V242 float16 840073 28\n",
      "V243 float16 840073 58\n",
      "V244 float16 840073 34\n",
      "V245 float16 818499 69\n",
      "V246 float16 840073 225\n",
      "V247 float16 840073 19\n",
      "V248 float16 840073 31\n",
      "V249 float16 840073 23\n",
      "V250 float16 818499 19\n",
      "V251 float16 818499 19\n",
      "V252 float16 840073 25\n",
      "V253 float16 840073 105\n",
      "V254 float16 840073 55\n",
      "V255 float16 818499 52\n",
      "V256 float16 818499 56\n",
      "V257 float16 840073 225\n",
      "V258 float16 840073 270\n",
      "V259 float16 818499 77\n",
      "V260 float16 840073 15\n",
      "V261 float16 840073 73\n",
      "V262 float16 840073 32\n",
      "V263 float32 840073 18231\n",
      "V264 float32 840073 23849\n",
      "V265 float32 840073 20861\n",
      "V266 float16 840073 2767\n",
      "V267 float16 840073 4136\n",
      "V268 float16 840073 3300\n",
      "V269 float16 840073 226\n",
      "V270 float16 818499 2859\n",
      "V271 float16 818499 3366\n",
      "V272 float16 818499 3011\n",
      "V273 float32 840073 11283\n",
      "V274 float32 840073 15111\n",
      "V275 float32 840073 12263\n",
      "V276 float32 840073 3740\n",
      "V277 float32 840073 4208\n",
      "V278 float32 840073 3960\n",
      "V279 float16 15 881\n",
      "V280 float16 15 975\n",
      "V281 float16 7300 31\n",
      "V282 float16 7300 64\n",
      "V283 float16 7300 69\n",
      "V284 float16 15 13\n",
      "V285 float16 15 96\n",
      "V286 float16 15 9\n",
      "V287 float16 15 32\n",
      "V288 float16 7300 11\n",
      "V289 float16 7300 13\n",
      "V290 float16 15 61\n",
      "V291 float16 15 315\n",
      "V292 float16 15 192\n",
      "V293 float16 15 870\n",
      "V294 float16 15 1286\n",
      "V295 float16 15 928\n",
      "V296 float16 7300 180\n",
      "V297 float16 15 86\n",
      "V298 float16 15 126\n",
      "V299 float16 15 107\n",
      "V300 float16 7300 15\n",
      "V301 float16 7300 15\n",
      "V302 float16 15 17\n",
      "V303 float16 15 21\n",
      "V304 float16 15 18\n",
      "V305 float16 15 2\n",
      "V306 float32 15 25278\n",
      "V307 float32 15 54581\n",
      "V308 float32 15 35248\n",
      "V309 float16 15 4255\n",
      "V310 float32 15 19523\n",
      "V311 float16 15 3427\n",
      "V312 float32 15 9958\n",
      "V313 float16 7300 5039\n",
      "V314 float16 7300 7177\n",
      "V315 float16 7300 5722\n",
      "V316 float32 15 14984\n",
      "V317 float32 15 22592\n",
      "V318 float32 15 18607\n",
      "V319 float32 15 8172\n",
      "V320 float32 15 10833\n",
      "V321 float32 15 9384\n",
      "V322 float16 938449 881\n",
      "V323 float16 938449 1411\n",
      "V324 float16 938449 976\n",
      "V325 float16 938449 13\n",
      "V326 float16 938449 45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V327 float16 938449 32\n",
      "V328 float16 938449 86\n",
      "V329 float16 938449 126\n",
      "V330 float16 938449 107\n",
      "V331 float32 938449 2092\n",
      "V332 float32 938449 3233\n",
      "V333 float32 938449 2491\n",
      "V334 float16 938449 220\n",
      "V335 float16 938449 1010\n",
      "V336 float16 938449 503\n",
      "V337 float32 938449 437\n",
      "V338 float32 938449 794\n",
      "V339 float32 938449 624\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(i, df[i].dtypes, df[i].isnull().sum(), df[i].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try this: Remove the feature if there are too many missing values for this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples=df.shape[0]\n",
    "threshold = 0.98*num_samples\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() > threshold:\n",
    "        del df[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dimension Reduction and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PCA to V1 to V339 (all of them are float types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the PCA algorithm with our Data - determine the appropriate number of components\n",
    "\n",
    "mas_v = ['V{}'.format(i) for i in range(1,340)]\n",
    "\n",
    "# standardize the data and fill missing values first \n",
    "for col in mas_v:\n",
    "    df[col].fillna((df_trans[col].median()), inplace=True)\n",
    "    df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n",
    "    df_trans[col].fillna((df_trans[col].median()), inplace=True)\n",
    "    df_trans[col] = (minmax_scale(df_trans[col], feature_range=(0,1)))\n",
    "v_data = df_trans[mas_v].values\n",
    "pca = PCA().fit(v_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select the appropriate number of components by eyeballing the variance chart\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_[:100]))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will select 30 components as they already capture >90% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = PCA_change(df, mas_v, prefix='PCA_V_', n_components=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PCA to C1 to C14 (all of them are float types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_feat = ['C' + str(i) for i in range(1,15)]\n",
    "\n",
    "for col in c_feat:\n",
    "    df[col].fillna((df_trans[col].median()), inplace=True)\n",
    "    df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n",
    "    df_trans[col].fillna((df_trans[col].median()), inplace=True)\n",
    "    df_trans[col] = (minmax_scale(df_trans[col], feature_range=(0,1)))\n",
    "c_data = df_trans[c_feat].values\n",
    "pca = PCA().fit(c_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select the appropriate number of components by eyeballing the variance chart\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will select 2 components as they already capture >95% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = PCA_change(df, c_feat, prefix='PCA_C_', n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 251.14 Mb (45.0% reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1097231, 73)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = reduce_mem_usage(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply PCA to D1 to D15 (all of them are float types)\n",
    "# We can also skip this (try it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_feat = ['D' + str(i) for i in range(1,16)]\n",
    "\n",
    "for col in d_feat:\n",
    "    df[col].fillna((df_trans[col].median()), inplace=True)\n",
    "    df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n",
    "    df_trans[col].fillna((df_trans[col].median()), inplace=True)\n",
    "    df_trans[col] = (minmax_scale(df_trans[col], feature_range=(0,1)))\n",
    "d_data = df_trans[d_feat].values\n",
    "pca = PCA().fit(d_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select the appropriate number of components by eyeballing the variance chart\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will select 8 components as they already capture >95% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = PCA_change(df, d_feat, prefix='PCA_D_', n_components=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 236.49 Mb (17.5% reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1097231, 66)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = reduce_mem_usage(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M features\n",
    "- Only feeling Na's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_feat = ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']\n",
    "\n",
    "for col in m_feat:\n",
    "    df[col].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Email Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum',\n",
    "          'scranton.edu': 'other', 'netzero.net': 'other',\n",
    "          'optonline.net': 'other', 'comcast.net': 'other', \n",
    "          'cfl.rr.com': 'other', 'sc.rr.com': 'other',\n",
    "          'suddenlink.net': 'other', 'windstream.net': 'other',\n",
    "          'gmx.de': 'other', 'earthlink.net': 'other', \n",
    "          'servicios-ta.com': 'other', 'bellsouth.net': 'other', \n",
    "          'web.de': 'other', 'mail.com': 'other',\n",
    "          'cableone.net': 'other', 'roadrunner.com': 'other', \n",
    "          'protonmail.com': 'other', 'anonymous.com': 'other',\n",
    "          'juno.com': 'other', 'ptd.net': 'other',\n",
    "          'netzero.com': 'other', 'cox.net': 'other', \n",
    "          'hotmail.co.uk': 'microsoft', \n",
    "          'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', \n",
    "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', \n",
    "          'live.com': 'microsoft', 'aim.com': 'aol',\n",
    "          'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n",
    "          'gmail.com': 'google', 'me.com': 'apple', \n",
    "          'hotmail.com': 'microsoft',  \n",
    "          'hotmail.fr': 'microsoft',\n",
    "          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', \n",
    "          'yahoo.de': 'yahoo', \n",
    "          'live.fr': 'microsoft', 'verizon.net': 'yahoo', \n",
    "          'msn.com': 'microsoft', 'q.com': 'centurylink',\n",
    "          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', \n",
    "           'rocketmail.com': 'yahoo', \n",
    "          'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n",
    "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', \n",
    "          'embarqmail.com': 'centurylink', \n",
    "          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo',\n",
    "          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft',\n",
    "           'aol.com': 'aol', 'icloud.com': 'apple'}\n",
    "\n",
    "us_emails = ['gmail', 'net', 'edu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\n",
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    df[c + '_bin'] = df[c].map(emails)\n",
    "    df[c + '_suffix'] = df[c].map(lambda x: str(x).split('.')[-1])\n",
    "    df[c + '_suffix'] = df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['P_emaildomain', 'R_emaildomain'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Transaction Date - extract day and hour info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "START_DATE = '2017-12-01'\n",
    "startdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n",
    "df[\"Date\"] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n",
    "df['_Weekdays'] = df['Date'].dt.dayofweek\n",
    "df['_Hours'] = df['Date'].dt.hour\n",
    "df['_Days'] = df['Date'].dt.day\n",
    "#df.drop(['TransactionDT'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Card info\n",
    "*card1,card2, card3, card5 - numeric\n",
    "*card4 - card type (discover, mastercard, visa)\n",
    "*card5 - card type (credit/debit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corret_card_id(x): \n",
    "    x=x.replace('.0','')\n",
    "    x=x.replace('-999','nan')\n",
    "    return x\n",
    "\n",
    "def process_card_info(df_in):\n",
    "\n",
    "    \n",
    "    # create card ID by concatenating all 4 cards together\n",
    "    cards_cols= ['card1', 'card2', 'card3', 'card5']\n",
    "    for card in cards_cols: \n",
    "        if '1' in card: \n",
    "            df_in['Card_ID']= df_in[card].map(str)\n",
    "        else : \n",
    "            df_in['Card_ID']+= ' '+df_in[card].map(str)\n",
    "    \n",
    "    # sort train data by Card_ID and then by transaction date \n",
    "    df_in= df_in.sort_values(['Card_ID', 'Date'], ascending=[True, True])\n",
    "    \n",
    "    # small correction of the Card_ID\n",
    "    df_in['Card_ID']=df_in['Card_ID'].apply(corret_card_id)\n",
    "    \n",
    "    df_in['mean_last'] = df_in.groupby('Card_ID')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).mean())\n",
    "    df_in['min_last'] = df_in.groupby('Card_ID')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).min())\n",
    "    df_in['max_last'] = df_in.groupby('Card_ID')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).max())\n",
    "    #df_in['std_last'] = df_in['mean_last'] / df.groupby('Card_ID')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).std())\n",
    "    # df['count_last'] = df.groupby('Card_ID')['TransactionAmt'].transform(lambda x: x.rolling(30, 1).count())\n",
    "    df_in['mean_last'].fillna(0, inplace=True, )\n",
    "    #df_in['std_last'].fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 273.11 Mb (7.4% reduction)\n"
     ]
    }
   ],
   "source": [
    "df = process_card_info(df)\n",
    "df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply log transform to transaction amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TransactionAmt'] = np.log(df['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransactionID int32 0\n",
      "isFraud object 0\n",
      "TransactionDT int32 0\n",
      "TransactionAmt float16 0\n",
      "ProductCD object 0\n",
      "card1 int16 0\n",
      "card2 float16 17587\n",
      "card3 float16 4567\n",
      "card4 object 4663\n",
      "card5 float16 8806\n",
      "card6 object 4578\n",
      "addr1 float16 131315\n",
      "addr2 float16 131315\n",
      "dist1 float16 643488\n",
      "dist2 float16 1023168\n",
      "M1 object 0\n",
      "M2 object 0\n",
      "M3 object 0\n",
      "M4 object 0\n",
      "M5 object 0\n",
      "M6 object 0\n",
      "M7 object 0\n",
      "M8 object 0\n",
      "M9 object 0\n",
      "PCA_V_0 float16 0\n",
      "PCA_V_1 float16 0\n",
      "PCA_V_2 float16 0\n",
      "PCA_V_3 float16 0\n",
      "PCA_V_4 float16 0\n",
      "PCA_V_5 float16 0\n",
      "PCA_V_6 float16 0\n",
      "PCA_V_7 float16 0\n",
      "PCA_V_8 float16 0\n",
      "PCA_V_9 float16 0\n",
      "PCA_V_10 float16 0\n",
      "PCA_V_11 float16 0\n",
      "PCA_V_12 float16 0\n",
      "PCA_V_13 float16 0\n",
      "PCA_V_14 float16 0\n",
      "PCA_V_15 float16 0\n",
      "PCA_V_16 float16 0\n",
      "PCA_V_17 float16 0\n",
      "PCA_V_18 float16 0\n",
      "PCA_V_19 float16 0\n",
      "PCA_V_20 float16 0\n",
      "PCA_V_21 float16 0\n",
      "PCA_V_22 float16 0\n",
      "PCA_V_23 float16 0\n",
      "PCA_V_24 float16 0\n",
      "PCA_V_25 float16 0\n",
      "PCA_V_26 float16 0\n",
      "PCA_V_27 float16 0\n",
      "PCA_V_28 float16 0\n",
      "PCA_V_29 float16 0\n",
      "PCA_C_0 float16 0\n",
      "PCA_C_1 float16 0\n",
      "PCA_D_0 float16 0\n",
      "PCA_D_1 float16 0\n",
      "PCA_D_2 float16 0\n",
      "PCA_D_3 float16 0\n",
      "PCA_D_4 float16 0\n",
      "PCA_D_5 float16 0\n",
      "PCA_D_6 float16 0\n",
      "PCA_D_7 float16 0\n",
      "P_emaildomain_bin object 163648\n",
      "P_emaildomain_suffix object 0\n",
      "R_emaildomain_bin object 824070\n",
      "R_emaildomain_suffix object 0\n",
      "Date datetime64[ns] 0\n",
      "_Weekdays int8 0\n",
      "_Hours int8 0\n",
      "_Days int8 0\n",
      "Card_ID object 0\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(i, df[i].dtype, df[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with ID Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans_id = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv', index_col='TransactionID')\n",
    "df_test_id = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv', index_col='TransactionID')\n",
    "\n",
    "sample_submission = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv', index_col='TransactionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 26.41 Mb (41.5% reduction)\n",
      "Mem. usage decreased to 52.39 Mb (41.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "## Joining test and train id's\n",
    "df_trans_id = reduce_mem_usage(df_trans_id)\n",
    "df_id = pd.concat([df_trans_id, df_test_id], axis=0, sort=False )\n",
    "del df_test_id\n",
    "df_id = reduce_mem_usage(df_id)\n",
    "df_id=df_id.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_num_cols = ['id_01', 'id_02', 'id_03', 'id_04', 'id_05', \n",
    "               'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11',\n",
    "               'id_13', 'id_14', 'id_17', 'id_18', 'id_19', 'id_20',\n",
    "               'id_21', 'id_22', 'id_24', 'id_25', 'id_26', 'id_32']   \n",
    "    \n",
    "for col in id_num_cols:\n",
    "    df_id[col].fillna(df_id[col].median(), inplace=True)\n",
    "    df_id[col] = (minmax_scale(df_id[col], feature_range=(0,1)))\n",
    "    df_trans_id[col].fillna((df_trans_id[col].median()), inplace=True)\n",
    "    df_trans_id[col] = (minmax_scale(df_trans_id[col], feature_range=(0,1)))\n",
    "id_data = df_trans_id[id_num_cols].values\n",
    "pca = PCA().fit(id_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the appropriate number of components by eyeballing the variance chart\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will select 10 components as they already capture >95% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id = PCA_change(df_id, id_num_cols, prefix='PCA_ID_', n_components=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill None's for categorical ID columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cat_cols = ['id_12', 'id_15', 'id_16', \n",
    "               'id_23', 'id_27', 'id_28', \n",
    "               'id_29', 'id_30', 'id_31',\n",
    "               'id_33', 'id_34', 'id_35', \n",
    "               'id_36', 'id_37', 'id_38']\n",
    "\n",
    "for col in id_cat_cols:\n",
    "    df_id[col].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID columns containing special information which need to be processed separately:\n",
    "* DeviceInfo\n",
    "* id_30 (contains OS/version info)\n",
    "* id_31 (contains browser/version info)\n",
    "* id_33 (contains screen width/height info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id['device_name'] = df_id['DeviceInfo'].str.split('/', expand=True)[0]\n",
    "df_id['device_version'] = df_id['DeviceInfo'].str.split('/', expand=True)[1]\n",
    "df_id.drop('DeviceInfo', axis=1, inplace=True)\n",
    "df_id['device_name'].fillna('None', inplace=True)\n",
    "df_id['device_version'].fillna('None', inplace=True)\n",
    "df_id['DeviceType'].fillna('None', inplace=True)\n",
    "\n",
    "df_id['OS_id_30'] = df_id['id_30'].str.split(' ', expand=True)[0]\n",
    "df_id['version_id_30'] = df_id['id_30'].str.split(' ', expand=True)[1]\n",
    "df_id['OS_id_30'].fillna('None', inplace=True)\n",
    "df_id['version_id_30'].fillna('None', inplace=True)\n",
    "df_id.drop('id_30', axis=1, inplace=True)\n",
    "\n",
    "df_id['browser_id_31'] = df_id['id_31'].str.split(' ', expand=True)[0]\n",
    "df_id['version_id_31'] = df_id['id_31'].str.split(' ', expand=True)[1]\n",
    "df_id['browser_id_31'].fillna('None', inplace=True)\n",
    "df_id['version_id_31'].fillna('None', inplace=True)\n",
    "df_id.drop('id_31', axis=1, inplace=True)\n",
    "\n",
    "df_id['screen_width'] = df_id['id_33'].str.split('x', expand=True)[0]\n",
    "df_id['screen_height'] = df_id['id_33'].str.split('x', expand=True)[1]\n",
    "df_id['screen_width'].fillna(-1, inplace=True)\n",
    "df_id['screen_height'].fillna(-1, inplace=True)\n",
    "df_id.drop('id_33', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "## Device renaming\n",
    "df_id.loc[df_id['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n",
    "df_id.loc[df_id['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n",
    "df_id.loc[df_id['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n",
    "df_id.loc[df_id['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n",
    "df_id.loc[df_id['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n",
    "df_id.loc[df_id['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n",
    "df_id.loc[df_id['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n",
    "df_id.loc[df_id['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n",
    "df_id.loc[df_id['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n",
    "df_id.loc[df_id['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n",
    "df_id.loc[df_id['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n",
    "df_id.loc[df_id['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n",
    "df_id.loc[df_id['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n",
    "df_id.loc[df_id['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n",
    "df_id.loc[df_id['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n",
    "df_id.loc[df_id['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n",
    "df_id.loc[df_id['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group other less popular device names\n",
    "df_id.loc[df_id.device_name.isin(df_id.device_name.value_counts()[df_id.device_name.value_counts() < 200].index), 'device_name'] = \"Others\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge ID and transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN       643488\n",
       "0.0        39102\n",
       "1.0        36355\n",
       "2.0        32715\n",
       "4.0        25204\n",
       "           ...  \n",
       "3390.0         1\n",
       "1670.0         1\n",
       "3348.0         1\n",
       "1368.0         1\n",
       "3182.0         1\n",
       "Name: dist1, Length: 2480, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dist1.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop all null column\n",
    "df.drop('dist1',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge ID information into transaction data so we consolidate everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.merge(df,df_id, on=['TransactionID'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in df.columns:\n",
    "    if df[f].isnull().sum() > 0:\n",
    "        if df[f].dtype == 'object':\n",
    "            df[f].fillna(df[f].mode()[0],inplace=True)\n",
    "        else:\n",
    "            df[f].fillna(df[f].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More work needed - try one-hot encoding, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "for f in df.columns:\n",
    "    if f=='isFraud':\n",
    "        continue\n",
    "    elif df[f].dtype=='object' and f != 'isFraud': \n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        df[f] = lbl.fit_transform(list(df[f].values))\n",
    "    else:\n",
    "        df[f] = (minmax_scale(df[f], feature_range=(0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At this point, check and make sure that there are no colummns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransactionID 0 float64\n",
      "isFraud 0 object\n",
      "TransactionDT 0 float64\n",
      "TransactionAmt 0 float16\n",
      "ProductCD 0 int64\n",
      "card1 0 float64\n",
      "card2 0 float16\n",
      "card3 0 float16\n",
      "card4 0 int64\n",
      "card5 0 float16\n",
      "card6 0 int64\n",
      "addr1 0 float16\n",
      "addr2 0 float16\n",
      "dist2 0 float16\n",
      "M1 0 int64\n",
      "M2 0 int64\n",
      "M3 0 int64\n",
      "M4 0 int64\n",
      "M5 0 int64\n",
      "M6 0 int64\n",
      "M7 0 int64\n",
      "M8 0 int64\n",
      "M9 0 int64\n",
      "PCA_V_0 0 float16\n",
      "PCA_V_1 0 float16\n",
      "PCA_V_2 0 float16\n",
      "PCA_V_3 0 float16\n",
      "PCA_V_4 0 float16\n",
      "PCA_V_5 0 float16\n",
      "PCA_V_6 0 float16\n",
      "PCA_V_7 0 float16\n",
      "PCA_V_8 0 float16\n",
      "PCA_V_9 0 float16\n",
      "PCA_V_10 0 float16\n",
      "PCA_V_11 0 float16\n",
      "PCA_V_12 0 float16\n",
      "PCA_V_13 0 float16\n",
      "PCA_V_14 0 float16\n",
      "PCA_V_15 0 float16\n",
      "PCA_V_16 0 float16\n",
      "PCA_V_17 0 float16\n",
      "PCA_V_18 0 float16\n",
      "PCA_V_19 0 float16\n",
      "PCA_V_20 0 float16\n",
      "PCA_V_21 0 float16\n",
      "PCA_V_22 0 float16\n",
      "PCA_V_23 0 float16\n",
      "PCA_V_24 0 float16\n",
      "PCA_V_25 0 float16\n",
      "PCA_V_26 0 float16\n",
      "PCA_V_27 0 float16\n",
      "PCA_V_28 0 float16\n",
      "PCA_V_29 0 float16\n",
      "PCA_C_0 0 float16\n",
      "PCA_C_1 0 float16\n",
      "PCA_D_0 0 float16\n",
      "PCA_D_1 0 float16\n",
      "PCA_D_2 0 float16\n",
      "PCA_D_3 0 float16\n",
      "PCA_D_4 0 float16\n",
      "PCA_D_5 0 float16\n",
      "PCA_D_6 0 float16\n",
      "PCA_D_7 0 float16\n",
      "P_emaildomain_bin 0 int64\n",
      "P_emaildomain_suffix 0 int64\n",
      "R_emaildomain_bin 0 int64\n",
      "R_emaildomain_suffix 0 int64\n",
      "_Weekdays 0 float64\n",
      "_Hours 0 float64\n",
      "_Days 0 float64\n",
      "Card_ID 0 int64\n",
      "id_12 0 int64\n",
      "id_15 0 int64\n",
      "id_16 0 int64\n",
      "id_23 0 int64\n",
      "id_27 0 int64\n",
      "id_28 0 int64\n",
      "id_29 0 int64\n",
      "id_34 0 int64\n",
      "id_35 0 int64\n",
      "id_36 0 int64\n",
      "id_37 0 int64\n",
      "id_38 0 int64\n",
      "DeviceType 0 int64\n",
      "PCA_ID_0 0 float64\n",
      "PCA_ID_1 0 float64\n",
      "PCA_ID_2 0 float64\n",
      "PCA_ID_3 0 float64\n",
      "PCA_ID_4 0 float64\n",
      "PCA_ID_5 0 float64\n",
      "PCA_ID_6 0 float64\n",
      "PCA_ID_7 0 float64\n",
      "PCA_ID_8 0 float64\n",
      "PCA_ID_9 0 float64\n",
      "device_name 0 int64\n",
      "device_version 0 int64\n",
      "OS_id_30 0 int64\n",
      "version_id_30 0 int64\n",
      "browser_id_31 0 int64\n",
      "version_id_31 0 int64\n",
      "screen_width 0 int64\n",
      "screen_height 0 int64\n"
     ]
    }
   ],
   "source": [
    "for c in df.columns:\n",
    "    print(c, df[c].isnull().sum(), df[c].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the processed data so far to a file so you don't have to repeat the data processing process above again every time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf('train_test_data_preprocessed.hdf5',key='data',mode='w')\n",
    "df.to_csv('train_test_data_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start from here to read saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File train_test_data_preprocessed.hdf5 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fe09edd29203>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_test_data_preprocessed.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             raise FileNotFoundError(\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;34m\"File {path} does not exist\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m             )\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File train_test_data_preprocessed.hdf5 does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_hdf('train_test_data_preprocessed.hdf5','data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset again into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop('isFraud', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.98\n",
    "    \n",
    "# Absolute value correlation matrix\n",
    "corr_matrix = df_train[df_train['isFraud'].notnull()].corr().abs()\n",
    "\n",
    "# Getting the upper triangle of correlations\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Select columns with correlations above threshold\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "#to_drop.remove('TransactionDT')\n",
    "\n",
    "df_train = df_train.drop(columns = to_drop)\n",
    "df_test = df_test.drop(columns = to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TransactionDT']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting X and y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_train.sort_values('TransactionID').drop(['isFraud', \n",
    "                                                      'TransactionID', \n",
    "                                                      'Card_ID'\n",
    "                                                     ],\n",
    "                                                     axis=1)\n",
    "y_train = df_train.sort_values('TransactionID')['isFraud'].astype(bool)\n",
    "\n",
    "X_test = df_test.sort_values('TransactionID').drop(['TransactionID',\n",
    "                                                    'Card_ID'\n",
    "                                                   ], \n",
    "                                                   axis=1)\n",
    "del df_train\n",
    "df_test = df_test[[\"TransactionID\"]]\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransactionAmt 0 float16\n",
      "ProductCD 0 int64\n",
      "card1 0 float64\n",
      "card2 0 float16\n",
      "card3 0 float16\n",
      "card4 0 int64\n",
      "card5 0 float16\n",
      "card6 0 int64\n",
      "addr1 0 float16\n",
      "addr2 0 float16\n",
      "dist2 0 float16\n",
      "M1 0 int64\n",
      "M2 0 int64\n",
      "M3 0 int64\n",
      "M4 0 int64\n",
      "M5 0 int64\n",
      "M6 0 int64\n",
      "M7 0 int64\n",
      "M8 0 int64\n",
      "M9 0 int64\n",
      "PCA_V_0 0 float16\n",
      "PCA_V_1 0 float16\n",
      "PCA_V_2 0 float16\n",
      "PCA_V_3 0 float16\n",
      "PCA_V_4 0 float16\n",
      "PCA_V_5 0 float16\n",
      "PCA_V_6 0 float16\n",
      "PCA_V_7 0 float16\n",
      "PCA_V_8 0 float16\n",
      "PCA_V_9 0 float16\n",
      "PCA_V_10 0 float16\n",
      "PCA_V_11 0 float16\n",
      "PCA_V_12 0 float16\n",
      "PCA_V_13 0 float16\n",
      "PCA_V_14 0 float16\n",
      "PCA_V_15 0 float16\n",
      "PCA_V_16 0 float16\n",
      "PCA_V_17 0 float16\n",
      "PCA_V_18 0 float16\n",
      "PCA_V_19 0 float16\n",
      "PCA_V_20 0 float16\n",
      "PCA_V_21 0 float16\n",
      "PCA_V_22 0 float16\n",
      "PCA_V_23 0 float16\n",
      "PCA_V_24 0 float16\n",
      "PCA_V_25 0 float16\n",
      "PCA_V_26 0 float16\n",
      "PCA_V_27 0 float16\n",
      "PCA_V_28 0 float16\n",
      "PCA_V_29 0 float16\n",
      "PCA_C_0 0 float16\n",
      "PCA_C_1 0 float16\n",
      "PCA_D_0 0 float16\n",
      "PCA_D_1 0 float16\n",
      "PCA_D_2 0 float16\n",
      "PCA_D_3 0 float16\n",
      "PCA_D_4 0 float16\n",
      "PCA_D_5 0 float16\n",
      "PCA_D_6 0 float16\n",
      "PCA_D_7 0 float16\n",
      "P_emaildomain_bin 0 int64\n",
      "P_emaildomain_suffix 0 int64\n",
      "R_emaildomain_bin 0 int64\n",
      "R_emaildomain_suffix 0 int64\n",
      "_Weekdays 0 float64\n",
      "_Hours 0 float64\n",
      "_Days 0 float64\n",
      "id_12 0 int64\n",
      "id_15 0 int64\n",
      "id_16 0 int64\n",
      "id_23 0 int64\n",
      "id_27 0 int64\n",
      "id_28 0 int64\n",
      "id_29 0 int64\n",
      "id_34 0 int64\n",
      "id_35 0 int64\n",
      "id_36 0 int64\n",
      "id_37 0 int64\n",
      "id_38 0 int64\n",
      "DeviceType 0 int64\n",
      "PCA_ID_0 0 float64\n",
      "PCA_ID_1 0 float64\n",
      "PCA_ID_2 0 float64\n",
      "PCA_ID_3 0 float64\n",
      "PCA_ID_4 0 float64\n",
      "PCA_ID_5 0 float64\n",
      "PCA_ID_6 0 float64\n",
      "PCA_ID_7 0 float64\n",
      "PCA_ID_8 0 float64\n",
      "PCA_ID_9 0 float64\n",
      "device_name 0 int64\n",
      "device_version 0 int64\n",
      "OS_id_30 0 int64\n",
      "version_id_30 0 int64\n",
      "browser_id_31 0 int64\n",
      "version_id_31 0 int64\n",
      "screen_width 0 int64\n",
      "screen_height 0 int64\n"
     ]
    }
   ],
   "source": [
    "for c in X_train.columns:\n",
    "    print(c, X_train[c].isnull().sum(), X_train[c].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert training target from boolean to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by try a few individual models to get a gut feeling, before moving on into more sophisticated fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X_train, y_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Warm-up - Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "lr_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(cross_val_score(lr_model, X_train, y_train, cv=10))  \n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = lr_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00241583, 0.02139476, 0.04118522, ..., 0.01091927, 0.020193  ,\n",
       "       0.04933072])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note - below code is for illustration. It has not been fine-tuned and optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.797297\teval-auc:0.791576\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 200 rounds.\n",
      "[99]\ttrain-auc:0.930764\teval-auc:0.912531\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "hyper = {\n",
    "    'booster' : 'gbtree',\n",
    "    'max_depth' : 6,\n",
    "    'nthread' : -1,\n",
    "    'num_class' : 1,\n",
    "    'objective' : 'binary:logistic',\n",
    "    'verbosity' : 0,\n",
    "    'eval_metric' : 'auc',\n",
    "    'eta' : 0.1,\n",
    "    'tree_method' : 'auto',\n",
    "    'min_child_weight' : 1,\n",
    "    'colsample_bytree' : 0.8,\n",
    "    'colsample_bylevel' : 0.8,\n",
    "    'seed' : 0\n",
    "}\n",
    "\n",
    "dtrn = xgb.DMatrix(train_X, label=train_y, feature_names=X_train.columns)\n",
    "dval = xgb.DMatrix(val_X, label=val_y, feature_names=X_train.columns)\n",
    "\n",
    "#num_boost_round = 10000\n",
    "num_boost_round=100\n",
    "_xgb = xgb.train(hyper, dtrn, num_boost_round=100, evals=[(dtrn, 'train'), (dval, 'eval')], early_stopping_rounds=200, verbose_eval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst = xgb.DMatrix(X_test, feature_names=X_test.columns)\n",
    "pred_xgb = _xgb.predict(dtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00591842, 0.01099287, 0.02207627, ..., 0.00919327, 0.01699483,\n",
       "       0.01386742], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Light Gradient Boosting Machine(LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.914699\tvalid_1's auc: 0.900664\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgbm\n",
    "\n",
    "hyper = {\n",
    "    'num_leaves' : 500,\n",
    "    'min_child_weight': 0.03,\n",
    "    'feature_fraction': 0.4,\n",
    "    'bagging_fraction': 0.4,\n",
    "    'min_data_in_leaf': 100,\n",
    "    'objective': 'binary',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'bagging_seed': 10,\n",
    "    'metric': 'auc',\n",
    "    'verbosity': 0,\n",
    "    'reg_alpha': 0.4,\n",
    "    'reg_lambda': 0.6,\n",
    "    'random_state': 0\n",
    "}\n",
    "\n",
    "dtrain = lgbm.Dataset(train_X, label=train_y)\n",
    "dvalid = lgbm.Dataset(val_X, label=val_y)\n",
    "n_iter = 100\n",
    "lgbm_model = lgbm.train(hyper, dtrain, n_iter, valid_sets=[dtrain, dvalid], verbose_eval=200, early_stopping_rounds=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_lgbm = lgbm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'depth': 8,\n",
    "          'iterations':100,\n",
    "          'learning_rate':0.01, \n",
    "          'l2_leaf_reg': 5,\n",
    "          'border_count': 10,\n",
    "          'leaf_estimation_method': 'Gradient',\n",
    "           'feature_border_type': 'MaxLogSum'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.10195\n",
      "0:\tlearn: 0.5371285\ttotal: 369ms\tremaining: 6m 8s\n",
      "1:\tlearn: 0.4200191\ttotal: 727ms\tremaining: 6m 2s\n",
      "2:\tlearn: 0.3371547\ttotal: 1.02s\tremaining: 5m 38s\n",
      "3:\tlearn: 0.2797513\ttotal: 1.35s\tremaining: 5m 36s\n",
      "4:\tlearn: 0.2386788\ttotal: 1.66s\tremaining: 5m 29s\n",
      "5:\tlearn: 0.2084511\ttotal: 1.97s\tremaining: 5m 26s\n",
      "6:\tlearn: 0.1860139\ttotal: 2.3s\tremaining: 5m 26s\n",
      "7:\tlearn: 0.1703321\ttotal: 2.62s\tremaining: 5m 24s\n",
      "8:\tlearn: 0.1585343\ttotal: 2.95s\tremaining: 5m 24s\n",
      "9:\tlearn: 0.1484961\ttotal: 3.28s\tremaining: 5m 24s\n",
      "10:\tlearn: 0.1412914\ttotal: 3.62s\tremaining: 5m 25s\n",
      "11:\tlearn: 0.1356100\ttotal: 3.95s\tremaining: 5m 25s\n",
      "12:\tlearn: 0.1314933\ttotal: 4.25s\tremaining: 5m 23s\n",
      "13:\tlearn: 0.1276312\ttotal: 4.59s\tremaining: 5m 23s\n",
      "14:\tlearn: 0.1247711\ttotal: 4.93s\tremaining: 5m 24s\n",
      "15:\tlearn: 0.1224719\ttotal: 5.25s\tremaining: 5m 23s\n",
      "16:\tlearn: 0.1200990\ttotal: 5.57s\tremaining: 5m 22s\n",
      "17:\tlearn: 0.1184164\ttotal: 5.92s\tremaining: 5m 22s\n",
      "18:\tlearn: 0.1167325\ttotal: 6.25s\tremaining: 5m 22s\n",
      "19:\tlearn: 0.1155519\ttotal: 6.56s\tremaining: 5m 21s\n",
      "20:\tlearn: 0.1145160\ttotal: 6.88s\tremaining: 5m 20s\n",
      "21:\tlearn: 0.1135600\ttotal: 7.2s\tremaining: 5m 20s\n",
      "22:\tlearn: 0.1128546\ttotal: 7.5s\tremaining: 5m 18s\n",
      "23:\tlearn: 0.1119784\ttotal: 7.86s\tremaining: 5m 19s\n",
      "24:\tlearn: 0.1114243\ttotal: 8.19s\tremaining: 5m 19s\n",
      "25:\tlearn: 0.1107472\ttotal: 8.54s\tremaining: 5m 19s\n",
      "26:\tlearn: 0.1101798\ttotal: 8.85s\tremaining: 5m 18s\n",
      "27:\tlearn: 0.1094959\ttotal: 9.2s\tremaining: 5m 19s\n",
      "28:\tlearn: 0.1089682\ttotal: 9.53s\tremaining: 5m 18s\n",
      "29:\tlearn: 0.1086614\ttotal: 9.85s\tremaining: 5m 18s\n",
      "30:\tlearn: 0.1081571\ttotal: 10.2s\tremaining: 5m 17s\n",
      "31:\tlearn: 0.1078395\ttotal: 10.5s\tremaining: 5m 16s\n",
      "32:\tlearn: 0.1074377\ttotal: 10.8s\tremaining: 5m 15s\n",
      "33:\tlearn: 0.1070711\ttotal: 11.1s\tremaining: 5m 15s\n",
      "34:\tlearn: 0.1067549\ttotal: 11.4s\tremaining: 5m 14s\n",
      "35:\tlearn: 0.1063923\ttotal: 11.7s\tremaining: 5m 13s\n",
      "36:\tlearn: 0.1061104\ttotal: 12s\tremaining: 5m 13s\n",
      "37:\tlearn: 0.1057739\ttotal: 12.4s\tremaining: 5m 12s\n",
      "38:\tlearn: 0.1055685\ttotal: 12.7s\tremaining: 5m 11s\n",
      "39:\tlearn: 0.1053567\ttotal: 12.9s\tremaining: 5m 10s\n",
      "40:\tlearn: 0.1051692\ttotal: 13.2s\tremaining: 5m 9s\n",
      "41:\tlearn: 0.1049261\ttotal: 13.6s\tremaining: 5m 9s\n",
      "42:\tlearn: 0.1046189\ttotal: 13.9s\tremaining: 5m 8s\n",
      "43:\tlearn: 0.1044193\ttotal: 14.2s\tremaining: 5m 9s\n",
      "44:\tlearn: 0.1042361\ttotal: 14.5s\tremaining: 5m 8s\n",
      "45:\tlearn: 0.1039734\ttotal: 14.9s\tremaining: 5m 8s\n",
      "46:\tlearn: 0.1037523\ttotal: 15.2s\tremaining: 5m 7s\n",
      "47:\tlearn: 0.1034617\ttotal: 15.5s\tremaining: 5m 7s\n",
      "48:\tlearn: 0.1032318\ttotal: 15.8s\tremaining: 5m 6s\n",
      "49:\tlearn: 0.1030350\ttotal: 16.1s\tremaining: 5m 5s\n",
      "50:\tlearn: 0.1029254\ttotal: 16.4s\tremaining: 5m 4s\n",
      "51:\tlearn: 0.1027130\ttotal: 16.7s\tremaining: 5m 4s\n",
      "52:\tlearn: 0.1025066\ttotal: 17.1s\tremaining: 5m 4s\n",
      "53:\tlearn: 0.1023931\ttotal: 17.4s\tremaining: 5m 4s\n",
      "54:\tlearn: 0.1021625\ttotal: 17.7s\tremaining: 5m 4s\n",
      "55:\tlearn: 0.1020176\ttotal: 18s\tremaining: 5m 3s\n",
      "56:\tlearn: 0.1018564\ttotal: 18.3s\tremaining: 5m 3s\n",
      "57:\tlearn: 0.1016471\ttotal: 18.7s\tremaining: 5m 3s\n",
      "58:\tlearn: 0.1015050\ttotal: 19s\tremaining: 5m 2s\n",
      "59:\tlearn: 0.1012127\ttotal: 19.3s\tremaining: 5m 2s\n",
      "60:\tlearn: 0.1010852\ttotal: 19.7s\tremaining: 5m 2s\n",
      "61:\tlearn: 0.1009164\ttotal: 20s\tremaining: 5m 2s\n",
      "62:\tlearn: 0.1007543\ttotal: 20.3s\tremaining: 5m 1s\n",
      "63:\tlearn: 0.1006033\ttotal: 20.6s\tremaining: 5m 1s\n",
      "64:\tlearn: 0.1004698\ttotal: 20.9s\tremaining: 5m\n",
      "65:\tlearn: 0.1004239\ttotal: 21.2s\tremaining: 4m 59s\n",
      "66:\tlearn: 0.1002737\ttotal: 21.5s\tremaining: 4m 59s\n",
      "67:\tlearn: 0.1001549\ttotal: 21.8s\tremaining: 4m 59s\n",
      "68:\tlearn: 0.0999708\ttotal: 22.2s\tremaining: 4m 59s\n",
      "69:\tlearn: 0.0998639\ttotal: 22.5s\tremaining: 4m 58s\n",
      "70:\tlearn: 0.0996450\ttotal: 22.9s\tremaining: 4m 59s\n",
      "71:\tlearn: 0.0994855\ttotal: 23.2s\tremaining: 4m 59s\n",
      "72:\tlearn: 0.0993646\ttotal: 23.6s\tremaining: 4m 59s\n",
      "73:\tlearn: 0.0992891\ttotal: 23.9s\tremaining: 4m 58s\n",
      "74:\tlearn: 0.0991493\ttotal: 24.2s\tremaining: 4m 58s\n",
      "75:\tlearn: 0.0990713\ttotal: 24.5s\tremaining: 4m 57s\n",
      "76:\tlearn: 0.0989262\ttotal: 24.8s\tremaining: 4m 57s\n",
      "77:\tlearn: 0.0988394\ttotal: 25.1s\tremaining: 4m 56s\n",
      "78:\tlearn: 0.0986678\ttotal: 25.4s\tremaining: 4m 56s\n",
      "79:\tlearn: 0.0986123\ttotal: 25.7s\tremaining: 4m 55s\n",
      "80:\tlearn: 0.0984474\ttotal: 26s\tremaining: 4m 55s\n",
      "81:\tlearn: 0.0983275\ttotal: 26.3s\tremaining: 4m 54s\n",
      "82:\tlearn: 0.0982127\ttotal: 26.7s\tremaining: 4m 54s\n",
      "83:\tlearn: 0.0981307\ttotal: 26.9s\tremaining: 4m 53s\n",
      "84:\tlearn: 0.0979871\ttotal: 27.3s\tremaining: 4m 53s\n",
      "85:\tlearn: 0.0978816\ttotal: 27.6s\tremaining: 4m 52s\n",
      "86:\tlearn: 0.0977360\ttotal: 27.9s\tremaining: 4m 52s\n",
      "87:\tlearn: 0.0976938\ttotal: 28.2s\tremaining: 4m 52s\n",
      "88:\tlearn: 0.0976304\ttotal: 28.5s\tremaining: 4m 51s\n",
      "89:\tlearn: 0.0975213\ttotal: 28.8s\tremaining: 4m 51s\n",
      "90:\tlearn: 0.0974693\ttotal: 29.1s\tremaining: 4m 50s\n",
      "91:\tlearn: 0.0973978\ttotal: 29.4s\tremaining: 4m 49s\n",
      "92:\tlearn: 0.0972621\ttotal: 29.6s\tremaining: 4m 49s\n",
      "93:\tlearn: 0.0971563\ttotal: 30s\tremaining: 4m 48s\n",
      "94:\tlearn: 0.0970034\ttotal: 30.3s\tremaining: 4m 48s\n",
      "95:\tlearn: 0.0969350\ttotal: 30.6s\tremaining: 4m 48s\n",
      "96:\tlearn: 0.0968088\ttotal: 30.9s\tremaining: 4m 47s\n",
      "97:\tlearn: 0.0966747\ttotal: 31.2s\tremaining: 4m 47s\n",
      "98:\tlearn: 0.0965374\ttotal: 31.6s\tremaining: 4m 47s\n",
      "99:\tlearn: 0.0963728\ttotal: 31.9s\tremaining: 4m 47s\n",
      "100:\tlearn: 0.0962823\ttotal: 32.3s\tremaining: 4m 47s\n",
      "101:\tlearn: 0.0962043\ttotal: 32.6s\tremaining: 4m 46s\n",
      "102:\tlearn: 0.0960778\ttotal: 32.9s\tremaining: 4m 46s\n",
      "103:\tlearn: 0.0960041\ttotal: 33.3s\tremaining: 4m 46s\n",
      "104:\tlearn: 0.0958981\ttotal: 33.6s\tremaining: 4m 46s\n",
      "105:\tlearn: 0.0958127\ttotal: 33.9s\tremaining: 4m 46s\n",
      "106:\tlearn: 0.0957229\ttotal: 34.3s\tremaining: 4m 46s\n",
      "107:\tlearn: 0.0956552\ttotal: 34.6s\tremaining: 4m 45s\n",
      "108:\tlearn: 0.0955613\ttotal: 34.9s\tremaining: 4m 45s\n",
      "109:\tlearn: 0.0955091\ttotal: 35.2s\tremaining: 4m 44s\n",
      "110:\tlearn: 0.0954028\ttotal: 35.5s\tremaining: 4m 44s\n",
      "111:\tlearn: 0.0953344\ttotal: 35.8s\tremaining: 4m 44s\n",
      "112:\tlearn: 0.0952835\ttotal: 36.2s\tremaining: 4m 43s\n",
      "113:\tlearn: 0.0952007\ttotal: 36.5s\tremaining: 4m 43s\n",
      "114:\tlearn: 0.0950960\ttotal: 36.8s\tremaining: 4m 43s\n",
      "115:\tlearn: 0.0950107\ttotal: 37.1s\tremaining: 4m 42s\n",
      "116:\tlearn: 0.0949229\ttotal: 37.4s\tremaining: 4m 42s\n",
      "117:\tlearn: 0.0948505\ttotal: 37.7s\tremaining: 4m 42s\n",
      "118:\tlearn: 0.0948067\ttotal: 38s\tremaining: 4m 41s\n",
      "119:\tlearn: 0.0947181\ttotal: 38.3s\tremaining: 4m 41s\n",
      "120:\tlearn: 0.0946002\ttotal: 38.7s\tremaining: 4m 40s\n",
      "121:\tlearn: 0.0945360\ttotal: 39s\tremaining: 4m 40s\n",
      "122:\tlearn: 0.0944554\ttotal: 39.3s\tremaining: 4m 40s\n",
      "123:\tlearn: 0.0943585\ttotal: 39.7s\tremaining: 4m 40s\n",
      "124:\tlearn: 0.0943087\ttotal: 40s\tremaining: 4m 39s\n",
      "125:\tlearn: 0.0942725\ttotal: 40.3s\tremaining: 4m 39s\n",
      "126:\tlearn: 0.0941813\ttotal: 40.6s\tremaining: 4m 39s\n",
      "127:\tlearn: 0.0940986\ttotal: 40.9s\tremaining: 4m 38s\n",
      "128:\tlearn: 0.0940653\ttotal: 41.3s\tremaining: 4m 38s\n",
      "129:\tlearn: 0.0939706\ttotal: 41.6s\tremaining: 4m 38s\n",
      "130:\tlearn: 0.0939231\ttotal: 41.9s\tremaining: 4m 37s\n",
      "131:\tlearn: 0.0938165\ttotal: 42.3s\tremaining: 4m 37s\n",
      "132:\tlearn: 0.0937775\ttotal: 42.5s\tremaining: 4m 37s\n",
      "133:\tlearn: 0.0936933\ttotal: 42.9s\tremaining: 4m 37s\n",
      "134:\tlearn: 0.0936263\ttotal: 43.2s\tremaining: 4m 36s\n",
      "135:\tlearn: 0.0935395\ttotal: 43.5s\tremaining: 4m 36s\n",
      "136:\tlearn: 0.0934641\ttotal: 43.8s\tremaining: 4m 36s\n",
      "137:\tlearn: 0.0934312\ttotal: 44.1s\tremaining: 4m 35s\n",
      "138:\tlearn: 0.0933839\ttotal: 44.4s\tremaining: 4m 35s\n",
      "139:\tlearn: 0.0933419\ttotal: 44.7s\tremaining: 4m 34s\n",
      "140:\tlearn: 0.0932874\ttotal: 45s\tremaining: 4m 34s\n",
      "141:\tlearn: 0.0932465\ttotal: 45.3s\tremaining: 4m 33s\n",
      "142:\tlearn: 0.0931814\ttotal: 45.6s\tremaining: 4m 33s\n",
      "143:\tlearn: 0.0930935\ttotal: 45.9s\tremaining: 4m 32s\n",
      "144:\tlearn: 0.0929720\ttotal: 46.2s\tremaining: 4m 32s\n",
      "145:\tlearn: 0.0928802\ttotal: 46.6s\tremaining: 4m 32s\n",
      "146:\tlearn: 0.0928206\ttotal: 46.9s\tremaining: 4m 32s\n",
      "147:\tlearn: 0.0927767\ttotal: 47.2s\tremaining: 4m 31s\n",
      "148:\tlearn: 0.0926807\ttotal: 47.5s\tremaining: 4m 31s\n",
      "149:\tlearn: 0.0925486\ttotal: 47.8s\tremaining: 4m 30s\n",
      "150:\tlearn: 0.0924654\ttotal: 48.1s\tremaining: 4m 30s\n",
      "151:\tlearn: 0.0923626\ttotal: 48.5s\tremaining: 4m 30s\n",
      "152:\tlearn: 0.0922478\ttotal: 48.8s\tremaining: 4m 30s\n",
      "153:\tlearn: 0.0921891\ttotal: 49.2s\tremaining: 4m 30s\n",
      "154:\tlearn: 0.0921555\ttotal: 49.4s\tremaining: 4m 29s\n",
      "155:\tlearn: 0.0920895\ttotal: 49.7s\tremaining: 4m 29s\n",
      "156:\tlearn: 0.0919731\ttotal: 50.1s\tremaining: 4m 28s\n",
      "157:\tlearn: 0.0918503\ttotal: 50.4s\tremaining: 4m 28s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158:\tlearn: 0.0917531\ttotal: 50.7s\tremaining: 4m 28s\n",
      "159:\tlearn: 0.0916845\ttotal: 51s\tremaining: 4m 27s\n",
      "160:\tlearn: 0.0916144\ttotal: 51.3s\tremaining: 4m 27s\n",
      "161:\tlearn: 0.0915794\ttotal: 51.7s\tremaining: 4m 27s\n",
      "162:\tlearn: 0.0914917\ttotal: 52s\tremaining: 4m 26s\n",
      "163:\tlearn: 0.0913965\ttotal: 52.3s\tremaining: 4m 26s\n",
      "164:\tlearn: 0.0913520\ttotal: 52.7s\tremaining: 4m 26s\n",
      "165:\tlearn: 0.0912961\ttotal: 53s\tremaining: 4m 26s\n",
      "166:\tlearn: 0.0912448\ttotal: 53.3s\tremaining: 4m 25s\n",
      "167:\tlearn: 0.0911663\ttotal: 53.6s\tremaining: 4m 25s\n",
      "168:\tlearn: 0.0910893\ttotal: 53.9s\tremaining: 4m 25s\n",
      "169:\tlearn: 0.0910222\ttotal: 54.2s\tremaining: 4m 24s\n",
      "170:\tlearn: 0.0909606\ttotal: 54.6s\tremaining: 4m 24s\n",
      "171:\tlearn: 0.0908723\ttotal: 54.9s\tremaining: 4m 24s\n",
      "172:\tlearn: 0.0908132\ttotal: 55.2s\tremaining: 4m 23s\n",
      "173:\tlearn: 0.0907456\ttotal: 55.5s\tremaining: 4m 23s\n",
      "174:\tlearn: 0.0906622\ttotal: 55.9s\tremaining: 4m 23s\n",
      "175:\tlearn: 0.0906293\ttotal: 56.1s\tremaining: 4m 22s\n",
      "176:\tlearn: 0.0905942\ttotal: 56.5s\tremaining: 4m 22s\n",
      "177:\tlearn: 0.0904985\ttotal: 56.8s\tremaining: 4m 22s\n",
      "178:\tlearn: 0.0904341\ttotal: 57.2s\tremaining: 4m 22s\n",
      "179:\tlearn: 0.0903828\ttotal: 57.5s\tremaining: 4m 21s\n",
      "180:\tlearn: 0.0903156\ttotal: 57.8s\tremaining: 4m 21s\n",
      "181:\tlearn: 0.0902697\ttotal: 58.1s\tremaining: 4m 21s\n",
      "182:\tlearn: 0.0901885\ttotal: 58.4s\tremaining: 4m 20s\n",
      "183:\tlearn: 0.0901634\ttotal: 58.8s\tremaining: 4m 20s\n",
      "184:\tlearn: 0.0900958\ttotal: 59.1s\tremaining: 4m 20s\n",
      "185:\tlearn: 0.0900349\ttotal: 59.4s\tremaining: 4m 19s\n",
      "186:\tlearn: 0.0899588\ttotal: 59.7s\tremaining: 4m 19s\n",
      "187:\tlearn: 0.0898997\ttotal: 1m\tremaining: 4m 19s\n",
      "188:\tlearn: 0.0898375\ttotal: 1m\tremaining: 4m 18s\n",
      "189:\tlearn: 0.0897974\ttotal: 1m\tremaining: 4m 18s\n",
      "190:\tlearn: 0.0897499\ttotal: 1m\tremaining: 4m 17s\n",
      "191:\tlearn: 0.0896921\ttotal: 1m 1s\tremaining: 4m 17s\n",
      "192:\tlearn: 0.0896536\ttotal: 1m 1s\tremaining: 4m 17s\n",
      "193:\tlearn: 0.0895950\ttotal: 1m 1s\tremaining: 4m 16s\n",
      "194:\tlearn: 0.0895307\ttotal: 1m 2s\tremaining: 4m 16s\n",
      "195:\tlearn: 0.0894987\ttotal: 1m 2s\tremaining: 4m 16s\n",
      "196:\tlearn: 0.0894653\ttotal: 1m 2s\tremaining: 4m 15s\n",
      "197:\tlearn: 0.0894154\ttotal: 1m 3s\tremaining: 4m 15s\n",
      "198:\tlearn: 0.0893748\ttotal: 1m 3s\tremaining: 4m 14s\n",
      "199:\tlearn: 0.0893194\ttotal: 1m 3s\tremaining: 4m 14s\n",
      "200:\tlearn: 0.0892788\ttotal: 1m 3s\tremaining: 4m 14s\n",
      "201:\tlearn: 0.0892137\ttotal: 1m 4s\tremaining: 4m 13s\n",
      "202:\tlearn: 0.0891787\ttotal: 1m 4s\tremaining: 4m 13s\n",
      "203:\tlearn: 0.0891295\ttotal: 1m 4s\tremaining: 4m 13s\n",
      "204:\tlearn: 0.0891039\ttotal: 1m 5s\tremaining: 4m 12s\n",
      "205:\tlearn: 0.0890402\ttotal: 1m 5s\tremaining: 4m 12s\n",
      "206:\tlearn: 0.0889783\ttotal: 1m 5s\tremaining: 4m 11s\n",
      "207:\tlearn: 0.0889339\ttotal: 1m 6s\tremaining: 4m 11s\n",
      "208:\tlearn: 0.0888523\ttotal: 1m 6s\tremaining: 4m 11s\n",
      "209:\tlearn: 0.0887941\ttotal: 1m 6s\tremaining: 4m 10s\n",
      "210:\tlearn: 0.0887590\ttotal: 1m 7s\tremaining: 4m 10s\n",
      "211:\tlearn: 0.0887037\ttotal: 1m 7s\tremaining: 4m 10s\n",
      "212:\tlearn: 0.0886338\ttotal: 1m 7s\tremaining: 4m 10s\n",
      "213:\tlearn: 0.0885556\ttotal: 1m 7s\tremaining: 4m 9s\n",
      "214:\tlearn: 0.0885161\ttotal: 1m 8s\tremaining: 4m 9s\n",
      "215:\tlearn: 0.0884741\ttotal: 1m 8s\tremaining: 4m 9s\n",
      "216:\tlearn: 0.0884090\ttotal: 1m 8s\tremaining: 4m 8s\n",
      "217:\tlearn: 0.0883460\ttotal: 1m 9s\tremaining: 4m 8s\n",
      "218:\tlearn: 0.0883132\ttotal: 1m 9s\tremaining: 4m 7s\n",
      "219:\tlearn: 0.0882773\ttotal: 1m 9s\tremaining: 4m 7s\n",
      "220:\tlearn: 0.0882310\ttotal: 1m 10s\tremaining: 4m 7s\n",
      "221:\tlearn: 0.0881979\ttotal: 1m 10s\tremaining: 4m 6s\n",
      "222:\tlearn: 0.0881640\ttotal: 1m 10s\tremaining: 4m 6s\n",
      "223:\tlearn: 0.0881153\ttotal: 1m 11s\tremaining: 4m 6s\n",
      "224:\tlearn: 0.0880801\ttotal: 1m 11s\tremaining: 4m 5s\n",
      "225:\tlearn: 0.0880562\ttotal: 1m 11s\tremaining: 4m 5s\n",
      "226:\tlearn: 0.0880244\ttotal: 1m 12s\tremaining: 4m 5s\n",
      "227:\tlearn: 0.0879882\ttotal: 1m 12s\tremaining: 4m 4s\n",
      "228:\tlearn: 0.0879450\ttotal: 1m 12s\tremaining: 4m 4s\n",
      "229:\tlearn: 0.0879017\ttotal: 1m 12s\tremaining: 4m 4s\n",
      "230:\tlearn: 0.0878165\ttotal: 1m 13s\tremaining: 4m 3s\n",
      "231:\tlearn: 0.0877661\ttotal: 1m 13s\tremaining: 4m 3s\n",
      "232:\tlearn: 0.0876875\ttotal: 1m 13s\tremaining: 4m 3s\n",
      "233:\tlearn: 0.0876512\ttotal: 1m 14s\tremaining: 4m 2s\n",
      "234:\tlearn: 0.0875811\ttotal: 1m 14s\tremaining: 4m 2s\n",
      "235:\tlearn: 0.0875376\ttotal: 1m 14s\tremaining: 4m 2s\n",
      "236:\tlearn: 0.0874961\ttotal: 1m 15s\tremaining: 4m 1s\n",
      "237:\tlearn: 0.0874320\ttotal: 1m 15s\tremaining: 4m 1s\n",
      "238:\tlearn: 0.0873605\ttotal: 1m 15s\tremaining: 4m\n",
      "239:\tlearn: 0.0873279\ttotal: 1m 15s\tremaining: 4m\n",
      "240:\tlearn: 0.0873012\ttotal: 1m 16s\tremaining: 4m\n",
      "241:\tlearn: 0.0872701\ttotal: 1m 16s\tremaining: 3m 59s\n",
      "242:\tlearn: 0.0872492\ttotal: 1m 16s\tremaining: 3m 59s\n",
      "243:\tlearn: 0.0871219\ttotal: 1m 17s\tremaining: 3m 59s\n",
      "244:\tlearn: 0.0870777\ttotal: 1m 17s\tremaining: 3m 58s\n",
      "245:\tlearn: 0.0870349\ttotal: 1m 17s\tremaining: 3m 58s\n",
      "246:\tlearn: 0.0869994\ttotal: 1m 18s\tremaining: 3m 58s\n",
      "247:\tlearn: 0.0869484\ttotal: 1m 18s\tremaining: 3m 57s\n",
      "248:\tlearn: 0.0869068\ttotal: 1m 18s\tremaining: 3m 57s\n",
      "249:\tlearn: 0.0868761\ttotal: 1m 19s\tremaining: 3m 57s\n",
      "250:\tlearn: 0.0868341\ttotal: 1m 19s\tremaining: 3m 56s\n",
      "251:\tlearn: 0.0867582\ttotal: 1m 19s\tremaining: 3m 56s\n",
      "252:\tlearn: 0.0866918\ttotal: 1m 20s\tremaining: 3m 56s\n",
      "253:\tlearn: 0.0866475\ttotal: 1m 20s\tremaining: 3m 56s\n",
      "254:\tlearn: 0.0865862\ttotal: 1m 20s\tremaining: 3m 55s\n",
      "255:\tlearn: 0.0865692\ttotal: 1m 21s\tremaining: 3m 55s\n",
      "256:\tlearn: 0.0865112\ttotal: 1m 21s\tremaining: 3m 55s\n",
      "257:\tlearn: 0.0864458\ttotal: 1m 21s\tremaining: 3m 54s\n",
      "258:\tlearn: 0.0864300\ttotal: 1m 21s\tremaining: 3m 54s\n",
      "259:\tlearn: 0.0864030\ttotal: 1m 22s\tremaining: 3m 54s\n",
      "260:\tlearn: 0.0863625\ttotal: 1m 22s\tremaining: 3m 53s\n",
      "261:\tlearn: 0.0863074\ttotal: 1m 22s\tremaining: 3m 53s\n",
      "262:\tlearn: 0.0862669\ttotal: 1m 23s\tremaining: 3m 53s\n",
      "263:\tlearn: 0.0862438\ttotal: 1m 23s\tremaining: 3m 52s\n",
      "264:\tlearn: 0.0862160\ttotal: 1m 23s\tremaining: 3m 52s\n",
      "265:\tlearn: 0.0861603\ttotal: 1m 24s\tremaining: 3m 51s\n",
      "266:\tlearn: 0.0861325\ttotal: 1m 24s\tremaining: 3m 51s\n",
      "267:\tlearn: 0.0860935\ttotal: 1m 24s\tremaining: 3m 51s\n",
      "268:\tlearn: 0.0860231\ttotal: 1m 25s\tremaining: 3m 51s\n",
      "269:\tlearn: 0.0859751\ttotal: 1m 25s\tremaining: 3m 50s\n",
      "270:\tlearn: 0.0859502\ttotal: 1m 25s\tremaining: 3m 50s\n",
      "271:\tlearn: 0.0859159\ttotal: 1m 25s\tremaining: 3m 49s\n",
      "272:\tlearn: 0.0858580\ttotal: 1m 26s\tremaining: 3m 49s\n",
      "273:\tlearn: 0.0858009\ttotal: 1m 26s\tremaining: 3m 49s\n",
      "274:\tlearn: 0.0857689\ttotal: 1m 26s\tremaining: 3m 48s\n",
      "275:\tlearn: 0.0857399\ttotal: 1m 27s\tremaining: 3m 48s\n",
      "276:\tlearn: 0.0857126\ttotal: 1m 27s\tremaining: 3m 48s\n",
      "277:\tlearn: 0.0856824\ttotal: 1m 27s\tremaining: 3m 47s\n",
      "278:\tlearn: 0.0856284\ttotal: 1m 28s\tremaining: 3m 47s\n",
      "279:\tlearn: 0.0856047\ttotal: 1m 28s\tremaining: 3m 47s\n",
      "280:\tlearn: 0.0855838\ttotal: 1m 28s\tremaining: 3m 46s\n",
      "281:\tlearn: 0.0855440\ttotal: 1m 28s\tremaining: 3m 46s\n",
      "282:\tlearn: 0.0855050\ttotal: 1m 29s\tremaining: 3m 46s\n",
      "283:\tlearn: 0.0854772\ttotal: 1m 29s\tremaining: 3m 45s\n",
      "284:\tlearn: 0.0854108\ttotal: 1m 29s\tremaining: 3m 45s\n",
      "285:\tlearn: 0.0853882\ttotal: 1m 30s\tremaining: 3m 45s\n",
      "286:\tlearn: 0.0853485\ttotal: 1m 30s\tremaining: 3m 45s\n",
      "287:\tlearn: 0.0853203\ttotal: 1m 31s\tremaining: 3m 45s\n",
      "288:\tlearn: 0.0852803\ttotal: 1m 31s\tremaining: 3m 45s\n",
      "289:\tlearn: 0.0852454\ttotal: 1m 31s\tremaining: 3m 44s\n",
      "290:\tlearn: 0.0852053\ttotal: 1m 32s\tremaining: 3m 44s\n",
      "291:\tlearn: 0.0851786\ttotal: 1m 32s\tremaining: 3m 44s\n",
      "292:\tlearn: 0.0851509\ttotal: 1m 32s\tremaining: 3m 43s\n",
      "293:\tlearn: 0.0850985\ttotal: 1m 33s\tremaining: 3m 43s\n",
      "294:\tlearn: 0.0850793\ttotal: 1m 33s\tremaining: 3m 43s\n",
      "295:\tlearn: 0.0850335\ttotal: 1m 33s\tremaining: 3m 43s\n",
      "296:\tlearn: 0.0849568\ttotal: 1m 34s\tremaining: 3m 42s\n",
      "297:\tlearn: 0.0849375\ttotal: 1m 34s\tremaining: 3m 42s\n",
      "298:\tlearn: 0.0848901\ttotal: 1m 34s\tremaining: 3m 41s\n",
      "299:\tlearn: 0.0848639\ttotal: 1m 34s\tremaining: 3m 41s\n",
      "300:\tlearn: 0.0848309\ttotal: 1m 35s\tremaining: 3m 41s\n",
      "301:\tlearn: 0.0847785\ttotal: 1m 35s\tremaining: 3m 40s\n",
      "302:\tlearn: 0.0847496\ttotal: 1m 35s\tremaining: 3m 40s\n",
      "303:\tlearn: 0.0847117\ttotal: 1m 36s\tremaining: 3m 40s\n",
      "304:\tlearn: 0.0846807\ttotal: 1m 36s\tremaining: 3m 39s\n",
      "305:\tlearn: 0.0846556\ttotal: 1m 36s\tremaining: 3m 39s\n",
      "306:\tlearn: 0.0846126\ttotal: 1m 37s\tremaining: 3m 39s\n",
      "307:\tlearn: 0.0845676\ttotal: 1m 37s\tremaining: 3m 38s\n",
      "308:\tlearn: 0.0845481\ttotal: 1m 37s\tremaining: 3m 38s\n",
      "309:\tlearn: 0.0845141\ttotal: 1m 38s\tremaining: 3m 38s\n",
      "310:\tlearn: 0.0844635\ttotal: 1m 38s\tremaining: 3m 37s\n",
      "311:\tlearn: 0.0844374\ttotal: 1m 38s\tremaining: 3m 37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312:\tlearn: 0.0844044\ttotal: 1m 38s\tremaining: 3m 37s\n",
      "313:\tlearn: 0.0843692\ttotal: 1m 39s\tremaining: 3m 36s\n",
      "314:\tlearn: 0.0843185\ttotal: 1m 39s\tremaining: 3m 36s\n",
      "315:\tlearn: 0.0842806\ttotal: 1m 39s\tremaining: 3m 36s\n",
      "316:\tlearn: 0.0842354\ttotal: 1m 40s\tremaining: 3m 36s\n",
      "317:\tlearn: 0.0841945\ttotal: 1m 40s\tremaining: 3m 35s\n",
      "318:\tlearn: 0.0841743\ttotal: 1m 40s\tremaining: 3m 35s\n",
      "319:\tlearn: 0.0841561\ttotal: 1m 41s\tremaining: 3m 35s\n",
      "320:\tlearn: 0.0841300\ttotal: 1m 41s\tremaining: 3m 34s\n",
      "321:\tlearn: 0.0840826\ttotal: 1m 41s\tremaining: 3m 34s\n",
      "322:\tlearn: 0.0840686\ttotal: 1m 42s\tremaining: 3m 34s\n",
      "323:\tlearn: 0.0840550\ttotal: 1m 42s\tremaining: 3m 33s\n",
      "324:\tlearn: 0.0840127\ttotal: 1m 42s\tremaining: 3m 33s\n",
      "325:\tlearn: 0.0839863\ttotal: 1m 43s\tremaining: 3m 33s\n",
      "326:\tlearn: 0.0839557\ttotal: 1m 43s\tremaining: 3m 32s\n",
      "327:\tlearn: 0.0839406\ttotal: 1m 43s\tremaining: 3m 32s\n",
      "328:\tlearn: 0.0839086\ttotal: 1m 44s\tremaining: 3m 32s\n",
      "329:\tlearn: 0.0838734\ttotal: 1m 44s\tremaining: 3m 31s\n",
      "330:\tlearn: 0.0838334\ttotal: 1m 44s\tremaining: 3m 31s\n",
      "331:\tlearn: 0.0838018\ttotal: 1m 45s\tremaining: 3m 31s\n",
      "332:\tlearn: 0.0837677\ttotal: 1m 45s\tremaining: 3m 31s\n",
      "333:\tlearn: 0.0837445\ttotal: 1m 45s\tremaining: 3m 30s\n",
      "334:\tlearn: 0.0837094\ttotal: 1m 45s\tremaining: 3m 30s\n",
      "335:\tlearn: 0.0836836\ttotal: 1m 46s\tremaining: 3m 30s\n",
      "336:\tlearn: 0.0836615\ttotal: 1m 46s\tremaining: 3m 29s\n",
      "337:\tlearn: 0.0836310\ttotal: 1m 46s\tremaining: 3m 29s\n",
      "338:\tlearn: 0.0836071\ttotal: 1m 47s\tremaining: 3m 28s\n",
      "339:\tlearn: 0.0835692\ttotal: 1m 47s\tremaining: 3m 28s\n",
      "340:\tlearn: 0.0835449\ttotal: 1m 47s\tremaining: 3m 28s\n",
      "341:\tlearn: 0.0835166\ttotal: 1m 48s\tremaining: 3m 28s\n",
      "342:\tlearn: 0.0834484\ttotal: 1m 48s\tremaining: 3m 27s\n",
      "343:\tlearn: 0.0834249\ttotal: 1m 48s\tremaining: 3m 27s\n",
      "344:\tlearn: 0.0833897\ttotal: 1m 49s\tremaining: 3m 27s\n",
      "345:\tlearn: 0.0833532\ttotal: 1m 49s\tremaining: 3m 26s\n",
      "346:\tlearn: 0.0833325\ttotal: 1m 49s\tremaining: 3m 26s\n",
      "347:\tlearn: 0.0832914\ttotal: 1m 50s\tremaining: 3m 26s\n",
      "348:\tlearn: 0.0832662\ttotal: 1m 50s\tremaining: 3m 25s\n",
      "349:\tlearn: 0.0832391\ttotal: 1m 50s\tremaining: 3m 25s\n",
      "350:\tlearn: 0.0832001\ttotal: 1m 50s\tremaining: 3m 25s\n",
      "351:\tlearn: 0.0831878\ttotal: 1m 51s\tremaining: 3m 24s\n",
      "352:\tlearn: 0.0831452\ttotal: 1m 51s\tremaining: 3m 24s\n",
      "353:\tlearn: 0.0831171\ttotal: 1m 51s\tremaining: 3m 24s\n",
      "354:\tlearn: 0.0830851\ttotal: 1m 52s\tremaining: 3m 23s\n",
      "355:\tlearn: 0.0830515\ttotal: 1m 52s\tremaining: 3m 23s\n",
      "356:\tlearn: 0.0830247\ttotal: 1m 52s\tremaining: 3m 23s\n",
      "357:\tlearn: 0.0829936\ttotal: 1m 53s\tremaining: 3m 22s\n",
      "358:\tlearn: 0.0829701\ttotal: 1m 53s\tremaining: 3m 22s\n",
      "359:\tlearn: 0.0829361\ttotal: 1m 53s\tremaining: 3m 22s\n",
      "360:\tlearn: 0.0829002\ttotal: 1m 54s\tremaining: 3m 21s\n",
      "361:\tlearn: 0.0828611\ttotal: 1m 54s\tremaining: 3m 21s\n",
      "362:\tlearn: 0.0828332\ttotal: 1m 54s\tremaining: 3m 21s\n",
      "363:\tlearn: 0.0828142\ttotal: 1m 54s\tremaining: 3m 20s\n",
      "364:\tlearn: 0.0827825\ttotal: 1m 55s\tremaining: 3m 20s\n",
      "365:\tlearn: 0.0827540\ttotal: 1m 55s\tremaining: 3m 20s\n",
      "366:\tlearn: 0.0827441\ttotal: 1m 55s\tremaining: 3m 19s\n",
      "367:\tlearn: 0.0827101\ttotal: 1m 56s\tremaining: 3m 19s\n",
      "368:\tlearn: 0.0826902\ttotal: 1m 56s\tremaining: 3m 19s\n",
      "369:\tlearn: 0.0826703\ttotal: 1m 56s\tremaining: 3m 18s\n",
      "370:\tlearn: 0.0826496\ttotal: 1m 57s\tremaining: 3m 18s\n",
      "371:\tlearn: 0.0826268\ttotal: 1m 57s\tremaining: 3m 18s\n",
      "372:\tlearn: 0.0826022\ttotal: 1m 57s\tremaining: 3m 17s\n",
      "373:\tlearn: 0.0825839\ttotal: 1m 57s\tremaining: 3m 17s\n",
      "374:\tlearn: 0.0825480\ttotal: 1m 58s\tremaining: 3m 17s\n",
      "375:\tlearn: 0.0825223\ttotal: 1m 58s\tremaining: 3m 16s\n",
      "376:\tlearn: 0.0824949\ttotal: 1m 58s\tremaining: 3m 16s\n",
      "377:\tlearn: 0.0824485\ttotal: 1m 59s\tremaining: 3m 16s\n",
      "378:\tlearn: 0.0824144\ttotal: 1m 59s\tremaining: 3m 15s\n",
      "379:\tlearn: 0.0823869\ttotal: 1m 59s\tremaining: 3m 15s\n",
      "380:\tlearn: 0.0823613\ttotal: 2m\tremaining: 3m 15s\n",
      "381:\tlearn: 0.0823353\ttotal: 2m\tremaining: 3m 14s\n",
      "382:\tlearn: 0.0823136\ttotal: 2m\tremaining: 3m 14s\n",
      "383:\tlearn: 0.0822716\ttotal: 2m 1s\tremaining: 3m 14s\n",
      "384:\tlearn: 0.0822526\ttotal: 2m 1s\tremaining: 3m 13s\n",
      "385:\tlearn: 0.0822308\ttotal: 2m 1s\tremaining: 3m 13s\n",
      "386:\tlearn: 0.0822126\ttotal: 2m 1s\tremaining: 3m 13s\n",
      "387:\tlearn: 0.0821695\ttotal: 2m 2s\tremaining: 3m 12s\n",
      "388:\tlearn: 0.0821485\ttotal: 2m 2s\tremaining: 3m 12s\n",
      "389:\tlearn: 0.0821080\ttotal: 2m 2s\tremaining: 3m 11s\n",
      "390:\tlearn: 0.0820635\ttotal: 2m 3s\tremaining: 3m 11s\n",
      "391:\tlearn: 0.0820154\ttotal: 2m 3s\tremaining: 3m 11s\n",
      "392:\tlearn: 0.0819426\ttotal: 2m 3s\tremaining: 3m 11s\n",
      "393:\tlearn: 0.0819107\ttotal: 2m 4s\tremaining: 3m 10s\n",
      "394:\tlearn: 0.0818947\ttotal: 2m 4s\tremaining: 3m 10s\n",
      "395:\tlearn: 0.0818721\ttotal: 2m 4s\tremaining: 3m 10s\n",
      "396:\tlearn: 0.0818058\ttotal: 2m 5s\tremaining: 3m 10s\n",
      "397:\tlearn: 0.0817896\ttotal: 2m 5s\tremaining: 3m 9s\n",
      "398:\tlearn: 0.0817723\ttotal: 2m 5s\tremaining: 3m 9s\n",
      "399:\tlearn: 0.0817560\ttotal: 2m 5s\tremaining: 3m 8s\n",
      "400:\tlearn: 0.0817263\ttotal: 2m 6s\tremaining: 3m 8s\n",
      "401:\tlearn: 0.0817080\ttotal: 2m 6s\tremaining: 3m 8s\n",
      "402:\tlearn: 0.0816845\ttotal: 2m 6s\tremaining: 3m 8s\n",
      "403:\tlearn: 0.0816553\ttotal: 2m 7s\tremaining: 3m 7s\n",
      "404:\tlearn: 0.0816339\ttotal: 2m 7s\tremaining: 3m 7s\n",
      "405:\tlearn: 0.0815974\ttotal: 2m 7s\tremaining: 3m 7s\n",
      "406:\tlearn: 0.0815692\ttotal: 2m 8s\tremaining: 3m 6s\n",
      "407:\tlearn: 0.0815520\ttotal: 2m 8s\tremaining: 3m 6s\n",
      "408:\tlearn: 0.0815228\ttotal: 2m 8s\tremaining: 3m 6s\n",
      "409:\tlearn: 0.0815016\ttotal: 2m 9s\tremaining: 3m 5s\n",
      "410:\tlearn: 0.0814711\ttotal: 2m 9s\tremaining: 3m 5s\n",
      "411:\tlearn: 0.0814351\ttotal: 2m 9s\tremaining: 3m 5s\n",
      "412:\tlearn: 0.0814074\ttotal: 2m 10s\tremaining: 3m 4s\n",
      "413:\tlearn: 0.0813860\ttotal: 2m 10s\tremaining: 3m 4s\n",
      "414:\tlearn: 0.0813613\ttotal: 2m 10s\tremaining: 3m 4s\n",
      "415:\tlearn: 0.0813312\ttotal: 2m 10s\tremaining: 3m 3s\n",
      "416:\tlearn: 0.0813050\ttotal: 2m 11s\tremaining: 3m 3s\n",
      "417:\tlearn: 0.0812644\ttotal: 2m 11s\tremaining: 3m 3s\n",
      "418:\tlearn: 0.0812420\ttotal: 2m 11s\tremaining: 3m 2s\n",
      "419:\tlearn: 0.0812279\ttotal: 2m 12s\tremaining: 3m 2s\n",
      "420:\tlearn: 0.0811835\ttotal: 2m 12s\tremaining: 3m 2s\n",
      "421:\tlearn: 0.0811539\ttotal: 2m 12s\tremaining: 3m 1s\n",
      "422:\tlearn: 0.0811211\ttotal: 2m 13s\tremaining: 3m 1s\n",
      "423:\tlearn: 0.0810996\ttotal: 2m 13s\tremaining: 3m 1s\n",
      "424:\tlearn: 0.0810721\ttotal: 2m 13s\tremaining: 3m\n",
      "425:\tlearn: 0.0810278\ttotal: 2m 14s\tremaining: 3m\n",
      "426:\tlearn: 0.0809946\ttotal: 2m 14s\tremaining: 3m\n",
      "427:\tlearn: 0.0809680\ttotal: 2m 14s\tremaining: 3m\n",
      "428:\tlearn: 0.0809534\ttotal: 2m 15s\tremaining: 2m 59s\n",
      "429:\tlearn: 0.0809316\ttotal: 2m 15s\tremaining: 2m 59s\n",
      "430:\tlearn: 0.0809125\ttotal: 2m 15s\tremaining: 2m 59s\n",
      "431:\tlearn: 0.0808889\ttotal: 2m 15s\tremaining: 2m 58s\n",
      "432:\tlearn: 0.0808628\ttotal: 2m 16s\tremaining: 2m 58s\n",
      "433:\tlearn: 0.0808472\ttotal: 2m 16s\tremaining: 2m 58s\n",
      "434:\tlearn: 0.0808308\ttotal: 2m 16s\tremaining: 2m 57s\n",
      "435:\tlearn: 0.0808030\ttotal: 2m 17s\tremaining: 2m 57s\n",
      "436:\tlearn: 0.0807661\ttotal: 2m 17s\tremaining: 2m 57s\n",
      "437:\tlearn: 0.0807375\ttotal: 2m 17s\tremaining: 2m 56s\n",
      "438:\tlearn: 0.0807033\ttotal: 2m 18s\tremaining: 2m 56s\n",
      "439:\tlearn: 0.0806846\ttotal: 2m 18s\tremaining: 2m 56s\n",
      "440:\tlearn: 0.0806755\ttotal: 2m 18s\tremaining: 2m 55s\n",
      "441:\tlearn: 0.0806292\ttotal: 2m 19s\tremaining: 2m 55s\n",
      "442:\tlearn: 0.0806149\ttotal: 2m 19s\tremaining: 2m 55s\n",
      "443:\tlearn: 0.0805761\ttotal: 2m 19s\tremaining: 2m 54s\n",
      "444:\tlearn: 0.0805267\ttotal: 2m 19s\tremaining: 2m 54s\n",
      "445:\tlearn: 0.0805075\ttotal: 2m 20s\tremaining: 2m 54s\n",
      "446:\tlearn: 0.0804859\ttotal: 2m 20s\tremaining: 2m 53s\n",
      "447:\tlearn: 0.0804746\ttotal: 2m 20s\tremaining: 2m 53s\n",
      "448:\tlearn: 0.0804070\ttotal: 2m 21s\tremaining: 2m 53s\n",
      "449:\tlearn: 0.0803935\ttotal: 2m 21s\tremaining: 2m 52s\n",
      "450:\tlearn: 0.0803613\ttotal: 2m 21s\tremaining: 2m 52s\n",
      "451:\tlearn: 0.0803226\ttotal: 2m 22s\tremaining: 2m 52s\n",
      "452:\tlearn: 0.0802888\ttotal: 2m 22s\tremaining: 2m 52s\n",
      "453:\tlearn: 0.0802656\ttotal: 2m 22s\tremaining: 2m 51s\n",
      "454:\tlearn: 0.0802407\ttotal: 2m 23s\tremaining: 2m 51s\n",
      "455:\tlearn: 0.0802241\ttotal: 2m 23s\tremaining: 2m 51s\n",
      "456:\tlearn: 0.0801929\ttotal: 2m 23s\tremaining: 2m 50s\n",
      "457:\tlearn: 0.0801772\ttotal: 2m 24s\tremaining: 2m 50s\n",
      "458:\tlearn: 0.0801523\ttotal: 2m 24s\tremaining: 2m 50s\n",
      "459:\tlearn: 0.0801261\ttotal: 2m 24s\tremaining: 2m 49s\n",
      "460:\tlearn: 0.0801145\ttotal: 2m 24s\tremaining: 2m 49s\n",
      "461:\tlearn: 0.0800965\ttotal: 2m 25s\tremaining: 2m 49s\n",
      "462:\tlearn: 0.0800720\ttotal: 2m 25s\tremaining: 2m 48s\n",
      "463:\tlearn: 0.0800557\ttotal: 2m 25s\tremaining: 2m 48s\n",
      "464:\tlearn: 0.0800256\ttotal: 2m 26s\tremaining: 2m 48s\n",
      "465:\tlearn: 0.0800148\ttotal: 2m 26s\tremaining: 2m 47s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466:\tlearn: 0.0799899\ttotal: 2m 26s\tremaining: 2m 47s\n",
      "467:\tlearn: 0.0799699\ttotal: 2m 27s\tremaining: 2m 47s\n",
      "468:\tlearn: 0.0799450\ttotal: 2m 27s\tremaining: 2m 46s\n",
      "469:\tlearn: 0.0799129\ttotal: 2m 27s\tremaining: 2m 46s\n",
      "470:\tlearn: 0.0798889\ttotal: 2m 28s\tremaining: 2m 46s\n",
      "471:\tlearn: 0.0798511\ttotal: 2m 28s\tremaining: 2m 46s\n",
      "472:\tlearn: 0.0798338\ttotal: 2m 28s\tremaining: 2m 45s\n",
      "473:\tlearn: 0.0798186\ttotal: 2m 29s\tremaining: 2m 45s\n",
      "474:\tlearn: 0.0798012\ttotal: 2m 29s\tremaining: 2m 44s\n",
      "475:\tlearn: 0.0797888\ttotal: 2m 29s\tremaining: 2m 44s\n",
      "476:\tlearn: 0.0797528\ttotal: 2m 29s\tremaining: 2m 44s\n",
      "477:\tlearn: 0.0797231\ttotal: 2m 30s\tremaining: 2m 44s\n",
      "478:\tlearn: 0.0797154\ttotal: 2m 30s\tremaining: 2m 43s\n",
      "479:\tlearn: 0.0796784\ttotal: 2m 30s\tremaining: 2m 43s\n",
      "480:\tlearn: 0.0796586\ttotal: 2m 31s\tremaining: 2m 43s\n",
      "481:\tlearn: 0.0796431\ttotal: 2m 31s\tremaining: 2m 42s\n",
      "482:\tlearn: 0.0796085\ttotal: 2m 31s\tremaining: 2m 42s\n",
      "483:\tlearn: 0.0795816\ttotal: 2m 32s\tremaining: 2m 42s\n",
      "484:\tlearn: 0.0795675\ttotal: 2m 32s\tremaining: 2m 41s\n",
      "485:\tlearn: 0.0795474\ttotal: 2m 32s\tremaining: 2m 41s\n",
      "486:\tlearn: 0.0795355\ttotal: 2m 33s\tremaining: 2m 41s\n",
      "487:\tlearn: 0.0795221\ttotal: 2m 33s\tremaining: 2m 40s\n",
      "488:\tlearn: 0.0795056\ttotal: 2m 33s\tremaining: 2m 40s\n",
      "489:\tlearn: 0.0794947\ttotal: 2m 33s\tremaining: 2m 40s\n",
      "490:\tlearn: 0.0794658\ttotal: 2m 34s\tremaining: 2m 39s\n",
      "491:\tlearn: 0.0794458\ttotal: 2m 34s\tremaining: 2m 39s\n",
      "492:\tlearn: 0.0794256\ttotal: 2m 34s\tremaining: 2m 39s\n",
      "493:\tlearn: 0.0794130\ttotal: 2m 35s\tremaining: 2m 38s\n",
      "494:\tlearn: 0.0793803\ttotal: 2m 35s\tremaining: 2m 38s\n",
      "495:\tlearn: 0.0793631\ttotal: 2m 35s\tremaining: 2m 38s\n",
      "496:\tlearn: 0.0793277\ttotal: 2m 36s\tremaining: 2m 37s\n",
      "497:\tlearn: 0.0793017\ttotal: 2m 36s\tremaining: 2m 37s\n",
      "498:\tlearn: 0.0792708\ttotal: 2m 36s\tremaining: 2m 37s\n",
      "499:\tlearn: 0.0792551\ttotal: 2m 36s\tremaining: 2m 36s\n",
      "500:\tlearn: 0.0792339\ttotal: 2m 37s\tremaining: 2m 36s\n",
      "501:\tlearn: 0.0792172\ttotal: 2m 37s\tremaining: 2m 36s\n",
      "502:\tlearn: 0.0792027\ttotal: 2m 37s\tremaining: 2m 35s\n",
      "503:\tlearn: 0.0791704\ttotal: 2m 38s\tremaining: 2m 35s\n",
      "504:\tlearn: 0.0791317\ttotal: 2m 38s\tremaining: 2m 35s\n",
      "505:\tlearn: 0.0791118\ttotal: 2m 38s\tremaining: 2m 35s\n",
      "506:\tlearn: 0.0790885\ttotal: 2m 39s\tremaining: 2m 34s\n",
      "507:\tlearn: 0.0790580\ttotal: 2m 39s\tremaining: 2m 34s\n",
      "508:\tlearn: 0.0790341\ttotal: 2m 39s\tremaining: 2m 34s\n",
      "509:\tlearn: 0.0789981\ttotal: 2m 40s\tremaining: 2m 33s\n",
      "510:\tlearn: 0.0789347\ttotal: 2m 40s\tremaining: 2m 33s\n",
      "511:\tlearn: 0.0789222\ttotal: 2m 40s\tremaining: 2m 33s\n",
      "512:\tlearn: 0.0788930\ttotal: 2m 41s\tremaining: 2m 32s\n",
      "513:\tlearn: 0.0788657\ttotal: 2m 41s\tremaining: 2m 32s\n",
      "514:\tlearn: 0.0788234\ttotal: 2m 41s\tremaining: 2m 32s\n",
      "515:\tlearn: 0.0788027\ttotal: 2m 42s\tremaining: 2m 31s\n",
      "516:\tlearn: 0.0787953\ttotal: 2m 42s\tremaining: 2m 31s\n",
      "517:\tlearn: 0.0787843\ttotal: 2m 42s\tremaining: 2m 31s\n",
      "518:\tlearn: 0.0787334\ttotal: 2m 43s\tremaining: 2m 31s\n",
      "519:\tlearn: 0.0786892\ttotal: 2m 43s\tremaining: 2m 30s\n",
      "520:\tlearn: 0.0786665\ttotal: 2m 43s\tremaining: 2m 30s\n",
      "521:\tlearn: 0.0786374\ttotal: 2m 44s\tremaining: 2m 30s\n",
      "522:\tlearn: 0.0786104\ttotal: 2m 44s\tremaining: 2m 29s\n",
      "523:\tlearn: 0.0785865\ttotal: 2m 44s\tremaining: 2m 29s\n",
      "524:\tlearn: 0.0785535\ttotal: 2m 45s\tremaining: 2m 29s\n",
      "525:\tlearn: 0.0785171\ttotal: 2m 45s\tremaining: 2m 29s\n",
      "526:\tlearn: 0.0784959\ttotal: 2m 45s\tremaining: 2m 28s\n",
      "527:\tlearn: 0.0784512\ttotal: 2m 46s\tremaining: 2m 28s\n",
      "528:\tlearn: 0.0784395\ttotal: 2m 46s\tremaining: 2m 28s\n",
      "529:\tlearn: 0.0784158\ttotal: 2m 46s\tremaining: 2m 27s\n",
      "530:\tlearn: 0.0783895\ttotal: 2m 47s\tremaining: 2m 27s\n",
      "531:\tlearn: 0.0783723\ttotal: 2m 47s\tremaining: 2m 27s\n",
      "532:\tlearn: 0.0783620\ttotal: 2m 47s\tremaining: 2m 26s\n",
      "533:\tlearn: 0.0783462\ttotal: 2m 47s\tremaining: 2m 26s\n",
      "534:\tlearn: 0.0783108\ttotal: 2m 48s\tremaining: 2m 26s\n",
      "535:\tlearn: 0.0782850\ttotal: 2m 48s\tremaining: 2m 25s\n",
      "536:\tlearn: 0.0782630\ttotal: 2m 48s\tremaining: 2m 25s\n",
      "537:\tlearn: 0.0782307\ttotal: 2m 49s\tremaining: 2m 25s\n",
      "538:\tlearn: 0.0782025\ttotal: 2m 49s\tremaining: 2m 24s\n",
      "539:\tlearn: 0.0781920\ttotal: 2m 49s\tremaining: 2m 24s\n",
      "540:\tlearn: 0.0781642\ttotal: 2m 50s\tremaining: 2m 24s\n",
      "541:\tlearn: 0.0781415\ttotal: 2m 50s\tremaining: 2m 23s\n",
      "542:\tlearn: 0.0781248\ttotal: 2m 50s\tremaining: 2m 23s\n",
      "543:\tlearn: 0.0780918\ttotal: 2m 50s\tremaining: 2m 23s\n",
      "544:\tlearn: 0.0780817\ttotal: 2m 51s\tremaining: 2m 22s\n",
      "545:\tlearn: 0.0780735\ttotal: 2m 51s\tremaining: 2m 22s\n",
      "546:\tlearn: 0.0780579\ttotal: 2m 51s\tremaining: 2m 22s\n",
      "547:\tlearn: 0.0780299\ttotal: 2m 52s\tremaining: 2m 21s\n",
      "548:\tlearn: 0.0779934\ttotal: 2m 52s\tremaining: 2m 21s\n",
      "549:\tlearn: 0.0779780\ttotal: 2m 52s\tremaining: 2m 21s\n",
      "550:\tlearn: 0.0779666\ttotal: 2m 53s\tremaining: 2m 20s\n",
      "551:\tlearn: 0.0779260\ttotal: 2m 53s\tremaining: 2m 20s\n",
      "552:\tlearn: 0.0779080\ttotal: 2m 53s\tremaining: 2m 20s\n",
      "553:\tlearn: 0.0778911\ttotal: 2m 53s\tremaining: 2m 19s\n",
      "554:\tlearn: 0.0778414\ttotal: 2m 54s\tremaining: 2m 19s\n",
      "555:\tlearn: 0.0778207\ttotal: 2m 54s\tremaining: 2m 19s\n",
      "556:\tlearn: 0.0778084\ttotal: 2m 54s\tremaining: 2m 19s\n",
      "557:\tlearn: 0.0777979\ttotal: 2m 55s\tremaining: 2m 18s\n",
      "558:\tlearn: 0.0777839\ttotal: 2m 55s\tremaining: 2m 18s\n",
      "559:\tlearn: 0.0777552\ttotal: 2m 55s\tremaining: 2m 18s\n",
      "560:\tlearn: 0.0777402\ttotal: 2m 56s\tremaining: 2m 17s\n",
      "561:\tlearn: 0.0777158\ttotal: 2m 56s\tremaining: 2m 17s\n",
      "562:\tlearn: 0.0776964\ttotal: 2m 56s\tremaining: 2m 17s\n",
      "563:\tlearn: 0.0776803\ttotal: 2m 57s\tremaining: 2m 16s\n",
      "564:\tlearn: 0.0776589\ttotal: 2m 57s\tremaining: 2m 16s\n",
      "565:\tlearn: 0.0776266\ttotal: 2m 57s\tremaining: 2m 16s\n",
      "566:\tlearn: 0.0776071\ttotal: 2m 57s\tremaining: 2m 15s\n",
      "567:\tlearn: 0.0775864\ttotal: 2m 58s\tremaining: 2m 15s\n",
      "568:\tlearn: 0.0775640\ttotal: 2m 58s\tremaining: 2m 15s\n",
      "569:\tlearn: 0.0775497\ttotal: 2m 58s\tremaining: 2m 14s\n",
      "570:\tlearn: 0.0775282\ttotal: 2m 59s\tremaining: 2m 14s\n",
      "571:\tlearn: 0.0775158\ttotal: 2m 59s\tremaining: 2m 14s\n",
      "572:\tlearn: 0.0774884\ttotal: 2m 59s\tremaining: 2m 13s\n",
      "573:\tlearn: 0.0774778\ttotal: 3m\tremaining: 2m 13s\n",
      "574:\tlearn: 0.0774445\ttotal: 3m\tremaining: 2m 13s\n",
      "575:\tlearn: 0.0774042\ttotal: 3m\tremaining: 2m 12s\n",
      "576:\tlearn: 0.0773947\ttotal: 3m\tremaining: 2m 12s\n",
      "577:\tlearn: 0.0773277\ttotal: 3m 1s\tremaining: 2m 12s\n",
      "578:\tlearn: 0.0773075\ttotal: 3m 1s\tremaining: 2m 12s\n",
      "579:\tlearn: 0.0772658\ttotal: 3m 1s\tremaining: 2m 11s\n",
      "580:\tlearn: 0.0772484\ttotal: 3m 2s\tremaining: 2m 11s\n",
      "581:\tlearn: 0.0772205\ttotal: 3m 2s\tremaining: 2m 11s\n",
      "582:\tlearn: 0.0772102\ttotal: 3m 2s\tremaining: 2m 10s\n",
      "583:\tlearn: 0.0771856\ttotal: 3m 3s\tremaining: 2m 10s\n",
      "584:\tlearn: 0.0771515\ttotal: 3m 3s\tremaining: 2m 10s\n",
      "585:\tlearn: 0.0771321\ttotal: 3m 3s\tremaining: 2m 9s\n",
      "586:\tlearn: 0.0771137\ttotal: 3m 4s\tremaining: 2m 9s\n",
      "587:\tlearn: 0.0770633\ttotal: 3m 4s\tremaining: 2m 9s\n",
      "588:\tlearn: 0.0770340\ttotal: 3m 4s\tremaining: 2m 8s\n",
      "589:\tlearn: 0.0770121\ttotal: 3m 5s\tremaining: 2m 8s\n",
      "590:\tlearn: 0.0769903\ttotal: 3m 5s\tremaining: 2m 8s\n",
      "591:\tlearn: 0.0769724\ttotal: 3m 5s\tremaining: 2m 7s\n",
      "592:\tlearn: 0.0769474\ttotal: 3m 5s\tremaining: 2m 7s\n",
      "593:\tlearn: 0.0769290\ttotal: 3m 6s\tremaining: 2m 7s\n",
      "594:\tlearn: 0.0769137\ttotal: 3m 6s\tremaining: 2m 7s\n",
      "595:\tlearn: 0.0769017\ttotal: 3m 6s\tremaining: 2m 6s\n",
      "596:\tlearn: 0.0768858\ttotal: 3m 7s\tremaining: 2m 6s\n",
      "597:\tlearn: 0.0768604\ttotal: 3m 7s\tremaining: 2m 6s\n",
      "598:\tlearn: 0.0768362\ttotal: 3m 7s\tremaining: 2m 5s\n",
      "599:\tlearn: 0.0768103\ttotal: 3m 8s\tremaining: 2m 5s\n",
      "600:\tlearn: 0.0767785\ttotal: 3m 8s\tremaining: 2m 5s\n",
      "601:\tlearn: 0.0767642\ttotal: 3m 8s\tremaining: 2m 4s\n",
      "602:\tlearn: 0.0767263\ttotal: 3m 8s\tremaining: 2m 4s\n",
      "603:\tlearn: 0.0767029\ttotal: 3m 9s\tremaining: 2m 4s\n",
      "604:\tlearn: 0.0766914\ttotal: 3m 9s\tremaining: 2m 3s\n",
      "605:\tlearn: 0.0766623\ttotal: 3m 9s\tremaining: 2m 3s\n",
      "606:\tlearn: 0.0766502\ttotal: 3m 10s\tremaining: 2m 3s\n",
      "607:\tlearn: 0.0766185\ttotal: 3m 10s\tremaining: 2m 2s\n",
      "608:\tlearn: 0.0766030\ttotal: 3m 10s\tremaining: 2m 2s\n",
      "609:\tlearn: 0.0765748\ttotal: 3m 11s\tremaining: 2m 2s\n",
      "610:\tlearn: 0.0765606\ttotal: 3m 11s\tremaining: 2m 1s\n",
      "611:\tlearn: 0.0765372\ttotal: 3m 11s\tremaining: 2m 1s\n",
      "612:\tlearn: 0.0765230\ttotal: 3m 11s\tremaining: 2m 1s\n",
      "613:\tlearn: 0.0765104\ttotal: 3m 12s\tremaining: 2m\n",
      "614:\tlearn: 0.0764958\ttotal: 3m 12s\tremaining: 2m\n",
      "615:\tlearn: 0.0764642\ttotal: 3m 12s\tremaining: 2m\n",
      "616:\tlearn: 0.0764479\ttotal: 3m 13s\tremaining: 1m 59s\n",
      "617:\tlearn: 0.0764262\ttotal: 3m 13s\tremaining: 1m 59s\n",
      "618:\tlearn: 0.0764037\ttotal: 3m 13s\tremaining: 1m 59s\n",
      "619:\tlearn: 0.0763832\ttotal: 3m 14s\tremaining: 1m 59s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620:\tlearn: 0.0763476\ttotal: 3m 14s\tremaining: 1m 58s\n",
      "621:\tlearn: 0.0763372\ttotal: 3m 15s\tremaining: 1m 58s\n",
      "622:\tlearn: 0.0763160\ttotal: 3m 15s\tremaining: 1m 58s\n",
      "623:\tlearn: 0.0763007\ttotal: 3m 15s\tremaining: 1m 57s\n",
      "624:\tlearn: 0.0762689\ttotal: 3m 16s\tremaining: 1m 57s\n",
      "625:\tlearn: 0.0762461\ttotal: 3m 16s\tremaining: 1m 57s\n",
      "626:\tlearn: 0.0762234\ttotal: 3m 16s\tremaining: 1m 57s\n",
      "627:\tlearn: 0.0762133\ttotal: 3m 17s\tremaining: 1m 56s\n",
      "628:\tlearn: 0.0761985\ttotal: 3m 17s\tremaining: 1m 56s\n",
      "629:\tlearn: 0.0761839\ttotal: 3m 17s\tremaining: 1m 56s\n",
      "630:\tlearn: 0.0761715\ttotal: 3m 18s\tremaining: 1m 55s\n",
      "631:\tlearn: 0.0761558\ttotal: 3m 18s\tremaining: 1m 55s\n",
      "632:\tlearn: 0.0761360\ttotal: 3m 18s\tremaining: 1m 55s\n",
      "633:\tlearn: 0.0761214\ttotal: 3m 19s\tremaining: 1m 54s\n",
      "634:\tlearn: 0.0761065\ttotal: 3m 19s\tremaining: 1m 54s\n",
      "635:\tlearn: 0.0760884\ttotal: 3m 19s\tremaining: 1m 54s\n",
      "636:\tlearn: 0.0760679\ttotal: 3m 19s\tremaining: 1m 53s\n",
      "637:\tlearn: 0.0760430\ttotal: 3m 20s\tremaining: 1m 53s\n",
      "638:\tlearn: 0.0760097\ttotal: 3m 20s\tremaining: 1m 53s\n",
      "639:\tlearn: 0.0759751\ttotal: 3m 20s\tremaining: 1m 52s\n",
      "640:\tlearn: 0.0759523\ttotal: 3m 21s\tremaining: 1m 52s\n",
      "641:\tlearn: 0.0759336\ttotal: 3m 21s\tremaining: 1m 52s\n",
      "642:\tlearn: 0.0759151\ttotal: 3m 21s\tremaining: 1m 52s\n",
      "643:\tlearn: 0.0758864\ttotal: 3m 22s\tremaining: 1m 51s\n",
      "644:\tlearn: 0.0758406\ttotal: 3m 22s\tremaining: 1m 51s\n",
      "645:\tlearn: 0.0758293\ttotal: 3m 22s\tremaining: 1m 51s\n",
      "646:\tlearn: 0.0758119\ttotal: 3m 23s\tremaining: 1m 50s\n",
      "647:\tlearn: 0.0757970\ttotal: 3m 23s\tremaining: 1m 50s\n",
      "648:\tlearn: 0.0757819\ttotal: 3m 23s\tremaining: 1m 50s\n",
      "649:\tlearn: 0.0757629\ttotal: 3m 24s\tremaining: 1m 49s\n",
      "650:\tlearn: 0.0757497\ttotal: 3m 24s\tremaining: 1m 49s\n",
      "651:\tlearn: 0.0757279\ttotal: 3m 24s\tremaining: 1m 49s\n",
      "652:\tlearn: 0.0757075\ttotal: 3m 24s\tremaining: 1m 48s\n",
      "653:\tlearn: 0.0756874\ttotal: 3m 25s\tremaining: 1m 48s\n",
      "654:\tlearn: 0.0756562\ttotal: 3m 25s\tremaining: 1m 48s\n",
      "655:\tlearn: 0.0756405\ttotal: 3m 25s\tremaining: 1m 47s\n",
      "656:\tlearn: 0.0756167\ttotal: 3m 26s\tremaining: 1m 47s\n",
      "657:\tlearn: 0.0756089\ttotal: 3m 26s\tremaining: 1m 47s\n",
      "658:\tlearn: 0.0755962\ttotal: 3m 26s\tremaining: 1m 47s\n",
      "659:\tlearn: 0.0755801\ttotal: 3m 27s\tremaining: 1m 46s\n",
      "660:\tlearn: 0.0755632\ttotal: 3m 27s\tremaining: 1m 46s\n",
      "661:\tlearn: 0.0755538\ttotal: 3m 27s\tremaining: 1m 46s\n",
      "662:\tlearn: 0.0755370\ttotal: 3m 27s\tremaining: 1m 45s\n",
      "663:\tlearn: 0.0755225\ttotal: 3m 28s\tremaining: 1m 45s\n",
      "664:\tlearn: 0.0755077\ttotal: 3m 28s\tremaining: 1m 45s\n",
      "665:\tlearn: 0.0754673\ttotal: 3m 28s\tremaining: 1m 44s\n",
      "666:\tlearn: 0.0754600\ttotal: 3m 29s\tremaining: 1m 44s\n",
      "667:\tlearn: 0.0754455\ttotal: 3m 29s\tremaining: 1m 44s\n",
      "668:\tlearn: 0.0754273\ttotal: 3m 29s\tremaining: 1m 43s\n",
      "669:\tlearn: 0.0753922\ttotal: 3m 30s\tremaining: 1m 43s\n",
      "670:\tlearn: 0.0753716\ttotal: 3m 30s\tremaining: 1m 43s\n",
      "671:\tlearn: 0.0753615\ttotal: 3m 30s\tremaining: 1m 42s\n",
      "672:\tlearn: 0.0753526\ttotal: 3m 31s\tremaining: 1m 42s\n",
      "673:\tlearn: 0.0753366\ttotal: 3m 31s\tremaining: 1m 42s\n",
      "674:\tlearn: 0.0753213\ttotal: 3m 31s\tremaining: 1m 41s\n",
      "675:\tlearn: 0.0752845\ttotal: 3m 31s\tremaining: 1m 41s\n",
      "676:\tlearn: 0.0752623\ttotal: 3m 32s\tremaining: 1m 41s\n",
      "677:\tlearn: 0.0752426\ttotal: 3m 32s\tremaining: 1m 40s\n",
      "678:\tlearn: 0.0752113\ttotal: 3m 32s\tremaining: 1m 40s\n",
      "679:\tlearn: 0.0751962\ttotal: 3m 33s\tremaining: 1m 40s\n",
      "680:\tlearn: 0.0751785\ttotal: 3m 33s\tremaining: 1m 39s\n",
      "681:\tlearn: 0.0751668\ttotal: 3m 33s\tremaining: 1m 39s\n",
      "682:\tlearn: 0.0751517\ttotal: 3m 34s\tremaining: 1m 39s\n",
      "683:\tlearn: 0.0751239\ttotal: 3m 34s\tremaining: 1m 39s\n",
      "684:\tlearn: 0.0751121\ttotal: 3m 34s\tremaining: 1m 38s\n",
      "685:\tlearn: 0.0751037\ttotal: 3m 35s\tremaining: 1m 38s\n",
      "686:\tlearn: 0.0750905\ttotal: 3m 35s\tremaining: 1m 38s\n",
      "687:\tlearn: 0.0750529\ttotal: 3m 35s\tremaining: 1m 37s\n",
      "688:\tlearn: 0.0750341\ttotal: 3m 35s\tremaining: 1m 37s\n",
      "689:\tlearn: 0.0750173\ttotal: 3m 36s\tremaining: 1m 37s\n",
      "690:\tlearn: 0.0750089\ttotal: 3m 36s\tremaining: 1m 36s\n",
      "691:\tlearn: 0.0750027\ttotal: 3m 37s\tremaining: 1m 36s\n",
      "692:\tlearn: 0.0749916\ttotal: 3m 37s\tremaining: 1m 36s\n",
      "693:\tlearn: 0.0749600\ttotal: 3m 37s\tremaining: 1m 35s\n",
      "694:\tlearn: 0.0749519\ttotal: 3m 37s\tremaining: 1m 35s\n",
      "695:\tlearn: 0.0749323\ttotal: 3m 38s\tremaining: 1m 35s\n",
      "696:\tlearn: 0.0749119\ttotal: 3m 38s\tremaining: 1m 35s\n",
      "697:\tlearn: 0.0748941\ttotal: 3m 38s\tremaining: 1m 34s\n",
      "698:\tlearn: 0.0748636\ttotal: 3m 39s\tremaining: 1m 34s\n",
      "699:\tlearn: 0.0748503\ttotal: 3m 39s\tremaining: 1m 34s\n",
      "700:\tlearn: 0.0748259\ttotal: 3m 39s\tremaining: 1m 33s\n",
      "701:\tlearn: 0.0748133\ttotal: 3m 40s\tremaining: 1m 33s\n",
      "702:\tlearn: 0.0748058\ttotal: 3m 40s\tremaining: 1m 33s\n",
      "703:\tlearn: 0.0747902\ttotal: 3m 40s\tremaining: 1m 32s\n",
      "704:\tlearn: 0.0747671\ttotal: 3m 41s\tremaining: 1m 32s\n",
      "705:\tlearn: 0.0747432\ttotal: 3m 41s\tremaining: 1m 32s\n",
      "706:\tlearn: 0.0747190\ttotal: 3m 41s\tremaining: 1m 31s\n",
      "707:\tlearn: 0.0747106\ttotal: 3m 41s\tremaining: 1m 31s\n",
      "708:\tlearn: 0.0746939\ttotal: 3m 42s\tremaining: 1m 31s\n",
      "709:\tlearn: 0.0746794\ttotal: 3m 42s\tremaining: 1m 30s\n",
      "710:\tlearn: 0.0746539\ttotal: 3m 42s\tremaining: 1m 30s\n",
      "711:\tlearn: 0.0746405\ttotal: 3m 43s\tremaining: 1m 30s\n",
      "712:\tlearn: 0.0746196\ttotal: 3m 43s\tremaining: 1m 29s\n",
      "713:\tlearn: 0.0746063\ttotal: 3m 43s\tremaining: 1m 29s\n",
      "714:\tlearn: 0.0745824\ttotal: 3m 44s\tremaining: 1m 29s\n",
      "715:\tlearn: 0.0745678\ttotal: 3m 44s\tremaining: 1m 29s\n",
      "716:\tlearn: 0.0745528\ttotal: 3m 44s\tremaining: 1m 28s\n",
      "717:\tlearn: 0.0745427\ttotal: 3m 45s\tremaining: 1m 28s\n",
      "718:\tlearn: 0.0745270\ttotal: 3m 45s\tremaining: 1m 28s\n",
      "719:\tlearn: 0.0745067\ttotal: 3m 45s\tremaining: 1m 27s\n",
      "720:\tlearn: 0.0744895\ttotal: 3m 46s\tremaining: 1m 27s\n",
      "721:\tlearn: 0.0744682\ttotal: 3m 46s\tremaining: 1m 27s\n",
      "722:\tlearn: 0.0744389\ttotal: 3m 46s\tremaining: 1m 26s\n",
      "723:\tlearn: 0.0744226\ttotal: 3m 47s\tremaining: 1m 26s\n",
      "724:\tlearn: 0.0744022\ttotal: 3m 47s\tremaining: 1m 26s\n",
      "725:\tlearn: 0.0743644\ttotal: 3m 47s\tremaining: 1m 25s\n",
      "726:\tlearn: 0.0743282\ttotal: 3m 47s\tremaining: 1m 25s\n",
      "727:\tlearn: 0.0743182\ttotal: 3m 48s\tremaining: 1m 25s\n",
      "728:\tlearn: 0.0743051\ttotal: 3m 48s\tremaining: 1m 24s\n",
      "729:\tlearn: 0.0742887\ttotal: 3m 48s\tremaining: 1m 24s\n",
      "730:\tlearn: 0.0742685\ttotal: 3m 49s\tremaining: 1m 24s\n",
      "731:\tlearn: 0.0742525\ttotal: 3m 49s\tremaining: 1m 24s\n",
      "732:\tlearn: 0.0742370\ttotal: 3m 49s\tremaining: 1m 23s\n",
      "733:\tlearn: 0.0742257\ttotal: 3m 50s\tremaining: 1m 23s\n",
      "734:\tlearn: 0.0741991\ttotal: 3m 50s\tremaining: 1m 23s\n",
      "735:\tlearn: 0.0741835\ttotal: 3m 50s\tremaining: 1m 22s\n",
      "736:\tlearn: 0.0741523\ttotal: 3m 51s\tremaining: 1m 22s\n",
      "737:\tlearn: 0.0741393\ttotal: 3m 51s\tremaining: 1m 22s\n",
      "738:\tlearn: 0.0741147\ttotal: 3m 51s\tremaining: 1m 21s\n",
      "739:\tlearn: 0.0741068\ttotal: 3m 51s\tremaining: 1m 21s\n",
      "740:\tlearn: 0.0740754\ttotal: 3m 52s\tremaining: 1m 21s\n",
      "741:\tlearn: 0.0740638\ttotal: 3m 52s\tremaining: 1m 20s\n",
      "742:\tlearn: 0.0740414\ttotal: 3m 52s\tremaining: 1m 20s\n",
      "743:\tlearn: 0.0740235\ttotal: 3m 53s\tremaining: 1m 20s\n",
      "744:\tlearn: 0.0740048\ttotal: 3m 53s\tremaining: 1m 19s\n",
      "745:\tlearn: 0.0739757\ttotal: 3m 53s\tremaining: 1m 19s\n",
      "746:\tlearn: 0.0739624\ttotal: 3m 54s\tremaining: 1m 19s\n",
      "747:\tlearn: 0.0739528\ttotal: 3m 54s\tremaining: 1m 18s\n",
      "748:\tlearn: 0.0739416\ttotal: 3m 54s\tremaining: 1m 18s\n",
      "749:\tlearn: 0.0739344\ttotal: 3m 54s\tremaining: 1m 18s\n",
      "750:\tlearn: 0.0739225\ttotal: 3m 55s\tremaining: 1m 18s\n",
      "751:\tlearn: 0.0739102\ttotal: 3m 55s\tremaining: 1m 17s\n",
      "752:\tlearn: 0.0739041\ttotal: 3m 55s\tremaining: 1m 17s\n",
      "753:\tlearn: 0.0738861\ttotal: 3m 56s\tremaining: 1m 17s\n",
      "754:\tlearn: 0.0738478\ttotal: 3m 56s\tremaining: 1m 16s\n",
      "755:\tlearn: 0.0738273\ttotal: 3m 56s\tremaining: 1m 16s\n",
      "756:\tlearn: 0.0738068\ttotal: 3m 57s\tremaining: 1m 16s\n",
      "757:\tlearn: 0.0737878\ttotal: 3m 57s\tremaining: 1m 15s\n",
      "758:\tlearn: 0.0737745\ttotal: 3m 57s\tremaining: 1m 15s\n",
      "759:\tlearn: 0.0737491\ttotal: 3m 58s\tremaining: 1m 15s\n",
      "760:\tlearn: 0.0737278\ttotal: 3m 58s\tremaining: 1m 14s\n",
      "761:\tlearn: 0.0737149\ttotal: 3m 58s\tremaining: 1m 14s\n",
      "762:\tlearn: 0.0737025\ttotal: 3m 59s\tremaining: 1m 14s\n",
      "763:\tlearn: 0.0736956\ttotal: 3m 59s\tremaining: 1m 13s\n",
      "764:\tlearn: 0.0736836\ttotal: 3m 59s\tremaining: 1m 13s\n",
      "765:\tlearn: 0.0736500\ttotal: 3m 59s\tremaining: 1m 13s\n",
      "766:\tlearn: 0.0736323\ttotal: 4m\tremaining: 1m 12s\n",
      "767:\tlearn: 0.0736117\ttotal: 4m\tremaining: 1m 12s\n",
      "768:\tlearn: 0.0735779\ttotal: 4m\tremaining: 1m 12s\n",
      "769:\tlearn: 0.0735557\ttotal: 4m 1s\tremaining: 1m 12s\n",
      "770:\tlearn: 0.0735376\ttotal: 4m 1s\tremaining: 1m 11s\n",
      "771:\tlearn: 0.0735044\ttotal: 4m 1s\tremaining: 1m 11s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772:\tlearn: 0.0734768\ttotal: 4m 2s\tremaining: 1m 11s\n",
      "773:\tlearn: 0.0734581\ttotal: 4m 2s\tremaining: 1m 10s\n",
      "774:\tlearn: 0.0734359\ttotal: 4m 2s\tremaining: 1m 10s\n",
      "775:\tlearn: 0.0734109\ttotal: 4m 3s\tremaining: 1m 10s\n",
      "776:\tlearn: 0.0733840\ttotal: 4m 3s\tremaining: 1m 9s\n",
      "777:\tlearn: 0.0733713\ttotal: 4m 3s\tremaining: 1m 9s\n",
      "778:\tlearn: 0.0733591\ttotal: 4m 4s\tremaining: 1m 9s\n",
      "779:\tlearn: 0.0733451\ttotal: 4m 4s\tremaining: 1m 8s\n",
      "780:\tlearn: 0.0733305\ttotal: 4m 4s\tremaining: 1m 8s\n",
      "781:\tlearn: 0.0733156\ttotal: 4m 5s\tremaining: 1m 8s\n",
      "782:\tlearn: 0.0733001\ttotal: 4m 5s\tremaining: 1m 7s\n",
      "783:\tlearn: 0.0732899\ttotal: 4m 5s\tremaining: 1m 7s\n",
      "784:\tlearn: 0.0732702\ttotal: 4m 5s\tremaining: 1m 7s\n",
      "785:\tlearn: 0.0732414\ttotal: 4m 6s\tremaining: 1m 7s\n",
      "786:\tlearn: 0.0732256\ttotal: 4m 6s\tremaining: 1m 6s\n",
      "787:\tlearn: 0.0732101\ttotal: 4m 6s\tremaining: 1m 6s\n",
      "788:\tlearn: 0.0732007\ttotal: 4m 7s\tremaining: 1m 6s\n",
      "789:\tlearn: 0.0731655\ttotal: 4m 7s\tremaining: 1m 5s\n",
      "790:\tlearn: 0.0731553\ttotal: 4m 7s\tremaining: 1m 5s\n",
      "791:\tlearn: 0.0731473\ttotal: 4m 8s\tremaining: 1m 5s\n",
      "792:\tlearn: 0.0731197\ttotal: 4m 8s\tremaining: 1m 4s\n",
      "793:\tlearn: 0.0730565\ttotal: 4m 8s\tremaining: 1m 4s\n",
      "794:\tlearn: 0.0730462\ttotal: 4m 9s\tremaining: 1m 4s\n",
      "795:\tlearn: 0.0730271\ttotal: 4m 9s\tremaining: 1m 3s\n",
      "796:\tlearn: 0.0730096\ttotal: 4m 9s\tremaining: 1m 3s\n",
      "797:\tlearn: 0.0729865\ttotal: 4m 10s\tremaining: 1m 3s\n",
      "798:\tlearn: 0.0729608\ttotal: 4m 10s\tremaining: 1m 2s\n",
      "799:\tlearn: 0.0729499\ttotal: 4m 10s\tremaining: 1m 2s\n",
      "800:\tlearn: 0.0729258\ttotal: 4m 10s\tremaining: 1m 2s\n",
      "801:\tlearn: 0.0729202\ttotal: 4m 11s\tremaining: 1m 2s\n",
      "802:\tlearn: 0.0729027\ttotal: 4m 11s\tremaining: 1m 1s\n",
      "803:\tlearn: 0.0728689\ttotal: 4m 11s\tremaining: 1m 1s\n",
      "804:\tlearn: 0.0728427\ttotal: 4m 12s\tremaining: 1m 1s\n",
      "805:\tlearn: 0.0728292\ttotal: 4m 12s\tremaining: 1m\n",
      "806:\tlearn: 0.0728143\ttotal: 4m 12s\tremaining: 1m\n",
      "807:\tlearn: 0.0727759\ttotal: 4m 13s\tremaining: 1m\n",
      "808:\tlearn: 0.0727538\ttotal: 4m 13s\tremaining: 59.8s\n",
      "809:\tlearn: 0.0727401\ttotal: 4m 13s\tremaining: 59.5s\n",
      "810:\tlearn: 0.0727197\ttotal: 4m 14s\tremaining: 59.2s\n",
      "811:\tlearn: 0.0727027\ttotal: 4m 14s\tremaining: 58.9s\n",
      "812:\tlearn: 0.0726909\ttotal: 4m 14s\tremaining: 58.6s\n",
      "813:\tlearn: 0.0726806\ttotal: 4m 15s\tremaining: 58.3s\n",
      "814:\tlearn: 0.0726672\ttotal: 4m 15s\tremaining: 58s\n",
      "815:\tlearn: 0.0726586\ttotal: 4m 15s\tremaining: 57.6s\n",
      "816:\tlearn: 0.0726370\ttotal: 4m 15s\tremaining: 57.3s\n",
      "817:\tlearn: 0.0726174\ttotal: 4m 16s\tremaining: 57s\n",
      "818:\tlearn: 0.0726091\ttotal: 4m 16s\tremaining: 56.7s\n",
      "819:\tlearn: 0.0725984\ttotal: 4m 16s\tremaining: 56.4s\n",
      "820:\tlearn: 0.0725847\ttotal: 4m 17s\tremaining: 56.1s\n",
      "821:\tlearn: 0.0725625\ttotal: 4m 17s\tremaining: 55.8s\n",
      "822:\tlearn: 0.0725421\ttotal: 4m 17s\tremaining: 55.4s\n",
      "823:\tlearn: 0.0725221\ttotal: 4m 18s\tremaining: 55.1s\n",
      "824:\tlearn: 0.0725050\ttotal: 4m 18s\tremaining: 54.8s\n",
      "825:\tlearn: 0.0724616\ttotal: 4m 18s\tremaining: 54.5s\n",
      "826:\tlearn: 0.0724489\ttotal: 4m 19s\tremaining: 54.2s\n",
      "827:\tlearn: 0.0724389\ttotal: 4m 19s\tremaining: 53.9s\n",
      "828:\tlearn: 0.0724236\ttotal: 4m 19s\tremaining: 53.6s\n",
      "829:\tlearn: 0.0724131\ttotal: 4m 19s\tremaining: 53.3s\n",
      "830:\tlearn: 0.0724003\ttotal: 4m 20s\tremaining: 52.9s\n",
      "831:\tlearn: 0.0723779\ttotal: 4m 20s\tremaining: 52.6s\n",
      "832:\tlearn: 0.0723476\ttotal: 4m 20s\tremaining: 52.3s\n",
      "833:\tlearn: 0.0723323\ttotal: 4m 21s\tremaining: 52s\n",
      "834:\tlearn: 0.0723203\ttotal: 4m 21s\tremaining: 51.7s\n",
      "835:\tlearn: 0.0722972\ttotal: 4m 21s\tremaining: 51.4s\n",
      "836:\tlearn: 0.0722662\ttotal: 4m 22s\tremaining: 51s\n",
      "837:\tlearn: 0.0722519\ttotal: 4m 22s\tremaining: 50.7s\n",
      "838:\tlearn: 0.0722312\ttotal: 4m 22s\tremaining: 50.4s\n",
      "839:\tlearn: 0.0722066\ttotal: 4m 23s\tremaining: 50.1s\n",
      "840:\tlearn: 0.0721930\ttotal: 4m 23s\tremaining: 49.8s\n",
      "841:\tlearn: 0.0721747\ttotal: 4m 23s\tremaining: 49.5s\n",
      "842:\tlearn: 0.0721588\ttotal: 4m 23s\tremaining: 49.2s\n",
      "843:\tlearn: 0.0721479\ttotal: 4m 24s\tremaining: 48.8s\n",
      "844:\tlearn: 0.0721435\ttotal: 4m 24s\tremaining: 48.5s\n",
      "845:\tlearn: 0.0721258\ttotal: 4m 24s\tremaining: 48.2s\n",
      "846:\tlearn: 0.0721160\ttotal: 4m 25s\tremaining: 47.9s\n",
      "847:\tlearn: 0.0721048\ttotal: 4m 25s\tremaining: 47.6s\n",
      "848:\tlearn: 0.0720580\ttotal: 4m 25s\tremaining: 47.3s\n",
      "849:\tlearn: 0.0720418\ttotal: 4m 26s\tremaining: 47s\n",
      "850:\tlearn: 0.0720291\ttotal: 4m 26s\tremaining: 46.6s\n",
      "851:\tlearn: 0.0720116\ttotal: 4m 26s\tremaining: 46.3s\n",
      "852:\tlearn: 0.0719921\ttotal: 4m 27s\tremaining: 46s\n",
      "853:\tlearn: 0.0719714\ttotal: 4m 27s\tremaining: 45.7s\n",
      "854:\tlearn: 0.0719638\ttotal: 4m 27s\tremaining: 45.4s\n",
      "855:\tlearn: 0.0719602\ttotal: 4m 27s\tremaining: 45.1s\n",
      "856:\tlearn: 0.0719349\ttotal: 4m 28s\tremaining: 44.8s\n",
      "857:\tlearn: 0.0719216\ttotal: 4m 28s\tremaining: 44.4s\n",
      "858:\tlearn: 0.0719019\ttotal: 4m 28s\tremaining: 44.1s\n",
      "859:\tlearn: 0.0718956\ttotal: 4m 29s\tremaining: 43.8s\n",
      "860:\tlearn: 0.0718754\ttotal: 4m 29s\tremaining: 43.5s\n",
      "861:\tlearn: 0.0718578\ttotal: 4m 29s\tremaining: 43.2s\n",
      "862:\tlearn: 0.0718422\ttotal: 4m 30s\tremaining: 42.9s\n",
      "863:\tlearn: 0.0718283\ttotal: 4m 30s\tremaining: 42.6s\n",
      "864:\tlearn: 0.0718195\ttotal: 4m 30s\tremaining: 42.3s\n",
      "865:\tlearn: 0.0718057\ttotal: 4m 31s\tremaining: 41.9s\n",
      "866:\tlearn: 0.0717935\ttotal: 4m 31s\tremaining: 41.6s\n",
      "867:\tlearn: 0.0717827\ttotal: 4m 31s\tremaining: 41.3s\n",
      "868:\tlearn: 0.0717629\ttotal: 4m 31s\tremaining: 41s\n",
      "869:\tlearn: 0.0717457\ttotal: 4m 32s\tremaining: 40.7s\n",
      "870:\tlearn: 0.0717195\ttotal: 4m 32s\tremaining: 40.4s\n",
      "871:\tlearn: 0.0716793\ttotal: 4m 32s\tremaining: 40.1s\n",
      "872:\tlearn: 0.0716687\ttotal: 4m 33s\tremaining: 39.8s\n",
      "873:\tlearn: 0.0716608\ttotal: 4m 33s\tremaining: 39.4s\n",
      "874:\tlearn: 0.0716499\ttotal: 4m 33s\tremaining: 39.1s\n",
      "875:\tlearn: 0.0716292\ttotal: 4m 34s\tremaining: 38.8s\n",
      "876:\tlearn: 0.0716139\ttotal: 4m 34s\tremaining: 38.5s\n",
      "877:\tlearn: 0.0715911\ttotal: 4m 34s\tremaining: 38.2s\n",
      "878:\tlearn: 0.0715812\ttotal: 4m 35s\tremaining: 37.9s\n",
      "879:\tlearn: 0.0715615\ttotal: 4m 35s\tremaining: 37.6s\n",
      "880:\tlearn: 0.0715519\ttotal: 4m 35s\tremaining: 37.2s\n",
      "881:\tlearn: 0.0715405\ttotal: 4m 36s\tremaining: 36.9s\n",
      "882:\tlearn: 0.0715275\ttotal: 4m 36s\tremaining: 36.6s\n",
      "883:\tlearn: 0.0714991\ttotal: 4m 36s\tremaining: 36.3s\n",
      "884:\tlearn: 0.0714820\ttotal: 4m 36s\tremaining: 36s\n",
      "885:\tlearn: 0.0714646\ttotal: 4m 37s\tremaining: 35.7s\n",
      "886:\tlearn: 0.0714582\ttotal: 4m 37s\tremaining: 35.4s\n",
      "887:\tlearn: 0.0714486\ttotal: 4m 37s\tremaining: 35s\n",
      "888:\tlearn: 0.0714383\ttotal: 4m 38s\tremaining: 34.7s\n",
      "889:\tlearn: 0.0714175\ttotal: 4m 38s\tremaining: 34.4s\n",
      "890:\tlearn: 0.0713950\ttotal: 4m 38s\tremaining: 34.1s\n",
      "891:\tlearn: 0.0713801\ttotal: 4m 39s\tremaining: 33.8s\n",
      "892:\tlearn: 0.0713687\ttotal: 4m 39s\tremaining: 33.5s\n",
      "893:\tlearn: 0.0713600\ttotal: 4m 39s\tremaining: 33.2s\n",
      "894:\tlearn: 0.0713432\ttotal: 4m 40s\tremaining: 32.9s\n",
      "895:\tlearn: 0.0713313\ttotal: 4m 40s\tremaining: 32.5s\n",
      "896:\tlearn: 0.0713183\ttotal: 4m 40s\tremaining: 32.2s\n",
      "897:\tlearn: 0.0713078\ttotal: 4m 40s\tremaining: 31.9s\n",
      "898:\tlearn: 0.0712975\ttotal: 4m 41s\tremaining: 31.6s\n",
      "899:\tlearn: 0.0712659\ttotal: 4m 41s\tremaining: 31.3s\n",
      "900:\tlearn: 0.0712489\ttotal: 4m 41s\tremaining: 31s\n",
      "901:\tlearn: 0.0712358\ttotal: 4m 42s\tremaining: 30.7s\n",
      "902:\tlearn: 0.0712103\ttotal: 4m 42s\tremaining: 30.4s\n",
      "903:\tlearn: 0.0711798\ttotal: 4m 42s\tremaining: 30s\n",
      "904:\tlearn: 0.0711590\ttotal: 4m 43s\tremaining: 29.7s\n",
      "905:\tlearn: 0.0711383\ttotal: 4m 43s\tremaining: 29.4s\n",
      "906:\tlearn: 0.0711326\ttotal: 4m 43s\tremaining: 29.1s\n",
      "907:\tlearn: 0.0711207\ttotal: 4m 44s\tremaining: 28.8s\n",
      "908:\tlearn: 0.0711010\ttotal: 4m 44s\tremaining: 28.5s\n",
      "909:\tlearn: 0.0710805\ttotal: 4m 44s\tremaining: 28.2s\n",
      "910:\tlearn: 0.0710620\ttotal: 4m 45s\tremaining: 27.9s\n",
      "911:\tlearn: 0.0710385\ttotal: 4m 45s\tremaining: 27.6s\n",
      "912:\tlearn: 0.0710235\ttotal: 4m 45s\tremaining: 27.2s\n",
      "913:\tlearn: 0.0710011\ttotal: 4m 46s\tremaining: 26.9s\n",
      "914:\tlearn: 0.0709864\ttotal: 4m 46s\tremaining: 26.6s\n",
      "915:\tlearn: 0.0709748\ttotal: 4m 46s\tremaining: 26.3s\n",
      "916:\tlearn: 0.0709506\ttotal: 4m 47s\tremaining: 26s\n",
      "917:\tlearn: 0.0709336\ttotal: 4m 47s\tremaining: 25.7s\n",
      "918:\tlearn: 0.0709169\ttotal: 4m 47s\tremaining: 25.4s\n",
      "919:\tlearn: 0.0709033\ttotal: 4m 48s\tremaining: 25.1s\n",
      "920:\tlearn: 0.0708846\ttotal: 4m 48s\tremaining: 24.8s\n",
      "921:\tlearn: 0.0708727\ttotal: 4m 48s\tremaining: 24.4s\n",
      "922:\tlearn: 0.0708588\ttotal: 4m 49s\tremaining: 24.1s\n",
      "923:\tlearn: 0.0708271\ttotal: 4m 49s\tremaining: 23.8s\n",
      "924:\tlearn: 0.0708152\ttotal: 4m 49s\tremaining: 23.5s\n",
      "925:\tlearn: 0.0707977\ttotal: 4m 50s\tremaining: 23.2s\n",
      "926:\tlearn: 0.0707815\ttotal: 4m 50s\tremaining: 22.9s\n",
      "927:\tlearn: 0.0707693\ttotal: 4m 50s\tremaining: 22.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "928:\tlearn: 0.0707561\ttotal: 4m 51s\tremaining: 22.3s\n",
      "929:\tlearn: 0.0707389\ttotal: 4m 51s\tremaining: 21.9s\n",
      "930:\tlearn: 0.0707286\ttotal: 4m 51s\tremaining: 21.6s\n",
      "931:\tlearn: 0.0707026\ttotal: 4m 52s\tremaining: 21.3s\n",
      "932:\tlearn: 0.0706808\ttotal: 4m 52s\tremaining: 21s\n",
      "933:\tlearn: 0.0706650\ttotal: 4m 52s\tremaining: 20.7s\n",
      "934:\tlearn: 0.0706537\ttotal: 4m 53s\tremaining: 20.4s\n",
      "935:\tlearn: 0.0706427\ttotal: 4m 53s\tremaining: 20.1s\n",
      "936:\tlearn: 0.0706276\ttotal: 4m 53s\tremaining: 19.8s\n",
      "937:\tlearn: 0.0706191\ttotal: 4m 54s\tremaining: 19.4s\n",
      "938:\tlearn: 0.0706092\ttotal: 4m 54s\tremaining: 19.1s\n",
      "939:\tlearn: 0.0705984\ttotal: 4m 54s\tremaining: 18.8s\n",
      "940:\tlearn: 0.0705890\ttotal: 4m 55s\tremaining: 18.5s\n",
      "941:\tlearn: 0.0705789\ttotal: 4m 55s\tremaining: 18.2s\n",
      "942:\tlearn: 0.0705610\ttotal: 4m 55s\tremaining: 17.9s\n",
      "943:\tlearn: 0.0705487\ttotal: 4m 55s\tremaining: 17.6s\n",
      "944:\tlearn: 0.0705329\ttotal: 4m 56s\tremaining: 17.2s\n",
      "945:\tlearn: 0.0705199\ttotal: 4m 56s\tremaining: 16.9s\n",
      "946:\tlearn: 0.0705007\ttotal: 4m 56s\tremaining: 16.6s\n",
      "947:\tlearn: 0.0704912\ttotal: 4m 57s\tremaining: 16.3s\n",
      "948:\tlearn: 0.0704817\ttotal: 4m 57s\tremaining: 16s\n",
      "949:\tlearn: 0.0704667\ttotal: 4m 57s\tremaining: 15.7s\n",
      "950:\tlearn: 0.0704590\ttotal: 4m 58s\tremaining: 15.4s\n",
      "951:\tlearn: 0.0704462\ttotal: 4m 58s\tremaining: 15s\n",
      "952:\tlearn: 0.0704323\ttotal: 4m 58s\tremaining: 14.7s\n",
      "953:\tlearn: 0.0704014\ttotal: 4m 58s\tremaining: 14.4s\n",
      "954:\tlearn: 0.0703907\ttotal: 4m 59s\tremaining: 14.1s\n",
      "955:\tlearn: 0.0703605\ttotal: 4m 59s\tremaining: 13.8s\n",
      "956:\tlearn: 0.0703447\ttotal: 4m 59s\tremaining: 13.5s\n",
      "957:\tlearn: 0.0703187\ttotal: 5m\tremaining: 13.2s\n",
      "958:\tlearn: 0.0703049\ttotal: 5m\tremaining: 12.8s\n",
      "959:\tlearn: 0.0702658\ttotal: 5m\tremaining: 12.5s\n",
      "960:\tlearn: 0.0702576\ttotal: 5m 1s\tremaining: 12.2s\n",
      "961:\tlearn: 0.0702270\ttotal: 5m 1s\tremaining: 11.9s\n",
      "962:\tlearn: 0.0702173\ttotal: 5m 1s\tremaining: 11.6s\n",
      "963:\tlearn: 0.0702119\ttotal: 5m 2s\tremaining: 11.3s\n",
      "964:\tlearn: 0.0701990\ttotal: 5m 2s\tremaining: 11s\n",
      "965:\tlearn: 0.0701880\ttotal: 5m 2s\tremaining: 10.7s\n",
      "966:\tlearn: 0.0701815\ttotal: 5m 2s\tremaining: 10.3s\n",
      "967:\tlearn: 0.0701602\ttotal: 5m 3s\tremaining: 10s\n",
      "968:\tlearn: 0.0701438\ttotal: 5m 3s\tremaining: 9.71s\n",
      "969:\tlearn: 0.0701367\ttotal: 5m 3s\tremaining: 9.4s\n",
      "970:\tlearn: 0.0701255\ttotal: 5m 4s\tremaining: 9.09s\n",
      "971:\tlearn: 0.0701191\ttotal: 5m 4s\tremaining: 8.77s\n",
      "972:\tlearn: 0.0701064\ttotal: 5m 4s\tremaining: 8.46s\n",
      "973:\tlearn: 0.0700906\ttotal: 5m 5s\tremaining: 8.14s\n",
      "974:\tlearn: 0.0700759\ttotal: 5m 5s\tremaining: 7.83s\n",
      "975:\tlearn: 0.0700574\ttotal: 5m 5s\tremaining: 7.52s\n",
      "976:\tlearn: 0.0700215\ttotal: 5m 6s\tremaining: 7.21s\n",
      "977:\tlearn: 0.0700087\ttotal: 5m 6s\tremaining: 6.89s\n",
      "978:\tlearn: 0.0699927\ttotal: 5m 6s\tremaining: 6.58s\n",
      "979:\tlearn: 0.0699762\ttotal: 5m 7s\tremaining: 6.27s\n",
      "980:\tlearn: 0.0699648\ttotal: 5m 7s\tremaining: 5.95s\n",
      "981:\tlearn: 0.0699493\ttotal: 5m 7s\tremaining: 5.64s\n",
      "982:\tlearn: 0.0699410\ttotal: 5m 7s\tremaining: 5.33s\n",
      "983:\tlearn: 0.0699274\ttotal: 5m 8s\tremaining: 5.01s\n",
      "984:\tlearn: 0.0699162\ttotal: 5m 8s\tremaining: 4.7s\n",
      "985:\tlearn: 0.0698918\ttotal: 5m 8s\tremaining: 4.39s\n",
      "986:\tlearn: 0.0698491\ttotal: 5m 9s\tremaining: 4.07s\n",
      "987:\tlearn: 0.0698191\ttotal: 5m 9s\tremaining: 3.76s\n",
      "988:\tlearn: 0.0698053\ttotal: 5m 9s\tremaining: 3.45s\n",
      "989:\tlearn: 0.0697979\ttotal: 5m 10s\tremaining: 3.13s\n",
      "990:\tlearn: 0.0697732\ttotal: 5m 10s\tremaining: 2.82s\n",
      "991:\tlearn: 0.0697612\ttotal: 5m 10s\tremaining: 2.51s\n",
      "992:\tlearn: 0.0697427\ttotal: 5m 11s\tremaining: 2.19s\n",
      "993:\tlearn: 0.0697272\ttotal: 5m 11s\tremaining: 1.88s\n",
      "994:\tlearn: 0.0697120\ttotal: 5m 11s\tremaining: 1.57s\n",
      "995:\tlearn: 0.0696990\ttotal: 5m 12s\tremaining: 1.25s\n",
      "996:\tlearn: 0.0696829\ttotal: 5m 12s\tremaining: 940ms\n",
      "997:\tlearn: 0.0696705\ttotal: 5m 12s\tremaining: 627ms\n",
      "998:\tlearn: 0.0696546\ttotal: 5m 13s\tremaining: 313ms\n",
      "999:\tlearn: 0.0696180\ttotal: 5m 13s\tremaining: 0us\n",
      "Learning rate set to 0.408538\n",
      "0:\ttest: 0.6981337\tbest: 0.6981337 (0)\ttotal: 336ms\tremaining: 33.3s\n",
      "1:\ttest: 0.7827174\tbest: 0.7827174 (1)\ttotal: 679ms\tremaining: 33.3s\n",
      "2:\ttest: 0.7949025\tbest: 0.7949025 (2)\ttotal: 1.03s\tremaining: 33.3s\n",
      "3:\ttest: 0.8146079\tbest: 0.8146079 (3)\ttotal: 1.38s\tremaining: 33s\n",
      "4:\ttest: 0.8342459\tbest: 0.8342459 (4)\ttotal: 1.74s\tremaining: 33s\n",
      "5:\ttest: 0.8376358\tbest: 0.8376358 (5)\ttotal: 2.04s\tremaining: 32s\n",
      "6:\ttest: 0.8467542\tbest: 0.8467542 (6)\ttotal: 2.42s\tremaining: 32.1s\n",
      "7:\ttest: 0.8493016\tbest: 0.8493016 (7)\ttotal: 2.73s\tremaining: 31.4s\n",
      "8:\ttest: 0.8526085\tbest: 0.8526085 (8)\ttotal: 3.1s\tremaining: 31.3s\n",
      "9:\ttest: 0.8546904\tbest: 0.8546904 (9)\ttotal: 3.43s\tremaining: 30.8s\n",
      "10:\ttest: 0.8567332\tbest: 0.8567332 (10)\ttotal: 3.77s\tremaining: 30.5s\n",
      "11:\ttest: 0.8604192\tbest: 0.8604192 (11)\ttotal: 4.08s\tremaining: 30s\n",
      "12:\ttest: 0.8626317\tbest: 0.8626317 (12)\ttotal: 4.46s\tremaining: 29.8s\n",
      "13:\ttest: 0.8630323\tbest: 0.8630323 (13)\ttotal: 4.84s\tremaining: 29.7s\n",
      "14:\ttest: 0.8638927\tbest: 0.8638927 (14)\ttotal: 5.15s\tremaining: 29.2s\n",
      "15:\ttest: 0.8651582\tbest: 0.8651582 (15)\ttotal: 5.48s\tremaining: 28.8s\n",
      "16:\ttest: 0.8675366\tbest: 0.8675366 (16)\ttotal: 5.82s\tremaining: 28.4s\n",
      "17:\ttest: 0.8693288\tbest: 0.8693288 (17)\ttotal: 6.14s\tremaining: 28s\n",
      "18:\ttest: 0.8702294\tbest: 0.8702294 (18)\ttotal: 6.47s\tremaining: 27.6s\n",
      "19:\ttest: 0.8708334\tbest: 0.8708334 (19)\ttotal: 6.79s\tremaining: 27.2s\n",
      "20:\ttest: 0.8722011\tbest: 0.8722011 (20)\ttotal: 7.14s\tremaining: 26.9s\n",
      "21:\ttest: 0.8733215\tbest: 0.8733215 (21)\ttotal: 7.49s\tremaining: 26.6s\n",
      "22:\ttest: 0.8735565\tbest: 0.8735565 (22)\ttotal: 7.79s\tremaining: 26.1s\n",
      "23:\ttest: 0.8739796\tbest: 0.8739796 (23)\ttotal: 8.16s\tremaining: 25.9s\n",
      "24:\ttest: 0.8746978\tbest: 0.8746978 (24)\ttotal: 8.47s\tremaining: 25.4s\n",
      "25:\ttest: 0.8756769\tbest: 0.8756769 (25)\ttotal: 8.85s\tremaining: 25.2s\n",
      "26:\ttest: 0.8762648\tbest: 0.8762648 (26)\ttotal: 9.18s\tremaining: 24.8s\n",
      "27:\ttest: 0.8763391\tbest: 0.8763391 (27)\ttotal: 9.49s\tremaining: 24.4s\n",
      "28:\ttest: 0.8769469\tbest: 0.8769469 (28)\ttotal: 9.9s\tremaining: 24.2s\n",
      "29:\ttest: 0.8773990\tbest: 0.8773990 (29)\ttotal: 10.2s\tremaining: 23.8s\n",
      "30:\ttest: 0.8778678\tbest: 0.8778678 (30)\ttotal: 10.5s\tremaining: 23.4s\n",
      "31:\ttest: 0.8780479\tbest: 0.8780479 (31)\ttotal: 10.8s\tremaining: 23s\n",
      "32:\ttest: 0.8800262\tbest: 0.8800262 (32)\ttotal: 11.2s\tremaining: 22.7s\n",
      "33:\ttest: 0.8808371\tbest: 0.8808371 (33)\ttotal: 11.5s\tremaining: 22.4s\n",
      "34:\ttest: 0.8811303\tbest: 0.8811303 (34)\ttotal: 11.8s\tremaining: 22s\n",
      "35:\ttest: 0.8816710\tbest: 0.8816710 (35)\ttotal: 12.1s\tremaining: 21.6s\n",
      "36:\ttest: 0.8823720\tbest: 0.8823720 (36)\ttotal: 12.5s\tremaining: 21.3s\n",
      "37:\ttest: 0.8829545\tbest: 0.8829545 (37)\ttotal: 12.9s\tremaining: 21s\n",
      "38:\ttest: 0.8838542\tbest: 0.8838542 (38)\ttotal: 13.2s\tremaining: 20.7s\n",
      "39:\ttest: 0.8843471\tbest: 0.8843471 (39)\ttotal: 13.5s\tremaining: 20.3s\n",
      "40:\ttest: 0.8851826\tbest: 0.8851826 (40)\ttotal: 13.9s\tremaining: 19.9s\n",
      "41:\ttest: 0.8858897\tbest: 0.8858897 (41)\ttotal: 14.2s\tremaining: 19.6s\n",
      "42:\ttest: 0.8862201\tbest: 0.8862201 (42)\ttotal: 14.5s\tremaining: 19.2s\n",
      "43:\ttest: 0.8865994\tbest: 0.8865994 (43)\ttotal: 14.8s\tremaining: 18.9s\n",
      "44:\ttest: 0.8870788\tbest: 0.8870788 (44)\ttotal: 15.2s\tremaining: 18.6s\n",
      "45:\ttest: 0.8874157\tbest: 0.8874157 (45)\ttotal: 15.5s\tremaining: 18.2s\n",
      "46:\ttest: 0.8878547\tbest: 0.8878547 (46)\ttotal: 15.9s\tremaining: 17.9s\n",
      "47:\ttest: 0.8885727\tbest: 0.8885727 (47)\ttotal: 16.2s\tremaining: 17.6s\n",
      "48:\ttest: 0.8889461\tbest: 0.8889461 (48)\ttotal: 16.6s\tremaining: 17.2s\n",
      "49:\ttest: 0.8896567\tbest: 0.8896567 (49)\ttotal: 16.9s\tremaining: 16.9s\n",
      "50:\ttest: 0.8898878\tbest: 0.8898878 (50)\ttotal: 17.2s\tremaining: 16.5s\n",
      "51:\ttest: 0.8901102\tbest: 0.8901102 (51)\ttotal: 17.5s\tremaining: 16.2s\n",
      "52:\ttest: 0.8906677\tbest: 0.8906677 (52)\ttotal: 17.8s\tremaining: 15.8s\n",
      "53:\ttest: 0.8910657\tbest: 0.8910657 (53)\ttotal: 18.2s\tremaining: 15.5s\n",
      "54:\ttest: 0.8919550\tbest: 0.8919550 (54)\ttotal: 18.5s\tremaining: 15.2s\n",
      "55:\ttest: 0.8922580\tbest: 0.8922580 (55)\ttotal: 18.9s\tremaining: 14.8s\n",
      "56:\ttest: 0.8926794\tbest: 0.8926794 (56)\ttotal: 19.2s\tremaining: 14.5s\n",
      "57:\ttest: 0.8928151\tbest: 0.8928151 (57)\ttotal: 19.5s\tremaining: 14.2s\n",
      "58:\ttest: 0.8930163\tbest: 0.8930163 (58)\ttotal: 19.8s\tremaining: 13.8s\n",
      "59:\ttest: 0.8934326\tbest: 0.8934326 (59)\ttotal: 20.2s\tremaining: 13.5s\n",
      "60:\ttest: 0.8939117\tbest: 0.8939117 (60)\ttotal: 20.5s\tremaining: 13.1s\n",
      "61:\ttest: 0.8944775\tbest: 0.8944775 (61)\ttotal: 20.8s\tremaining: 12.8s\n",
      "62:\ttest: 0.8950623\tbest: 0.8950623 (62)\ttotal: 21.2s\tremaining: 12.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63:\ttest: 0.8952422\tbest: 0.8952422 (63)\ttotal: 21.5s\tremaining: 12.1s\n",
      "64:\ttest: 0.8955153\tbest: 0.8955153 (64)\ttotal: 21.9s\tremaining: 11.8s\n",
      "65:\ttest: 0.8959867\tbest: 0.8959867 (65)\ttotal: 22.3s\tremaining: 11.5s\n",
      "66:\ttest: 0.8961938\tbest: 0.8961938 (66)\ttotal: 22.6s\tremaining: 11.1s\n",
      "67:\ttest: 0.8962810\tbest: 0.8962810 (67)\ttotal: 22.9s\tremaining: 10.8s\n",
      "68:\ttest: 0.8968699\tbest: 0.8968699 (68)\ttotal: 23.2s\tremaining: 10.4s\n",
      "69:\ttest: 0.8971396\tbest: 0.8971396 (69)\ttotal: 23.5s\tremaining: 10.1s\n",
      "70:\ttest: 0.8974735\tbest: 0.8974735 (70)\ttotal: 23.9s\tremaining: 9.77s\n",
      "71:\ttest: 0.8978058\tbest: 0.8978058 (71)\ttotal: 24.3s\tremaining: 9.44s\n",
      "72:\ttest: 0.8980984\tbest: 0.8980984 (72)\ttotal: 24.6s\tremaining: 9.1s\n",
      "73:\ttest: 0.8986872\tbest: 0.8986872 (73)\ttotal: 25s\tremaining: 8.78s\n",
      "74:\ttest: 0.8990007\tbest: 0.8990007 (74)\ttotal: 25.3s\tremaining: 8.44s\n",
      "75:\ttest: 0.8991693\tbest: 0.8991693 (75)\ttotal: 25.6s\tremaining: 8.09s\n",
      "76:\ttest: 0.8993609\tbest: 0.8993609 (76)\ttotal: 26s\tremaining: 7.75s\n",
      "77:\ttest: 0.8996700\tbest: 0.8996700 (77)\ttotal: 26.3s\tremaining: 7.42s\n",
      "78:\ttest: 0.8997313\tbest: 0.8997313 (78)\ttotal: 26.6s\tremaining: 7.08s\n",
      "79:\ttest: 0.8999434\tbest: 0.8999434 (79)\ttotal: 26.9s\tremaining: 6.73s\n",
      "80:\ttest: 0.9001166\tbest: 0.9001166 (80)\ttotal: 27.3s\tremaining: 6.4s\n",
      "81:\ttest: 0.9006819\tbest: 0.9006819 (81)\ttotal: 27.6s\tremaining: 6.07s\n",
      "82:\ttest: 0.9012605\tbest: 0.9012605 (82)\ttotal: 28s\tremaining: 5.73s\n",
      "83:\ttest: 0.9015449\tbest: 0.9015449 (83)\ttotal: 28.3s\tremaining: 5.4s\n",
      "84:\ttest: 0.9017917\tbest: 0.9017917 (84)\ttotal: 28.6s\tremaining: 5.05s\n",
      "85:\ttest: 0.9020000\tbest: 0.9020000 (85)\ttotal: 29s\tremaining: 4.71s\n",
      "86:\ttest: 0.9020817\tbest: 0.9020817 (86)\ttotal: 29.3s\tremaining: 4.38s\n",
      "87:\ttest: 0.9028436\tbest: 0.9028436 (87)\ttotal: 29.7s\tremaining: 4.04s\n",
      "88:\ttest: 0.9029619\tbest: 0.9029619 (88)\ttotal: 30s\tremaining: 3.7s\n",
      "89:\ttest: 0.9032389\tbest: 0.9032389 (89)\ttotal: 30.3s\tremaining: 3.37s\n",
      "90:\ttest: 0.9036401\tbest: 0.9036401 (90)\ttotal: 30.7s\tremaining: 3.04s\n",
      "91:\ttest: 0.9044062\tbest: 0.9044062 (91)\ttotal: 31.1s\tremaining: 2.7s\n",
      "92:\ttest: 0.9045442\tbest: 0.9045442 (92)\ttotal: 31.4s\tremaining: 2.36s\n",
      "93:\ttest: 0.9046027\tbest: 0.9046027 (93)\ttotal: 31.8s\tremaining: 2.03s\n",
      "94:\ttest: 0.9048116\tbest: 0.9048116 (94)\ttotal: 32.1s\tremaining: 1.69s\n",
      "95:\ttest: 0.9049828\tbest: 0.9049828 (95)\ttotal: 32.4s\tremaining: 1.35s\n",
      "96:\ttest: 0.9052963\tbest: 0.9052963 (96)\ttotal: 32.8s\tremaining: 1.01s\n",
      "97:\ttest: 0.9060448\tbest: 0.9060448 (97)\ttotal: 33.1s\tremaining: 676ms\n",
      "98:\ttest: 0.9062159\tbest: 0.9062159 (98)\ttotal: 33.4s\tremaining: 338ms\n",
      "99:\ttest: 0.9062869\tbest: 0.9062869 (99)\ttotal: 33.8s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9062869164\n",
      "bestIteration = 99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = CatBoostClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "iterations=100\n",
    "cb_model = CatBoostClassifier(eval_metric='AUC',use_best_model=True,random_seed=42,iterations=iterations)\n",
    "cb_model = cb_model.fit(X_train,y_train, eval_set=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cb = cb_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_TF(object):\n",
    "    def __init__(self, n_dim, n_categories, n_units_hidden, batchsize=64, epochs=20,\n",
    "                learning_rate=0.01, random_seed=None):\n",
    "        np.random.seed(random_seed)\n",
    "        self.batchsize = batchsize\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_dim = n_dim\n",
    "        self.n_categories = n_categories\n",
    "        self.n_units_hidden = n_units_hidden\n",
    "        \n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            tf.set_random_seed(random_seed)\n",
    "            self.build()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            self.sess = tf.Session(graph=g)\n",
    "    \n",
    "    def build(self):\n",
    "        tf_x  = tf.placeholder(tf.float32, shape=[None, self.n_dim], name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.int32, shape=[None], name='tf_y')\n",
    "        \n",
    "        is_train = tf.placeholder(tf.bool, shape=(), name='is_train')\n",
    "        \n",
    "        tf_y_onehot = tf.one_hot(indices=tf_y, depth=self.n_categories, dtype=tf.float32, name='input_y_onehot')\n",
    "        \n",
    "        hh = tf.layers.dense(tf_x, self.n_units_hidden, activation=tf.nn.tanh)\n",
    "        \n",
    "        hh2 = tf.layers.dense(hh, self.n_units_hidden, activation=tf.nn.tanh)\n",
    "        \n",
    "        output = tf.layers.dense(hh2, self.n_categories, activation=None)\n",
    "        \n",
    "        # predictions\n",
    "        predictions = {\n",
    "            'probabilities':  tf.nn.softmax(output, name='probabilities'),\n",
    "            'labels': tf.cast(tf.argmax(output, axis=1), tf.int32, name='labels')\n",
    "        }\n",
    "        \n",
    "        \n",
    "        cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_y_onehot),\n",
    "                                            name = 'cross_entropy_loss')\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        optimizer = optimizer.minimize(cross_entropy_loss, name='train_op')\n",
    "        \n",
    "        correct_predictions = tf.equal(predictions['labels'], tf_y, name='correct_preds')\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions,tf.float32), name='accuracy')\n",
    "    \n",
    "    def train(self, x_train, y_train, initialize=True):\n",
    "        if initialize:\n",
    "            self.sess.run(self.init_op)\n",
    "        \n",
    "        self.train_cost = []\n",
    "        \n",
    "        for epoch in range(1, self.epochs+1):\n",
    "            avg_loss = 0.0\n",
    "            #for i in range(0, x_train.shape[0] - self.batchsize):\n",
    "            #    batch_x = x_train[i:i+self.batchsize, :]\n",
    "            #   batch_y = y_train[i:i+self.batchsize]\n",
    "                \n",
    "            feed = {'tf_x:0': x_train,\n",
    "                    'tf_y:0': y_train,\n",
    "                     'is_train:0': True}\n",
    "                \n",
    "            loss, _ = self.sess.run(['cross_entropy_loss:0', 'train_op'], feed_dict=feed)\n",
    "            avg_loss += loss\n",
    "            print('Epoch %02d: Training Avg. Loss: %7.3f' % (epoch, avg_loss))\n",
    "            \n",
    "        print('Final accuracy: ', self.sess.run('accuracy:0', feed_dict={'tf_x:0':x_train, 'tf_y:0':y_train}))\n",
    "        \n",
    "    def predict(self, X_test, return_proba=False):\n",
    "        feed = {'tf_x:0': X_test, 'is_train:0':False}\n",
    "        \n",
    "        if return_proba:\n",
    "            return self.sess.run('probabilities:0', feed_dict = {'tf_x:0':X_test})\n",
    "        else:\n",
    "            return self.sess.run('labels:0', feed_dict=feed)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model = ANN_TF(n_dim=n_dim, n_categories=2, n_units_hidden=20, batchsize=10000, epochs=1009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Training Avg. Loss:   0.451\n",
      "Epoch 02: Training Avg. Loss:   0.402\n",
      "Epoch 03: Training Avg. Loss:   0.367\n",
      "Epoch 04: Training Avg. Loss:   0.338\n",
      "Epoch 05: Training Avg. Loss:   0.316\n",
      "Epoch 06: Training Avg. Loss:   0.297\n",
      "Epoch 07: Training Avg. Loss:   0.281\n",
      "Epoch 08: Training Avg. Loss:   0.268\n",
      "Epoch 09: Training Avg. Loss:   0.257\n",
      "Epoch 10: Training Avg. Loss:   0.248\n",
      "Epoch 11: Training Avg. Loss:   0.239\n",
      "Epoch 12: Training Avg. Loss:   0.225\n",
      "Epoch 13: Training Avg. Loss:   0.209\n",
      "Epoch 14: Training Avg. Loss:   0.205\n",
      "Epoch 15: Training Avg. Loss:   0.202\n",
      "Epoch 16: Training Avg. Loss:   0.198\n",
      "Epoch 17: Training Avg. Loss:   0.195\n",
      "Epoch 18: Training Avg. Loss:   0.193\n",
      "Epoch 19: Training Avg. Loss:   0.190\n",
      "Epoch 20: Training Avg. Loss:   0.188\n",
      "Epoch 21: Training Avg. Loss:   0.186\n",
      "Epoch 22: Training Avg. Loss:   0.184\n",
      "Epoch 23: Training Avg. Loss:   0.182\n",
      "Epoch 24: Training Avg. Loss:   0.180\n",
      "Epoch 25: Training Avg. Loss:   0.179\n",
      "Epoch 26: Training Avg. Loss:   0.177\n",
      "Epoch 27: Training Avg. Loss:   0.176\n",
      "Epoch 28: Training Avg. Loss:   0.175\n",
      "Epoch 29: Training Avg. Loss:   0.174\n",
      "Epoch 30: Training Avg. Loss:   0.173\n",
      "Epoch 31: Training Avg. Loss:   0.172\n",
      "Epoch 32: Training Avg. Loss:   0.171\n",
      "Epoch 33: Training Avg. Loss:   0.170\n",
      "Epoch 34: Training Avg. Loss:   0.169\n",
      "Epoch 35: Training Avg. Loss:   0.168\n",
      "Epoch 36: Training Avg. Loss:   0.168\n",
      "Epoch 37: Training Avg. Loss:   0.167\n",
      "Epoch 38: Training Avg. Loss:   0.166\n",
      "Epoch 39: Training Avg. Loss:   0.166\n",
      "Epoch 40: Training Avg. Loss:   0.165\n",
      "Epoch 41: Training Avg. Loss:   0.164\n",
      "Epoch 42: Training Avg. Loss:   0.164\n",
      "Epoch 43: Training Avg. Loss:   0.163\n",
      "Epoch 44: Training Avg. Loss:   0.163\n",
      "Epoch 45: Training Avg. Loss:   0.162\n",
      "Epoch 46: Training Avg. Loss:   0.162\n",
      "Epoch 47: Training Avg. Loss:   0.162\n",
      "Epoch 48: Training Avg. Loss:   0.161\n",
      "Epoch 49: Training Avg. Loss:   0.161\n",
      "Epoch 50: Training Avg. Loss:   0.160\n",
      "Epoch 51: Training Avg. Loss:   0.160\n",
      "Epoch 52: Training Avg. Loss:   0.160\n",
      "Epoch 53: Training Avg. Loss:   0.160\n",
      "Epoch 54: Training Avg. Loss:   0.159\n",
      "Epoch 55: Training Avg. Loss:   0.159\n",
      "Epoch 56: Training Avg. Loss:   0.159\n",
      "Epoch 57: Training Avg. Loss:   0.158\n",
      "Epoch 58: Training Avg. Loss:   0.158\n",
      "Epoch 59: Training Avg. Loss:   0.158\n",
      "Epoch 60: Training Avg. Loss:   0.158\n",
      "Epoch 61: Training Avg. Loss:   0.157\n",
      "Epoch 62: Training Avg. Loss:   0.157\n",
      "Epoch 63: Training Avg. Loss:   0.157\n",
      "Epoch 64: Training Avg. Loss:   0.157\n",
      "Epoch 65: Training Avg. Loss:   0.157\n",
      "Epoch 66: Training Avg. Loss:   0.156\n",
      "Epoch 67: Training Avg. Loss:   0.156\n",
      "Epoch 68: Training Avg. Loss:   0.156\n",
      "Epoch 69: Training Avg. Loss:   0.156\n",
      "Epoch 70: Training Avg. Loss:   0.156\n",
      "Epoch 71: Training Avg. Loss:   0.156\n",
      "Epoch 72: Training Avg. Loss:   0.156\n",
      "Epoch 73: Training Avg. Loss:   0.155\n",
      "Epoch 74: Training Avg. Loss:   0.155\n",
      "Epoch 75: Training Avg. Loss:   0.155\n",
      "Epoch 76: Training Avg. Loss:   0.155\n",
      "Epoch 77: Training Avg. Loss:   0.155\n",
      "Epoch 78: Training Avg. Loss:   0.155\n",
      "Epoch 79: Training Avg. Loss:   0.155\n",
      "Epoch 80: Training Avg. Loss:   0.155\n",
      "Epoch 81: Training Avg. Loss:   0.154\n",
      "Epoch 82: Training Avg. Loss:   0.154\n",
      "Epoch 83: Training Avg. Loss:   0.154\n",
      "Epoch 84: Training Avg. Loss:   0.154\n",
      "Epoch 85: Training Avg. Loss:   0.154\n",
      "Epoch 86: Training Avg. Loss:   0.154\n",
      "Epoch 87: Training Avg. Loss:   0.154\n",
      "Epoch 88: Training Avg. Loss:   0.154\n",
      "Epoch 89: Training Avg. Loss:   0.154\n",
      "Epoch 90: Training Avg. Loss:   0.154\n",
      "Epoch 91: Training Avg. Loss:   0.154\n",
      "Epoch 92: Training Avg. Loss:   0.154\n",
      "Epoch 93: Training Avg. Loss:   0.153\n",
      "Epoch 94: Training Avg. Loss:   0.153\n",
      "Epoch 95: Training Avg. Loss:   0.153\n",
      "Epoch 96: Training Avg. Loss:   0.153\n",
      "Epoch 97: Training Avg. Loss:   0.153\n",
      "Epoch 98: Training Avg. Loss:   0.153\n",
      "Epoch 99: Training Avg. Loss:   0.153\n",
      "Epoch 100: Training Avg. Loss:   0.153\n",
      "Epoch 101: Training Avg. Loss:   0.153\n",
      "Epoch 102: Training Avg. Loss:   0.153\n",
      "Epoch 103: Training Avg. Loss:   0.153\n",
      "Epoch 104: Training Avg. Loss:   0.153\n",
      "Epoch 105: Training Avg. Loss:   0.153\n",
      "Epoch 106: Training Avg. Loss:   0.153\n",
      "Epoch 107: Training Avg. Loss:   0.153\n",
      "Epoch 108: Training Avg. Loss:   0.153\n",
      "Epoch 109: Training Avg. Loss:   0.153\n",
      "Epoch 110: Training Avg. Loss:   0.153\n",
      "Epoch 111: Training Avg. Loss:   0.153\n",
      "Epoch 112: Training Avg. Loss:   0.152\n",
      "Epoch 113: Training Avg. Loss:   0.152\n",
      "Epoch 114: Training Avg. Loss:   0.152\n",
      "Epoch 115: Training Avg. Loss:   0.152\n",
      "Epoch 116: Training Avg. Loss:   0.152\n",
      "Epoch 117: Training Avg. Loss:   0.152\n",
      "Epoch 118: Training Avg. Loss:   0.152\n",
      "Epoch 119: Training Avg. Loss:   0.152\n",
      "Epoch 120: Training Avg. Loss:   0.152\n",
      "Epoch 121: Training Avg. Loss:   0.152\n",
      "Epoch 122: Training Avg. Loss:   0.152\n",
      "Epoch 123: Training Avg. Loss:   0.152\n",
      "Epoch 124: Training Avg. Loss:   0.152\n",
      "Epoch 125: Training Avg. Loss:   0.152\n",
      "Epoch 126: Training Avg. Loss:   0.152\n",
      "Epoch 127: Training Avg. Loss:   0.152\n",
      "Epoch 128: Training Avg. Loss:   0.152\n",
      "Epoch 129: Training Avg. Loss:   0.152\n",
      "Epoch 130: Training Avg. Loss:   0.152\n",
      "Epoch 131: Training Avg. Loss:   0.152\n",
      "Epoch 132: Training Avg. Loss:   0.152\n",
      "Epoch 133: Training Avg. Loss:   0.152\n",
      "Epoch 134: Training Avg. Loss:   0.152\n",
      "Epoch 135: Training Avg. Loss:   0.152\n",
      "Epoch 136: Training Avg. Loss:   0.152\n",
      "Epoch 137: Training Avg. Loss:   0.152\n",
      "Epoch 138: Training Avg. Loss:   0.152\n",
      "Epoch 139: Training Avg. Loss:   0.152\n",
      "Epoch 140: Training Avg. Loss:   0.152\n",
      "Epoch 141: Training Avg. Loss:   0.152\n",
      "Epoch 142: Training Avg. Loss:   0.152\n",
      "Epoch 143: Training Avg. Loss:   0.152\n",
      "Epoch 144: Training Avg. Loss:   0.152\n",
      "Epoch 145: Training Avg. Loss:   0.152\n",
      "Epoch 146: Training Avg. Loss:   0.151\n",
      "Epoch 147: Training Avg. Loss:   0.151\n",
      "Epoch 148: Training Avg. Loss:   0.151\n",
      "Epoch 149: Training Avg. Loss:   0.151\n",
      "Epoch 150: Training Avg. Loss:   0.151\n",
      "Epoch 151: Training Avg. Loss:   0.151\n",
      "Epoch 152: Training Avg. Loss:   0.151\n",
      "Epoch 153: Training Avg. Loss:   0.151\n",
      "Epoch 154: Training Avg. Loss:   0.151\n",
      "Epoch 155: Training Avg. Loss:   0.151\n",
      "Epoch 156: Training Avg. Loss:   0.151\n",
      "Epoch 157: Training Avg. Loss:   0.151\n",
      "Epoch 158: Training Avg. Loss:   0.151\n",
      "Epoch 159: Training Avg. Loss:   0.151\n",
      "Epoch 160: Training Avg. Loss:   0.151\n",
      "Epoch 161: Training Avg. Loss:   0.151\n",
      "Epoch 162: Training Avg. Loss:   0.151\n",
      "Epoch 163: Training Avg. Loss:   0.151\n",
      "Epoch 164: Training Avg. Loss:   0.151\n",
      "Epoch 165: Training Avg. Loss:   0.151\n",
      "Epoch 166: Training Avg. Loss:   0.151\n",
      "Epoch 167: Training Avg. Loss:   0.151\n",
      "Epoch 168: Training Avg. Loss:   0.151\n",
      "Epoch 169: Training Avg. Loss:   0.151\n",
      "Epoch 170: Training Avg. Loss:   0.151\n",
      "Epoch 171: Training Avg. Loss:   0.151\n",
      "Epoch 172: Training Avg. Loss:   0.151\n",
      "Epoch 173: Training Avg. Loss:   0.151\n",
      "Epoch 174: Training Avg. Loss:   0.151\n",
      "Epoch 175: Training Avg. Loss:   0.151\n",
      "Epoch 176: Training Avg. Loss:   0.151\n",
      "Epoch 177: Training Avg. Loss:   0.151\n",
      "Epoch 178: Training Avg. Loss:   0.151\n",
      "Epoch 179: Training Avg. Loss:   0.151\n",
      "Epoch 180: Training Avg. Loss:   0.151\n",
      "Epoch 181: Training Avg. Loss:   0.151\n",
      "Epoch 182: Training Avg. Loss:   0.151\n",
      "Epoch 183: Training Avg. Loss:   0.151\n",
      "Epoch 184: Training Avg. Loss:   0.151\n",
      "Epoch 185: Training Avg. Loss:   0.151\n",
      "Epoch 186: Training Avg. Loss:   0.151\n",
      "Epoch 187: Training Avg. Loss:   0.151\n",
      "Epoch 188: Training Avg. Loss:   0.151\n",
      "Epoch 189: Training Avg. Loss:   0.151\n",
      "Epoch 190: Training Avg. Loss:   0.151\n",
      "Epoch 191: Training Avg. Loss:   0.151\n",
      "Epoch 192: Training Avg. Loss:   0.151\n",
      "Epoch 193: Training Avg. Loss:   0.151\n",
      "Epoch 194: Training Avg. Loss:   0.151\n",
      "Epoch 195: Training Avg. Loss:   0.151\n",
      "Epoch 196: Training Avg. Loss:   0.151\n",
      "Epoch 197: Training Avg. Loss:   0.151\n",
      "Epoch 198: Training Avg. Loss:   0.151\n",
      "Epoch 199: Training Avg. Loss:   0.151\n",
      "Epoch 200: Training Avg. Loss:   0.151\n",
      "Epoch 201: Training Avg. Loss:   0.151\n",
      "Epoch 202: Training Avg. Loss:   0.151\n",
      "Epoch 203: Training Avg. Loss:   0.151\n",
      "Epoch 204: Training Avg. Loss:   0.151\n",
      "Epoch 205: Training Avg. Loss:   0.151\n",
      "Epoch 206: Training Avg. Loss:   0.151\n",
      "Epoch 207: Training Avg. Loss:   0.151\n",
      "Epoch 208: Training Avg. Loss:   0.151\n",
      "Epoch 209: Training Avg. Loss:   0.151\n",
      "Epoch 210: Training Avg. Loss:   0.151\n",
      "Epoch 211: Training Avg. Loss:   0.151\n",
      "Epoch 212: Training Avg. Loss:   0.151\n",
      "Epoch 213: Training Avg. Loss:   0.151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214: Training Avg. Loss:   0.151\n",
      "Epoch 215: Training Avg. Loss:   0.151\n",
      "Epoch 216: Training Avg. Loss:   0.151\n",
      "Epoch 217: Training Avg. Loss:   0.151\n",
      "Epoch 218: Training Avg. Loss:   0.151\n",
      "Epoch 219: Training Avg. Loss:   0.151\n",
      "Epoch 220: Training Avg. Loss:   0.151\n",
      "Epoch 221: Training Avg. Loss:   0.151\n",
      "Epoch 222: Training Avg. Loss:   0.151\n",
      "Epoch 223: Training Avg. Loss:   0.150\n",
      "Epoch 224: Training Avg. Loss:   0.150\n",
      "Epoch 225: Training Avg. Loss:   0.150\n",
      "Epoch 226: Training Avg. Loss:   0.150\n",
      "Epoch 227: Training Avg. Loss:   0.150\n",
      "Epoch 228: Training Avg. Loss:   0.150\n",
      "Epoch 229: Training Avg. Loss:   0.150\n",
      "Epoch 230: Training Avg. Loss:   0.150\n",
      "Epoch 231: Training Avg. Loss:   0.150\n",
      "Epoch 232: Training Avg. Loss:   0.150\n",
      "Epoch 233: Training Avg. Loss:   0.150\n",
      "Epoch 234: Training Avg. Loss:   0.150\n",
      "Epoch 235: Training Avg. Loss:   0.150\n",
      "Epoch 236: Training Avg. Loss:   0.150\n",
      "Epoch 237: Training Avg. Loss:   0.150\n",
      "Epoch 238: Training Avg. Loss:   0.150\n",
      "Epoch 239: Training Avg. Loss:   0.150\n",
      "Epoch 240: Training Avg. Loss:   0.150\n",
      "Epoch 241: Training Avg. Loss:   0.150\n",
      "Epoch 242: Training Avg. Loss:   0.150\n",
      "Epoch 243: Training Avg. Loss:   0.150\n",
      "Epoch 244: Training Avg. Loss:   0.150\n",
      "Epoch 245: Training Avg. Loss:   0.150\n",
      "Epoch 246: Training Avg. Loss:   0.150\n",
      "Epoch 247: Training Avg. Loss:   0.150\n",
      "Epoch 248: Training Avg. Loss:   0.150\n",
      "Epoch 249: Training Avg. Loss:   0.150\n",
      "Epoch 250: Training Avg. Loss:   0.150\n",
      "Epoch 251: Training Avg. Loss:   0.150\n",
      "Epoch 252: Training Avg. Loss:   0.150\n",
      "Epoch 253: Training Avg. Loss:   0.150\n",
      "Epoch 254: Training Avg. Loss:   0.150\n",
      "Epoch 255: Training Avg. Loss:   0.150\n",
      "Epoch 256: Training Avg. Loss:   0.150\n",
      "Epoch 257: Training Avg. Loss:   0.150\n",
      "Epoch 258: Training Avg. Loss:   0.150\n",
      "Epoch 259: Training Avg. Loss:   0.150\n",
      "Epoch 260: Training Avg. Loss:   0.150\n",
      "Epoch 261: Training Avg. Loss:   0.150\n",
      "Epoch 262: Training Avg. Loss:   0.150\n",
      "Epoch 263: Training Avg. Loss:   0.150\n",
      "Epoch 264: Training Avg. Loss:   0.150\n",
      "Epoch 265: Training Avg. Loss:   0.150\n",
      "Epoch 266: Training Avg. Loss:   0.150\n",
      "Epoch 267: Training Avg. Loss:   0.150\n",
      "Epoch 268: Training Avg. Loss:   0.150\n",
      "Epoch 269: Training Avg. Loss:   0.150\n",
      "Epoch 270: Training Avg. Loss:   0.150\n",
      "Epoch 271: Training Avg. Loss:   0.150\n",
      "Epoch 272: Training Avg. Loss:   0.150\n",
      "Epoch 273: Training Avg. Loss:   0.150\n",
      "Epoch 274: Training Avg. Loss:   0.150\n",
      "Epoch 275: Training Avg. Loss:   0.150\n",
      "Epoch 276: Training Avg. Loss:   0.150\n",
      "Epoch 277: Training Avg. Loss:   0.150\n",
      "Epoch 278: Training Avg. Loss:   0.150\n",
      "Epoch 279: Training Avg. Loss:   0.150\n",
      "Epoch 280: Training Avg. Loss:   0.150\n",
      "Epoch 281: Training Avg. Loss:   0.150\n",
      "Epoch 282: Training Avg. Loss:   0.150\n",
      "Epoch 283: Training Avg. Loss:   0.150\n",
      "Epoch 284: Training Avg. Loss:   0.150\n",
      "Epoch 285: Training Avg. Loss:   0.150\n",
      "Epoch 286: Training Avg. Loss:   0.150\n",
      "Epoch 287: Training Avg. Loss:   0.150\n",
      "Epoch 288: Training Avg. Loss:   0.150\n",
      "Epoch 289: Training Avg. Loss:   0.150\n",
      "Epoch 290: Training Avg. Loss:   0.150\n",
      "Epoch 291: Training Avg. Loss:   0.150\n",
      "Epoch 292: Training Avg. Loss:   0.150\n",
      "Epoch 293: Training Avg. Loss:   0.150\n",
      "Epoch 294: Training Avg. Loss:   0.150\n",
      "Epoch 295: Training Avg. Loss:   0.150\n",
      "Epoch 296: Training Avg. Loss:   0.150\n",
      "Epoch 297: Training Avg. Loss:   0.150\n",
      "Epoch 298: Training Avg. Loss:   0.150\n",
      "Epoch 299: Training Avg. Loss:   0.150\n",
      "Epoch 300: Training Avg. Loss:   0.150\n",
      "Epoch 301: Training Avg. Loss:   0.150\n",
      "Epoch 302: Training Avg. Loss:   0.150\n",
      "Epoch 303: Training Avg. Loss:   0.150\n",
      "Epoch 304: Training Avg. Loss:   0.150\n",
      "Epoch 305: Training Avg. Loss:   0.150\n",
      "Epoch 306: Training Avg. Loss:   0.150\n",
      "Epoch 307: Training Avg. Loss:   0.150\n",
      "Epoch 308: Training Avg. Loss:   0.150\n",
      "Epoch 309: Training Avg. Loss:   0.150\n",
      "Epoch 310: Training Avg. Loss:   0.150\n",
      "Epoch 311: Training Avg. Loss:   0.150\n",
      "Epoch 312: Training Avg. Loss:   0.150\n",
      "Epoch 313: Training Avg. Loss:   0.150\n",
      "Epoch 314: Training Avg. Loss:   0.150\n",
      "Epoch 315: Training Avg. Loss:   0.150\n",
      "Epoch 316: Training Avg. Loss:   0.150\n",
      "Epoch 317: Training Avg. Loss:   0.150\n",
      "Epoch 318: Training Avg. Loss:   0.150\n",
      "Epoch 319: Training Avg. Loss:   0.150\n",
      "Epoch 320: Training Avg. Loss:   0.150\n",
      "Epoch 321: Training Avg. Loss:   0.150\n",
      "Epoch 322: Training Avg. Loss:   0.150\n",
      "Epoch 323: Training Avg. Loss:   0.150\n",
      "Epoch 324: Training Avg. Loss:   0.150\n",
      "Epoch 325: Training Avg. Loss:   0.150\n",
      "Epoch 326: Training Avg. Loss:   0.150\n",
      "Epoch 327: Training Avg. Loss:   0.150\n",
      "Epoch 328: Training Avg. Loss:   0.150\n",
      "Epoch 329: Training Avg. Loss:   0.150\n",
      "Epoch 330: Training Avg. Loss:   0.150\n",
      "Epoch 331: Training Avg. Loss:   0.150\n",
      "Epoch 332: Training Avg. Loss:   0.150\n",
      "Epoch 333: Training Avg. Loss:   0.150\n",
      "Epoch 334: Training Avg. Loss:   0.150\n",
      "Epoch 335: Training Avg. Loss:   0.150\n",
      "Epoch 336: Training Avg. Loss:   0.150\n",
      "Epoch 337: Training Avg. Loss:   0.150\n",
      "Epoch 338: Training Avg. Loss:   0.150\n",
      "Epoch 339: Training Avg. Loss:   0.150\n",
      "Epoch 340: Training Avg. Loss:   0.150\n",
      "Epoch 341: Training Avg. Loss:   0.150\n",
      "Epoch 342: Training Avg. Loss:   0.150\n",
      "Epoch 343: Training Avg. Loss:   0.150\n",
      "Epoch 344: Training Avg. Loss:   0.150\n",
      "Epoch 345: Training Avg. Loss:   0.150\n",
      "Epoch 346: Training Avg. Loss:   0.150\n",
      "Epoch 347: Training Avg. Loss:   0.150\n",
      "Epoch 348: Training Avg. Loss:   0.150\n",
      "Epoch 349: Training Avg. Loss:   0.150\n",
      "Epoch 350: Training Avg. Loss:   0.150\n",
      "Epoch 351: Training Avg. Loss:   0.150\n",
      "Epoch 352: Training Avg. Loss:   0.150\n",
      "Epoch 353: Training Avg. Loss:   0.150\n",
      "Epoch 354: Training Avg. Loss:   0.150\n",
      "Epoch 355: Training Avg. Loss:   0.150\n",
      "Epoch 356: Training Avg. Loss:   0.150\n",
      "Epoch 357: Training Avg. Loss:   0.150\n",
      "Epoch 358: Training Avg. Loss:   0.150\n",
      "Epoch 359: Training Avg. Loss:   0.150\n",
      "Epoch 360: Training Avg. Loss:   0.150\n",
      "Epoch 361: Training Avg. Loss:   0.150\n",
      "Epoch 362: Training Avg. Loss:   0.150\n",
      "Epoch 363: Training Avg. Loss:   0.150\n",
      "Epoch 364: Training Avg. Loss:   0.150\n",
      "Epoch 365: Training Avg. Loss:   0.150\n",
      "Epoch 366: Training Avg. Loss:   0.150\n",
      "Epoch 367: Training Avg. Loss:   0.150\n",
      "Epoch 368: Training Avg. Loss:   0.150\n",
      "Epoch 369: Training Avg. Loss:   0.150\n",
      "Epoch 370: Training Avg. Loss:   0.150\n",
      "Epoch 371: Training Avg. Loss:   0.150\n",
      "Epoch 372: Training Avg. Loss:   0.150\n",
      "Epoch 373: Training Avg. Loss:   0.150\n",
      "Epoch 374: Training Avg. Loss:   0.150\n",
      "Epoch 375: Training Avg. Loss:   0.150\n",
      "Epoch 376: Training Avg. Loss:   0.150\n",
      "Epoch 377: Training Avg. Loss:   0.150\n",
      "Epoch 378: Training Avg. Loss:   0.150\n",
      "Epoch 379: Training Avg. Loss:   0.150\n",
      "Epoch 380: Training Avg. Loss:   0.150\n",
      "Epoch 381: Training Avg. Loss:   0.150\n",
      "Epoch 382: Training Avg. Loss:   0.150\n",
      "Epoch 383: Training Avg. Loss:   0.150\n",
      "Epoch 384: Training Avg. Loss:   0.150\n",
      "Epoch 385: Training Avg. Loss:   0.150\n",
      "Epoch 386: Training Avg. Loss:   0.150\n",
      "Epoch 387: Training Avg. Loss:   0.150\n",
      "Epoch 388: Training Avg. Loss:   0.150\n",
      "Epoch 389: Training Avg. Loss:   0.150\n",
      "Epoch 390: Training Avg. Loss:   0.150\n",
      "Epoch 391: Training Avg. Loss:   0.150\n",
      "Epoch 392: Training Avg. Loss:   0.150\n",
      "Epoch 393: Training Avg. Loss:   0.150\n",
      "Epoch 394: Training Avg. Loss:   0.150\n",
      "Epoch 395: Training Avg. Loss:   0.150\n",
      "Epoch 396: Training Avg. Loss:   0.150\n",
      "Epoch 397: Training Avg. Loss:   0.150\n",
      "Epoch 398: Training Avg. Loss:   0.150\n",
      "Epoch 399: Training Avg. Loss:   0.150\n",
      "Epoch 400: Training Avg. Loss:   0.150\n",
      "Epoch 401: Training Avg. Loss:   0.150\n",
      "Epoch 402: Training Avg. Loss:   0.150\n",
      "Epoch 403: Training Avg. Loss:   0.150\n",
      "Epoch 404: Training Avg. Loss:   0.150\n",
      "Epoch 405: Training Avg. Loss:   0.150\n",
      "Epoch 406: Training Avg. Loss:   0.150\n",
      "Epoch 407: Training Avg. Loss:   0.150\n",
      "Epoch 408: Training Avg. Loss:   0.150\n",
      "Epoch 409: Training Avg. Loss:   0.150\n",
      "Epoch 410: Training Avg. Loss:   0.150\n",
      "Epoch 411: Training Avg. Loss:   0.150\n",
      "Epoch 412: Training Avg. Loss:   0.150\n",
      "Epoch 413: Training Avg. Loss:   0.150\n",
      "Epoch 414: Training Avg. Loss:   0.150\n",
      "Epoch 415: Training Avg. Loss:   0.150\n",
      "Epoch 416: Training Avg. Loss:   0.150\n",
      "Epoch 417: Training Avg. Loss:   0.150\n",
      "Epoch 418: Training Avg. Loss:   0.150\n",
      "Epoch 419: Training Avg. Loss:   0.150\n",
      "Epoch 420: Training Avg. Loss:   0.150\n",
      "Epoch 421: Training Avg. Loss:   0.150\n",
      "Epoch 422: Training Avg. Loss:   0.150\n",
      "Epoch 423: Training Avg. Loss:   0.150\n",
      "Epoch 424: Training Avg. Loss:   0.150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 425: Training Avg. Loss:   0.150\n",
      "Epoch 426: Training Avg. Loss:   0.150\n",
      "Epoch 427: Training Avg. Loss:   0.150\n",
      "Epoch 428: Training Avg. Loss:   0.150\n",
      "Epoch 429: Training Avg. Loss:   0.150\n",
      "Epoch 430: Training Avg. Loss:   0.150\n",
      "Epoch 431: Training Avg. Loss:   0.150\n",
      "Epoch 432: Training Avg. Loss:   0.150\n",
      "Epoch 433: Training Avg. Loss:   0.150\n",
      "Epoch 434: Training Avg. Loss:   0.150\n",
      "Epoch 435: Training Avg. Loss:   0.150\n",
      "Epoch 436: Training Avg. Loss:   0.150\n",
      "Epoch 437: Training Avg. Loss:   0.150\n",
      "Epoch 438: Training Avg. Loss:   0.150\n",
      "Epoch 439: Training Avg. Loss:   0.150\n",
      "Epoch 440: Training Avg. Loss:   0.150\n",
      "Epoch 441: Training Avg. Loss:   0.150\n",
      "Epoch 442: Training Avg. Loss:   0.150\n",
      "Epoch 443: Training Avg. Loss:   0.150\n",
      "Epoch 444: Training Avg. Loss:   0.150\n",
      "Epoch 445: Training Avg. Loss:   0.150\n",
      "Epoch 446: Training Avg. Loss:   0.150\n",
      "Epoch 447: Training Avg. Loss:   0.150\n",
      "Epoch 448: Training Avg. Loss:   0.150\n",
      "Epoch 449: Training Avg. Loss:   0.150\n",
      "Epoch 450: Training Avg. Loss:   0.150\n",
      "Epoch 451: Training Avg. Loss:   0.150\n",
      "Epoch 452: Training Avg. Loss:   0.150\n",
      "Epoch 453: Training Avg. Loss:   0.150\n",
      "Epoch 454: Training Avg. Loss:   0.150\n",
      "Epoch 455: Training Avg. Loss:   0.150\n",
      "Epoch 456: Training Avg. Loss:   0.150\n",
      "Epoch 457: Training Avg. Loss:   0.150\n",
      "Epoch 458: Training Avg. Loss:   0.150\n",
      "Epoch 459: Training Avg. Loss:   0.150\n",
      "Epoch 460: Training Avg. Loss:   0.150\n",
      "Epoch 461: Training Avg. Loss:   0.150\n",
      "Epoch 462: Training Avg. Loss:   0.150\n",
      "Epoch 463: Training Avg. Loss:   0.150\n",
      "Epoch 464: Training Avg. Loss:   0.150\n",
      "Epoch 465: Training Avg. Loss:   0.150\n",
      "Epoch 466: Training Avg. Loss:   0.150\n",
      "Epoch 467: Training Avg. Loss:   0.150\n",
      "Epoch 468: Training Avg. Loss:   0.150\n",
      "Epoch 469: Training Avg. Loss:   0.150\n",
      "Epoch 470: Training Avg. Loss:   0.150\n",
      "Epoch 471: Training Avg. Loss:   0.150\n",
      "Epoch 472: Training Avg. Loss:   0.150\n",
      "Epoch 473: Training Avg. Loss:   0.150\n",
      "Epoch 474: Training Avg. Loss:   0.150\n",
      "Epoch 475: Training Avg. Loss:   0.150\n",
      "Epoch 476: Training Avg. Loss:   0.150\n",
      "Epoch 477: Training Avg. Loss:   0.150\n",
      "Epoch 478: Training Avg. Loss:   0.150\n",
      "Epoch 479: Training Avg. Loss:   0.150\n",
      "Epoch 480: Training Avg. Loss:   0.150\n",
      "Epoch 481: Training Avg. Loss:   0.150\n",
      "Epoch 482: Training Avg. Loss:   0.150\n",
      "Epoch 483: Training Avg. Loss:   0.150\n",
      "Epoch 484: Training Avg. Loss:   0.150\n",
      "Epoch 485: Training Avg. Loss:   0.150\n",
      "Epoch 486: Training Avg. Loss:   0.150\n",
      "Epoch 487: Training Avg. Loss:   0.150\n",
      "Epoch 488: Training Avg. Loss:   0.150\n",
      "Epoch 489: Training Avg. Loss:   0.150\n",
      "Epoch 490: Training Avg. Loss:   0.150\n",
      "Epoch 491: Training Avg. Loss:   0.150\n",
      "Epoch 492: Training Avg. Loss:   0.150\n",
      "Epoch 493: Training Avg. Loss:   0.150\n",
      "Epoch 494: Training Avg. Loss:   0.150\n",
      "Epoch 495: Training Avg. Loss:   0.150\n",
      "Epoch 496: Training Avg. Loss:   0.150\n",
      "Epoch 497: Training Avg. Loss:   0.150\n",
      "Epoch 498: Training Avg. Loss:   0.150\n",
      "Epoch 499: Training Avg. Loss:   0.150\n",
      "Epoch 500: Training Avg. Loss:   0.150\n",
      "Epoch 501: Training Avg. Loss:   0.150\n",
      "Epoch 502: Training Avg. Loss:   0.150\n",
      "Epoch 503: Training Avg. Loss:   0.150\n",
      "Epoch 504: Training Avg. Loss:   0.150\n",
      "Epoch 505: Training Avg. Loss:   0.150\n",
      "Epoch 506: Training Avg. Loss:   0.150\n",
      "Epoch 507: Training Avg. Loss:   0.150\n",
      "Epoch 508: Training Avg. Loss:   0.150\n",
      "Epoch 509: Training Avg. Loss:   0.149\n",
      "Epoch 510: Training Avg. Loss:   0.149\n",
      "Epoch 511: Training Avg. Loss:   0.149\n",
      "Epoch 512: Training Avg. Loss:   0.149\n",
      "Epoch 513: Training Avg. Loss:   0.149\n",
      "Epoch 514: Training Avg. Loss:   0.149\n",
      "Epoch 515: Training Avg. Loss:   0.149\n",
      "Epoch 516: Training Avg. Loss:   0.149\n",
      "Epoch 517: Training Avg. Loss:   0.149\n",
      "Epoch 518: Training Avg. Loss:   0.149\n",
      "Epoch 519: Training Avg. Loss:   0.149\n",
      "Epoch 520: Training Avg. Loss:   0.149\n",
      "Epoch 521: Training Avg. Loss:   0.149\n",
      "Epoch 522: Training Avg. Loss:   0.149\n",
      "Epoch 523: Training Avg. Loss:   0.149\n",
      "Epoch 524: Training Avg. Loss:   0.149\n",
      "Epoch 525: Training Avg. Loss:   0.149\n",
      "Epoch 526: Training Avg. Loss:   0.149\n",
      "Epoch 527: Training Avg. Loss:   0.149\n",
      "Epoch 528: Training Avg. Loss:   0.149\n",
      "Epoch 529: Training Avg. Loss:   0.149\n",
      "Epoch 530: Training Avg. Loss:   0.149\n",
      "Epoch 531: Training Avg. Loss:   0.149\n",
      "Epoch 532: Training Avg. Loss:   0.149\n",
      "Epoch 533: Training Avg. Loss:   0.149\n",
      "Epoch 534: Training Avg. Loss:   0.149\n",
      "Epoch 535: Training Avg. Loss:   0.149\n",
      "Epoch 536: Training Avg. Loss:   0.149\n",
      "Epoch 537: Training Avg. Loss:   0.149\n",
      "Epoch 538: Training Avg. Loss:   0.149\n",
      "Epoch 539: Training Avg. Loss:   0.149\n",
      "Epoch 540: Training Avg. Loss:   0.149\n",
      "Epoch 541: Training Avg. Loss:   0.149\n",
      "Epoch 542: Training Avg. Loss:   0.149\n",
      "Epoch 543: Training Avg. Loss:   0.149\n",
      "Epoch 544: Training Avg. Loss:   0.149\n",
      "Epoch 545: Training Avg. Loss:   0.149\n",
      "Epoch 546: Training Avg. Loss:   0.149\n",
      "Epoch 547: Training Avg. Loss:   0.149\n",
      "Epoch 548: Training Avg. Loss:   0.149\n",
      "Epoch 549: Training Avg. Loss:   0.149\n",
      "Epoch 550: Training Avg. Loss:   0.149\n",
      "Epoch 551: Training Avg. Loss:   0.149\n",
      "Epoch 552: Training Avg. Loss:   0.149\n",
      "Epoch 553: Training Avg. Loss:   0.149\n",
      "Epoch 554: Training Avg. Loss:   0.149\n",
      "Epoch 555: Training Avg. Loss:   0.149\n",
      "Epoch 556: Training Avg. Loss:   0.149\n",
      "Epoch 557: Training Avg. Loss:   0.149\n",
      "Epoch 558: Training Avg. Loss:   0.149\n",
      "Epoch 559: Training Avg. Loss:   0.149\n",
      "Epoch 560: Training Avg. Loss:   0.149\n",
      "Epoch 561: Training Avg. Loss:   0.149\n",
      "Epoch 562: Training Avg. Loss:   0.149\n",
      "Epoch 563: Training Avg. Loss:   0.149\n",
      "Epoch 564: Training Avg. Loss:   0.149\n",
      "Epoch 565: Training Avg. Loss:   0.149\n",
      "Epoch 566: Training Avg. Loss:   0.149\n",
      "Epoch 567: Training Avg. Loss:   0.149\n",
      "Epoch 568: Training Avg. Loss:   0.149\n",
      "Epoch 569: Training Avg. Loss:   0.149\n",
      "Epoch 570: Training Avg. Loss:   0.149\n",
      "Epoch 571: Training Avg. Loss:   0.149\n",
      "Epoch 572: Training Avg. Loss:   0.149\n",
      "Epoch 573: Training Avg. Loss:   0.149\n",
      "Epoch 574: Training Avg. Loss:   0.149\n",
      "Epoch 575: Training Avg. Loss:   0.149\n",
      "Epoch 576: Training Avg. Loss:   0.149\n",
      "Epoch 577: Training Avg. Loss:   0.149\n",
      "Epoch 578: Training Avg. Loss:   0.149\n",
      "Epoch 579: Training Avg. Loss:   0.149\n",
      "Epoch 580: Training Avg. Loss:   0.149\n",
      "Epoch 581: Training Avg. Loss:   0.149\n",
      "Epoch 582: Training Avg. Loss:   0.149\n",
      "Epoch 583: Training Avg. Loss:   0.149\n",
      "Epoch 584: Training Avg. Loss:   0.149\n",
      "Epoch 585: Training Avg. Loss:   0.149\n",
      "Epoch 586: Training Avg. Loss:   0.149\n",
      "Epoch 587: Training Avg. Loss:   0.149\n",
      "Epoch 588: Training Avg. Loss:   0.149\n",
      "Epoch 589: Training Avg. Loss:   0.149\n",
      "Epoch 590: Training Avg. Loss:   0.149\n",
      "Epoch 591: Training Avg. Loss:   0.149\n",
      "Epoch 592: Training Avg. Loss:   0.149\n",
      "Epoch 593: Training Avg. Loss:   0.149\n",
      "Epoch 594: Training Avg. Loss:   0.149\n",
      "Epoch 595: Training Avg. Loss:   0.149\n",
      "Epoch 596: Training Avg. Loss:   0.149\n",
      "Epoch 597: Training Avg. Loss:   0.149\n",
      "Epoch 598: Training Avg. Loss:   0.149\n",
      "Epoch 599: Training Avg. Loss:   0.149\n",
      "Epoch 600: Training Avg. Loss:   0.149\n",
      "Epoch 601: Training Avg. Loss:   0.149\n",
      "Epoch 602: Training Avg. Loss:   0.149\n",
      "Epoch 603: Training Avg. Loss:   0.149\n",
      "Epoch 604: Training Avg. Loss:   0.149\n",
      "Epoch 605: Training Avg. Loss:   0.149\n",
      "Epoch 606: Training Avg. Loss:   0.149\n",
      "Epoch 607: Training Avg. Loss:   0.149\n",
      "Epoch 608: Training Avg. Loss:   0.149\n",
      "Epoch 609: Training Avg. Loss:   0.149\n",
      "Epoch 610: Training Avg. Loss:   0.149\n",
      "Epoch 611: Training Avg. Loss:   0.149\n",
      "Epoch 612: Training Avg. Loss:   0.149\n",
      "Epoch 613: Training Avg. Loss:   0.149\n",
      "Epoch 614: Training Avg. Loss:   0.149\n",
      "Epoch 615: Training Avg. Loss:   0.149\n",
      "Epoch 616: Training Avg. Loss:   0.149\n",
      "Epoch 617: Training Avg. Loss:   0.149\n",
      "Epoch 618: Training Avg. Loss:   0.149\n",
      "Epoch 619: Training Avg. Loss:   0.149\n",
      "Epoch 620: Training Avg. Loss:   0.149\n",
      "Epoch 621: Training Avg. Loss:   0.149\n",
      "Epoch 622: Training Avg. Loss:   0.149\n",
      "Epoch 623: Training Avg. Loss:   0.149\n",
      "Epoch 624: Training Avg. Loss:   0.149\n",
      "Epoch 625: Training Avg. Loss:   0.149\n",
      "Epoch 626: Training Avg. Loss:   0.149\n",
      "Epoch 627: Training Avg. Loss:   0.149\n",
      "Epoch 628: Training Avg. Loss:   0.149\n",
      "Epoch 629: Training Avg. Loss:   0.149\n",
      "Epoch 630: Training Avg. Loss:   0.149\n",
      "Epoch 631: Training Avg. Loss:   0.149\n",
      "Epoch 632: Training Avg. Loss:   0.149\n",
      "Epoch 633: Training Avg. Loss:   0.149\n",
      "Epoch 634: Training Avg. Loss:   0.149\n",
      "Epoch 635: Training Avg. Loss:   0.149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 636: Training Avg. Loss:   0.149\n",
      "Epoch 637: Training Avg. Loss:   0.149\n",
      "Epoch 638: Training Avg. Loss:   0.149\n",
      "Epoch 639: Training Avg. Loss:   0.149\n",
      "Epoch 640: Training Avg. Loss:   0.149\n",
      "Epoch 641: Training Avg. Loss:   0.149\n",
      "Epoch 642: Training Avg. Loss:   0.149\n",
      "Epoch 643: Training Avg. Loss:   0.149\n",
      "Epoch 644: Training Avg. Loss:   0.149\n",
      "Epoch 645: Training Avg. Loss:   0.149\n",
      "Epoch 646: Training Avg. Loss:   0.149\n",
      "Epoch 647: Training Avg. Loss:   0.149\n",
      "Epoch 648: Training Avg. Loss:   0.149\n",
      "Epoch 649: Training Avg. Loss:   0.149\n",
      "Epoch 650: Training Avg. Loss:   0.149\n",
      "Epoch 651: Training Avg. Loss:   0.149\n",
      "Epoch 652: Training Avg. Loss:   0.149\n",
      "Epoch 653: Training Avg. Loss:   0.149\n",
      "Epoch 654: Training Avg. Loss:   0.149\n",
      "Epoch 655: Training Avg. Loss:   0.149\n",
      "Epoch 656: Training Avg. Loss:   0.149\n",
      "Epoch 657: Training Avg. Loss:   0.149\n",
      "Epoch 658: Training Avg. Loss:   0.149\n",
      "Epoch 659: Training Avg. Loss:   0.149\n",
      "Epoch 660: Training Avg. Loss:   0.149\n",
      "Epoch 661: Training Avg. Loss:   0.149\n",
      "Epoch 662: Training Avg. Loss:   0.149\n",
      "Epoch 663: Training Avg. Loss:   0.149\n",
      "Epoch 664: Training Avg. Loss:   0.149\n",
      "Epoch 665: Training Avg. Loss:   0.149\n",
      "Epoch 666: Training Avg. Loss:   0.149\n",
      "Epoch 667: Training Avg. Loss:   0.149\n",
      "Epoch 668: Training Avg. Loss:   0.149\n",
      "Epoch 669: Training Avg. Loss:   0.149\n",
      "Epoch 670: Training Avg. Loss:   0.149\n",
      "Epoch 671: Training Avg. Loss:   0.149\n",
      "Epoch 672: Training Avg. Loss:   0.149\n",
      "Epoch 673: Training Avg. Loss:   0.149\n",
      "Epoch 674: Training Avg. Loss:   0.149\n",
      "Epoch 675: Training Avg. Loss:   0.149\n",
      "Epoch 676: Training Avg. Loss:   0.149\n",
      "Epoch 677: Training Avg. Loss:   0.149\n",
      "Epoch 678: Training Avg. Loss:   0.149\n",
      "Epoch 679: Training Avg. Loss:   0.149\n",
      "Epoch 680: Training Avg. Loss:   0.149\n",
      "Epoch 681: Training Avg. Loss:   0.149\n",
      "Epoch 682: Training Avg. Loss:   0.149\n",
      "Epoch 683: Training Avg. Loss:   0.149\n",
      "Epoch 684: Training Avg. Loss:   0.149\n",
      "Epoch 685: Training Avg. Loss:   0.149\n",
      "Epoch 686: Training Avg. Loss:   0.149\n",
      "Epoch 687: Training Avg. Loss:   0.149\n",
      "Epoch 688: Training Avg. Loss:   0.149\n",
      "Epoch 689: Training Avg. Loss:   0.149\n",
      "Epoch 690: Training Avg. Loss:   0.149\n",
      "Epoch 691: Training Avg. Loss:   0.149\n",
      "Epoch 692: Training Avg. Loss:   0.149\n",
      "Epoch 693: Training Avg. Loss:   0.149\n",
      "Epoch 694: Training Avg. Loss:   0.149\n",
      "Epoch 695: Training Avg. Loss:   0.149\n",
      "Epoch 696: Training Avg. Loss:   0.149\n",
      "Epoch 697: Training Avg. Loss:   0.149\n",
      "Epoch 698: Training Avg. Loss:   0.149\n",
      "Epoch 699: Training Avg. Loss:   0.149\n",
      "Epoch 700: Training Avg. Loss:   0.149\n",
      "Epoch 701: Training Avg. Loss:   0.149\n",
      "Epoch 702: Training Avg. Loss:   0.149\n",
      "Epoch 703: Training Avg. Loss:   0.149\n",
      "Epoch 704: Training Avg. Loss:   0.149\n",
      "Epoch 705: Training Avg. Loss:   0.149\n",
      "Epoch 706: Training Avg. Loss:   0.149\n",
      "Epoch 707: Training Avg. Loss:   0.149\n",
      "Epoch 708: Training Avg. Loss:   0.149\n",
      "Epoch 709: Training Avg. Loss:   0.149\n",
      "Epoch 710: Training Avg. Loss:   0.149\n",
      "Epoch 711: Training Avg. Loss:   0.149\n",
      "Epoch 712: Training Avg. Loss:   0.149\n",
      "Epoch 713: Training Avg. Loss:   0.149\n",
      "Epoch 714: Training Avg. Loss:   0.149\n",
      "Epoch 715: Training Avg. Loss:   0.149\n",
      "Epoch 716: Training Avg. Loss:   0.149\n",
      "Epoch 717: Training Avg. Loss:   0.149\n",
      "Epoch 718: Training Avg. Loss:   0.149\n",
      "Epoch 719: Training Avg. Loss:   0.149\n",
      "Epoch 720: Training Avg. Loss:   0.149\n",
      "Epoch 721: Training Avg. Loss:   0.149\n",
      "Epoch 722: Training Avg. Loss:   0.149\n",
      "Epoch 723: Training Avg. Loss:   0.149\n",
      "Epoch 724: Training Avg. Loss:   0.149\n",
      "Epoch 725: Training Avg. Loss:   0.149\n",
      "Epoch 726: Training Avg. Loss:   0.149\n",
      "Epoch 727: Training Avg. Loss:   0.149\n",
      "Epoch 728: Training Avg. Loss:   0.149\n",
      "Epoch 729: Training Avg. Loss:   0.149\n",
      "Epoch 730: Training Avg. Loss:   0.149\n",
      "Epoch 731: Training Avg. Loss:   0.149\n",
      "Epoch 732: Training Avg. Loss:   0.149\n",
      "Epoch 733: Training Avg. Loss:   0.149\n",
      "Epoch 734: Training Avg. Loss:   0.149\n",
      "Epoch 735: Training Avg. Loss:   0.149\n",
      "Epoch 736: Training Avg. Loss:   0.149\n",
      "Epoch 737: Training Avg. Loss:   0.149\n",
      "Epoch 738: Training Avg. Loss:   0.149\n",
      "Epoch 739: Training Avg. Loss:   0.149\n",
      "Epoch 740: Training Avg. Loss:   0.149\n",
      "Epoch 741: Training Avg. Loss:   0.149\n",
      "Epoch 742: Training Avg. Loss:   0.149\n",
      "Epoch 743: Training Avg. Loss:   0.149\n",
      "Epoch 744: Training Avg. Loss:   0.149\n",
      "Epoch 745: Training Avg. Loss:   0.149\n",
      "Epoch 746: Training Avg. Loss:   0.149\n",
      "Epoch 747: Training Avg. Loss:   0.149\n",
      "Epoch 748: Training Avg. Loss:   0.149\n",
      "Epoch 749: Training Avg. Loss:   0.149\n",
      "Epoch 750: Training Avg. Loss:   0.149\n",
      "Epoch 751: Training Avg. Loss:   0.149\n",
      "Epoch 752: Training Avg. Loss:   0.149\n",
      "Epoch 753: Training Avg. Loss:   0.149\n",
      "Epoch 754: Training Avg. Loss:   0.149\n",
      "Epoch 755: Training Avg. Loss:   0.149\n",
      "Epoch 756: Training Avg. Loss:   0.149\n",
      "Epoch 757: Training Avg. Loss:   0.149\n",
      "Epoch 758: Training Avg. Loss:   0.149\n",
      "Epoch 759: Training Avg. Loss:   0.149\n",
      "Epoch 760: Training Avg. Loss:   0.149\n",
      "Epoch 761: Training Avg. Loss:   0.149\n",
      "Epoch 762: Training Avg. Loss:   0.149\n",
      "Epoch 763: Training Avg. Loss:   0.149\n",
      "Epoch 764: Training Avg. Loss:   0.149\n",
      "Epoch 765: Training Avg. Loss:   0.149\n",
      "Epoch 766: Training Avg. Loss:   0.149\n",
      "Epoch 767: Training Avg. Loss:   0.149\n",
      "Epoch 768: Training Avg. Loss:   0.149\n",
      "Epoch 769: Training Avg. Loss:   0.149\n",
      "Epoch 770: Training Avg. Loss:   0.149\n",
      "Epoch 771: Training Avg. Loss:   0.149\n",
      "Epoch 772: Training Avg. Loss:   0.149\n",
      "Epoch 773: Training Avg. Loss:   0.149\n",
      "Epoch 774: Training Avg. Loss:   0.149\n",
      "Epoch 775: Training Avg. Loss:   0.149\n",
      "Epoch 776: Training Avg. Loss:   0.149\n",
      "Epoch 777: Training Avg. Loss:   0.149\n",
      "Epoch 778: Training Avg. Loss:   0.149\n",
      "Epoch 779: Training Avg. Loss:   0.149\n",
      "Epoch 780: Training Avg. Loss:   0.149\n",
      "Epoch 781: Training Avg. Loss:   0.149\n",
      "Epoch 782: Training Avg. Loss:   0.149\n",
      "Epoch 783: Training Avg. Loss:   0.149\n",
      "Epoch 784: Training Avg. Loss:   0.149\n",
      "Epoch 785: Training Avg. Loss:   0.149\n",
      "Epoch 786: Training Avg. Loss:   0.149\n",
      "Epoch 787: Training Avg. Loss:   0.149\n",
      "Epoch 788: Training Avg. Loss:   0.149\n",
      "Epoch 789: Training Avg. Loss:   0.149\n",
      "Epoch 790: Training Avg. Loss:   0.149\n",
      "Epoch 791: Training Avg. Loss:   0.149\n",
      "Epoch 792: Training Avg. Loss:   0.149\n",
      "Epoch 793: Training Avg. Loss:   0.149\n",
      "Epoch 794: Training Avg. Loss:   0.149\n",
      "Epoch 795: Training Avg. Loss:   0.149\n",
      "Epoch 796: Training Avg. Loss:   0.149\n",
      "Epoch 797: Training Avg. Loss:   0.149\n",
      "Epoch 798: Training Avg. Loss:   0.149\n",
      "Epoch 799: Training Avg. Loss:   0.149\n",
      "Epoch 800: Training Avg. Loss:   0.149\n",
      "Epoch 801: Training Avg. Loss:   0.149\n",
      "Epoch 802: Training Avg. Loss:   0.149\n",
      "Epoch 803: Training Avg. Loss:   0.149\n",
      "Epoch 804: Training Avg. Loss:   0.149\n",
      "Epoch 805: Training Avg. Loss:   0.149\n",
      "Epoch 806: Training Avg. Loss:   0.149\n",
      "Epoch 807: Training Avg. Loss:   0.149\n",
      "Epoch 808: Training Avg. Loss:   0.149\n",
      "Epoch 809: Training Avg. Loss:   0.149\n",
      "Epoch 810: Training Avg. Loss:   0.149\n",
      "Epoch 811: Training Avg. Loss:   0.149\n",
      "Epoch 812: Training Avg. Loss:   0.149\n",
      "Epoch 813: Training Avg. Loss:   0.149\n",
      "Epoch 814: Training Avg. Loss:   0.149\n",
      "Epoch 815: Training Avg. Loss:   0.149\n",
      "Epoch 816: Training Avg. Loss:   0.149\n",
      "Epoch 817: Training Avg. Loss:   0.149\n",
      "Epoch 818: Training Avg. Loss:   0.149\n",
      "Epoch 819: Training Avg. Loss:   0.149\n",
      "Epoch 820: Training Avg. Loss:   0.149\n",
      "Epoch 821: Training Avg. Loss:   0.149\n",
      "Epoch 822: Training Avg. Loss:   0.149\n",
      "Epoch 823: Training Avg. Loss:   0.149\n",
      "Epoch 824: Training Avg. Loss:   0.149\n",
      "Epoch 825: Training Avg. Loss:   0.149\n",
      "Epoch 826: Training Avg. Loss:   0.149\n",
      "Epoch 827: Training Avg. Loss:   0.149\n",
      "Epoch 828: Training Avg. Loss:   0.149\n",
      "Epoch 829: Training Avg. Loss:   0.149\n",
      "Epoch 830: Training Avg. Loss:   0.149\n",
      "Epoch 831: Training Avg. Loss:   0.149\n",
      "Epoch 832: Training Avg. Loss:   0.149\n",
      "Epoch 833: Training Avg. Loss:   0.149\n",
      "Epoch 834: Training Avg. Loss:   0.149\n",
      "Epoch 835: Training Avg. Loss:   0.149\n",
      "Epoch 836: Training Avg. Loss:   0.149\n",
      "Epoch 837: Training Avg. Loss:   0.149\n",
      "Epoch 838: Training Avg. Loss:   0.149\n",
      "Epoch 839: Training Avg. Loss:   0.149\n",
      "Epoch 840: Training Avg. Loss:   0.149\n",
      "Epoch 841: Training Avg. Loss:   0.149\n",
      "Epoch 842: Training Avg. Loss:   0.149\n",
      "Epoch 843: Training Avg. Loss:   0.149\n",
      "Epoch 844: Training Avg. Loss:   0.149\n",
      "Epoch 845: Training Avg. Loss:   0.149\n",
      "Epoch 846: Training Avg. Loss:   0.149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 847: Training Avg. Loss:   0.149\n",
      "Epoch 848: Training Avg. Loss:   0.149\n",
      "Epoch 849: Training Avg. Loss:   0.149\n",
      "Epoch 850: Training Avg. Loss:   0.149\n",
      "Epoch 851: Training Avg. Loss:   0.149\n",
      "Epoch 852: Training Avg. Loss:   0.149\n",
      "Epoch 853: Training Avg. Loss:   0.149\n",
      "Epoch 854: Training Avg. Loss:   0.149\n",
      "Epoch 855: Training Avg. Loss:   0.149\n",
      "Epoch 856: Training Avg. Loss:   0.149\n",
      "Epoch 857: Training Avg. Loss:   0.149\n",
      "Epoch 858: Training Avg. Loss:   0.149\n",
      "Epoch 859: Training Avg. Loss:   0.149\n",
      "Epoch 860: Training Avg. Loss:   0.149\n",
      "Epoch 861: Training Avg. Loss:   0.149\n",
      "Epoch 862: Training Avg. Loss:   0.149\n",
      "Epoch 863: Training Avg. Loss:   0.149\n",
      "Epoch 864: Training Avg. Loss:   0.149\n",
      "Epoch 865: Training Avg. Loss:   0.149\n",
      "Epoch 866: Training Avg. Loss:   0.149\n",
      "Epoch 867: Training Avg. Loss:   0.149\n",
      "Epoch 868: Training Avg. Loss:   0.149\n",
      "Epoch 869: Training Avg. Loss:   0.149\n",
      "Epoch 870: Training Avg. Loss:   0.149\n",
      "Epoch 871: Training Avg. Loss:   0.149\n",
      "Epoch 872: Training Avg. Loss:   0.149\n",
      "Epoch 873: Training Avg. Loss:   0.149\n",
      "Epoch 874: Training Avg. Loss:   0.149\n",
      "Epoch 875: Training Avg. Loss:   0.149\n",
      "Epoch 876: Training Avg. Loss:   0.149\n",
      "Epoch 877: Training Avg. Loss:   0.149\n",
      "Epoch 878: Training Avg. Loss:   0.149\n",
      "Epoch 879: Training Avg. Loss:   0.149\n",
      "Epoch 880: Training Avg. Loss:   0.149\n",
      "Epoch 881: Training Avg. Loss:   0.149\n",
      "Epoch 882: Training Avg. Loss:   0.149\n",
      "Epoch 883: Training Avg. Loss:   0.149\n",
      "Epoch 884: Training Avg. Loss:   0.149\n",
      "Epoch 885: Training Avg. Loss:   0.149\n",
      "Epoch 886: Training Avg. Loss:   0.149\n",
      "Epoch 887: Training Avg. Loss:   0.149\n",
      "Epoch 888: Training Avg. Loss:   0.149\n",
      "Epoch 889: Training Avg. Loss:   0.149\n",
      "Epoch 890: Training Avg. Loss:   0.149\n",
      "Epoch 891: Training Avg. Loss:   0.149\n",
      "Epoch 892: Training Avg. Loss:   0.149\n",
      "Epoch 893: Training Avg. Loss:   0.149\n",
      "Epoch 894: Training Avg. Loss:   0.149\n",
      "Epoch 895: Training Avg. Loss:   0.149\n",
      "Epoch 896: Training Avg. Loss:   0.149\n",
      "Epoch 897: Training Avg. Loss:   0.149\n",
      "Epoch 898: Training Avg. Loss:   0.149\n",
      "Epoch 899: Training Avg. Loss:   0.149\n",
      "Epoch 900: Training Avg. Loss:   0.149\n",
      "Epoch 901: Training Avg. Loss:   0.149\n",
      "Epoch 902: Training Avg. Loss:   0.149\n",
      "Epoch 903: Training Avg. Loss:   0.149\n",
      "Epoch 904: Training Avg. Loss:   0.149\n",
      "Epoch 905: Training Avg. Loss:   0.149\n",
      "Epoch 906: Training Avg. Loss:   0.149\n",
      "Epoch 907: Training Avg. Loss:   0.149\n",
      "Epoch 908: Training Avg. Loss:   0.149\n",
      "Epoch 909: Training Avg. Loss:   0.149\n",
      "Epoch 910: Training Avg. Loss:   0.149\n",
      "Epoch 911: Training Avg. Loss:   0.149\n",
      "Epoch 912: Training Avg. Loss:   0.149\n",
      "Epoch 913: Training Avg. Loss:   0.149\n",
      "Epoch 914: Training Avg. Loss:   0.149\n",
      "Epoch 915: Training Avg. Loss:   0.149\n",
      "Epoch 916: Training Avg. Loss:   0.149\n",
      "Epoch 917: Training Avg. Loss:   0.149\n",
      "Epoch 918: Training Avg. Loss:   0.149\n",
      "Epoch 919: Training Avg. Loss:   0.149\n",
      "Epoch 920: Training Avg. Loss:   0.149\n",
      "Epoch 921: Training Avg. Loss:   0.149\n",
      "Epoch 922: Training Avg. Loss:   0.149\n",
      "Epoch 923: Training Avg. Loss:   0.149\n",
      "Epoch 924: Training Avg. Loss:   0.149\n",
      "Epoch 925: Training Avg. Loss:   0.149\n",
      "Epoch 926: Training Avg. Loss:   0.149\n",
      "Epoch 927: Training Avg. Loss:   0.149\n",
      "Epoch 928: Training Avg. Loss:   0.149\n",
      "Epoch 929: Training Avg. Loss:   0.149\n",
      "Epoch 930: Training Avg. Loss:   0.149\n",
      "Epoch 931: Training Avg. Loss:   0.149\n",
      "Epoch 932: Training Avg. Loss:   0.149\n",
      "Epoch 933: Training Avg. Loss:   0.149\n",
      "Epoch 934: Training Avg. Loss:   0.149\n",
      "Epoch 935: Training Avg. Loss:   0.149\n",
      "Epoch 936: Training Avg. Loss:   0.149\n",
      "Epoch 937: Training Avg. Loss:   0.149\n",
      "Epoch 938: Training Avg. Loss:   0.149\n",
      "Epoch 939: Training Avg. Loss:   0.149\n",
      "Epoch 940: Training Avg. Loss:   0.149\n",
      "Epoch 941: Training Avg. Loss:   0.149\n",
      "Epoch 942: Training Avg. Loss:   0.149\n",
      "Epoch 943: Training Avg. Loss:   0.149\n",
      "Epoch 944: Training Avg. Loss:   0.149\n",
      "Epoch 945: Training Avg. Loss:   0.149\n",
      "Epoch 946: Training Avg. Loss:   0.149\n",
      "Epoch 947: Training Avg. Loss:   0.149\n",
      "Epoch 948: Training Avg. Loss:   0.149\n",
      "Epoch 949: Training Avg. Loss:   0.149\n",
      "Epoch 950: Training Avg. Loss:   0.149\n",
      "Epoch 951: Training Avg. Loss:   0.149\n",
      "Epoch 952: Training Avg. Loss:   0.149\n",
      "Epoch 953: Training Avg. Loss:   0.149\n",
      "Epoch 954: Training Avg. Loss:   0.149\n",
      "Epoch 955: Training Avg. Loss:   0.149\n",
      "Epoch 956: Training Avg. Loss:   0.149\n",
      "Epoch 957: Training Avg. Loss:   0.149\n",
      "Epoch 958: Training Avg. Loss:   0.149\n",
      "Epoch 959: Training Avg. Loss:   0.149\n",
      "Epoch 960: Training Avg. Loss:   0.149\n",
      "Epoch 961: Training Avg. Loss:   0.149\n",
      "Epoch 962: Training Avg. Loss:   0.149\n",
      "Epoch 963: Training Avg. Loss:   0.149\n",
      "Epoch 964: Training Avg. Loss:   0.149\n",
      "Epoch 965: Training Avg. Loss:   0.149\n",
      "Epoch 966: Training Avg. Loss:   0.149\n",
      "Epoch 967: Training Avg. Loss:   0.149\n",
      "Epoch 968: Training Avg. Loss:   0.149\n",
      "Epoch 969: Training Avg. Loss:   0.149\n",
      "Epoch 970: Training Avg. Loss:   0.149\n",
      "Epoch 971: Training Avg. Loss:   0.149\n",
      "Epoch 972: Training Avg. Loss:   0.149\n",
      "Epoch 973: Training Avg. Loss:   0.149\n",
      "Epoch 974: Training Avg. Loss:   0.149\n",
      "Epoch 975: Training Avg. Loss:   0.149\n",
      "Epoch 976: Training Avg. Loss:   0.149\n",
      "Epoch 977: Training Avg. Loss:   0.149\n",
      "Epoch 978: Training Avg. Loss:   0.149\n",
      "Epoch 979: Training Avg. Loss:   0.149\n",
      "Epoch 980: Training Avg. Loss:   0.149\n",
      "Epoch 981: Training Avg. Loss:   0.149\n",
      "Epoch 982: Training Avg. Loss:   0.149\n",
      "Epoch 983: Training Avg. Loss:   0.149\n",
      "Epoch 984: Training Avg. Loss:   0.149\n",
      "Epoch 985: Training Avg. Loss:   0.149\n",
      "Epoch 986: Training Avg. Loss:   0.149\n",
      "Epoch 987: Training Avg. Loss:   0.149\n",
      "Epoch 988: Training Avg. Loss:   0.149\n",
      "Epoch 989: Training Avg. Loss:   0.149\n",
      "Epoch 990: Training Avg. Loss:   0.149\n",
      "Epoch 991: Training Avg. Loss:   0.149\n",
      "Epoch 992: Training Avg. Loss:   0.149\n",
      "Epoch 993: Training Avg. Loss:   0.149\n",
      "Epoch 994: Training Avg. Loss:   0.149\n",
      "Epoch 995: Training Avg. Loss:   0.149\n",
      "Epoch 996: Training Avg. Loss:   0.149\n",
      "Epoch 997: Training Avg. Loss:   0.149\n",
      "Epoch 998: Training Avg. Loss:   0.149\n",
      "Epoch 999: Training Avg. Loss:   0.149\n",
      "Epoch 1000: Training Avg. Loss:   0.149\n",
      "Epoch 1001: Training Avg. Loss:   0.149\n",
      "Epoch 1002: Training Avg. Loss:   0.149\n",
      "Epoch 1003: Training Avg. Loss:   0.149\n",
      "Epoch 1004: Training Avg. Loss:   0.148\n",
      "Epoch 1005: Training Avg. Loss:   0.148\n",
      "Epoch 1006: Training Avg. Loss:   0.148\n",
      "Epoch 1007: Training Avg. Loss:   0.148\n",
      "Epoch 1008: Training Avg. Loss:   0.148\n",
      "Epoch 1009: Training Avg. Loss:   0.148\n",
      "Final accuracy:  0.96501\n"
     ]
    }
   ],
   "source": [
    "ann_model.train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ann = ann_model.predict(X_test,return_proba=True)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5096283"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ann.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect outputs from all models and put in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506691, 5)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([pred_lr, pred_xgb,predict_lgbm,preds_cb,predict_ann]).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results=pd.DataFrame(np.vstack([pred_lr, pred_xgb,predict_lgbm,preds_cb,predict_ann]).T,\n",
    "                          columns=['LogisticRegression','XGBoost','LightGBM','CatBoost','ANN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>LightGBM</th>\n",
       "      <th>CatBoost</th>\n",
       "      <th>ANN</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>0.005289</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.016476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021395</td>\n",
       "      <td>0.010993</td>\n",
       "      <td>0.014443</td>\n",
       "      <td>0.027886</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.027784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.041185</td>\n",
       "      <td>0.022076</td>\n",
       "      <td>0.024321</td>\n",
       "      <td>0.033240</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.037006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009547</td>\n",
       "      <td>0.010852</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.004910</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.019701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005110</td>\n",
       "      <td>0.008084</td>\n",
       "      <td>0.010443</td>\n",
       "      <td>0.005224</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.018613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LogisticRegression   XGBoost  LightGBM  CatBoost       ANN     final\n",
       "0            0.002416  0.005918  0.005289  0.004550  0.064205  0.016476\n",
       "1            0.021395  0.010993  0.014443  0.027886  0.064205  0.027784\n",
       "2            0.041185  0.022076  0.024321  0.033240  0.064205  0.037006\n",
       "3            0.009547  0.010852  0.008990  0.004910  0.064205  0.019701\n",
       "4            0.005110  0.008084  0.010443  0.005224  0.064205  0.018613"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take weighted average as prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>LightGBM</th>\n",
       "      <th>CatBoost</th>\n",
       "      <th>ANN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>0.005289</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021395</td>\n",
       "      <td>0.010993</td>\n",
       "      <td>0.014443</td>\n",
       "      <td>0.027886</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.041185</td>\n",
       "      <td>0.022076</td>\n",
       "      <td>0.024321</td>\n",
       "      <td>0.033240</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009547</td>\n",
       "      <td>0.010852</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.004910</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005110</td>\n",
       "      <td>0.008084</td>\n",
       "      <td>0.010443</td>\n",
       "      <td>0.005224</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.003064</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.014471</td>\n",
       "      <td>0.023503</td>\n",
       "      <td>0.028182</td>\n",
       "      <td>0.039060</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.066416</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.049362</td>\n",
       "      <td>0.031516</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.007479</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.033134</td>\n",
       "      <td>0.011247</td>\n",
       "      <td>0.012707</td>\n",
       "      <td>0.007715</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.007580</td>\n",
       "      <td>0.007923</td>\n",
       "      <td>0.009039</td>\n",
       "      <td>0.008924</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.007894</td>\n",
       "      <td>0.007847</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.004459</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.048409</td>\n",
       "      <td>0.028621</td>\n",
       "      <td>0.046628</td>\n",
       "      <td>0.045156</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.008861</td>\n",
       "      <td>0.005237</td>\n",
       "      <td>0.005641</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.046893</td>\n",
       "      <td>0.009155</td>\n",
       "      <td>0.010051</td>\n",
       "      <td>0.003211</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.008111</td>\n",
       "      <td>0.006615</td>\n",
       "      <td>0.005658</td>\n",
       "      <td>0.004508</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.009922</td>\n",
       "      <td>0.009066</td>\n",
       "      <td>0.010569</td>\n",
       "      <td>0.008482</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.002544</td>\n",
       "      <td>0.011766</td>\n",
       "      <td>0.020960</td>\n",
       "      <td>0.011076</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.026361</td>\n",
       "      <td>0.019570</td>\n",
       "      <td>0.020687</td>\n",
       "      <td>0.024147</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.015642</td>\n",
       "      <td>0.015990</td>\n",
       "      <td>0.019969</td>\n",
       "      <td>0.014970</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.021183</td>\n",
       "      <td>0.023950</td>\n",
       "      <td>0.021165</td>\n",
       "      <td>0.029408</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.010648</td>\n",
       "      <td>0.004870</td>\n",
       "      <td>0.008590</td>\n",
       "      <td>0.004824</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.010835</td>\n",
       "      <td>0.014243</td>\n",
       "      <td>0.017459</td>\n",
       "      <td>0.020301</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.011991</td>\n",
       "      <td>0.024342</td>\n",
       "      <td>0.025766</td>\n",
       "      <td>0.023884</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.020696</td>\n",
       "      <td>0.022160</td>\n",
       "      <td>0.024537</td>\n",
       "      <td>0.028346</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.011082</td>\n",
       "      <td>0.023398</td>\n",
       "      <td>0.021341</td>\n",
       "      <td>0.021711</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.012615</td>\n",
       "      <td>0.004870</td>\n",
       "      <td>0.008754</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.062882</td>\n",
       "      <td>0.082971</td>\n",
       "      <td>0.090604</td>\n",
       "      <td>0.016911</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.011681</td>\n",
       "      <td>0.030307</td>\n",
       "      <td>0.026427</td>\n",
       "      <td>0.022516</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.008828</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.012917</td>\n",
       "      <td>0.009984</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506661</th>\n",
       "      <td>0.010841</td>\n",
       "      <td>0.011498</td>\n",
       "      <td>0.011650</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506662</th>\n",
       "      <td>0.029749</td>\n",
       "      <td>0.026308</td>\n",
       "      <td>0.025243</td>\n",
       "      <td>0.016622</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506663</th>\n",
       "      <td>0.006620</td>\n",
       "      <td>0.009370</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.008810</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506664</th>\n",
       "      <td>0.053267</td>\n",
       "      <td>0.120187</td>\n",
       "      <td>0.078631</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506665</th>\n",
       "      <td>0.025823</td>\n",
       "      <td>0.009926</td>\n",
       "      <td>0.012104</td>\n",
       "      <td>0.009556</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506666</th>\n",
       "      <td>0.049583</td>\n",
       "      <td>0.025941</td>\n",
       "      <td>0.025993</td>\n",
       "      <td>0.016686</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506667</th>\n",
       "      <td>0.032345</td>\n",
       "      <td>0.108520</td>\n",
       "      <td>0.083989</td>\n",
       "      <td>0.090571</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506668</th>\n",
       "      <td>0.004789</td>\n",
       "      <td>0.004712</td>\n",
       "      <td>0.004767</td>\n",
       "      <td>0.005898</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506669</th>\n",
       "      <td>0.034058</td>\n",
       "      <td>0.043258</td>\n",
       "      <td>0.031323</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506670</th>\n",
       "      <td>0.031307</td>\n",
       "      <td>0.011064</td>\n",
       "      <td>0.015344</td>\n",
       "      <td>0.008937</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506671</th>\n",
       "      <td>0.005483</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>0.008440</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506672</th>\n",
       "      <td>0.007861</td>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>0.003991</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506673</th>\n",
       "      <td>0.009755</td>\n",
       "      <td>0.025137</td>\n",
       "      <td>0.015574</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506674</th>\n",
       "      <td>0.046916</td>\n",
       "      <td>0.011020</td>\n",
       "      <td>0.013006</td>\n",
       "      <td>0.008268</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506675</th>\n",
       "      <td>0.056422</td>\n",
       "      <td>0.029514</td>\n",
       "      <td>0.030608</td>\n",
       "      <td>0.029484</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506676</th>\n",
       "      <td>0.066646</td>\n",
       "      <td>0.032277</td>\n",
       "      <td>0.040052</td>\n",
       "      <td>0.021848</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506677</th>\n",
       "      <td>0.017974</td>\n",
       "      <td>0.012306</td>\n",
       "      <td>0.031395</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506678</th>\n",
       "      <td>0.004898</td>\n",
       "      <td>0.002802</td>\n",
       "      <td>0.003869</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506679</th>\n",
       "      <td>0.115901</td>\n",
       "      <td>0.095161</td>\n",
       "      <td>0.117876</td>\n",
       "      <td>0.055770</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506680</th>\n",
       "      <td>0.050582</td>\n",
       "      <td>0.026672</td>\n",
       "      <td>0.035620</td>\n",
       "      <td>0.036187</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506681</th>\n",
       "      <td>0.047381</td>\n",
       "      <td>0.017212</td>\n",
       "      <td>0.018374</td>\n",
       "      <td>0.020970</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506682</th>\n",
       "      <td>0.031088</td>\n",
       "      <td>0.105144</td>\n",
       "      <td>0.083989</td>\n",
       "      <td>0.090571</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506683</th>\n",
       "      <td>0.001288</td>\n",
       "      <td>0.004414</td>\n",
       "      <td>0.004921</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506684</th>\n",
       "      <td>0.029652</td>\n",
       "      <td>0.026957</td>\n",
       "      <td>0.030604</td>\n",
       "      <td>0.006950</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506685</th>\n",
       "      <td>0.007014</td>\n",
       "      <td>0.009893</td>\n",
       "      <td>0.007705</td>\n",
       "      <td>0.005638</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506686</th>\n",
       "      <td>0.042789</td>\n",
       "      <td>0.020516</td>\n",
       "      <td>0.021018</td>\n",
       "      <td>0.014690</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506687</th>\n",
       "      <td>0.068706</td>\n",
       "      <td>0.034536</td>\n",
       "      <td>0.037966</td>\n",
       "      <td>0.081585</td>\n",
       "      <td>0.064210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506688</th>\n",
       "      <td>0.010919</td>\n",
       "      <td>0.009193</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506689</th>\n",
       "      <td>0.020193</td>\n",
       "      <td>0.016995</td>\n",
       "      <td>0.015126</td>\n",
       "      <td>0.018594</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506690</th>\n",
       "      <td>0.049331</td>\n",
       "      <td>0.013867</td>\n",
       "      <td>0.016733</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506691 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LogisticRegression   XGBoost  LightGBM  CatBoost       ANN\n",
       "0                 0.002416  0.005918  0.005289  0.004550  0.064205\n",
       "1                 0.021395  0.010993  0.014443  0.027886  0.064205\n",
       "2                 0.041185  0.022076  0.024321  0.033240  0.064205\n",
       "3                 0.009547  0.010852  0.008990  0.004910  0.064205\n",
       "4                 0.005110  0.008084  0.010443  0.005224  0.064205\n",
       "...                    ...       ...       ...       ...       ...\n",
       "506686            0.042789  0.020516  0.021018  0.014690  0.064205\n",
       "506687            0.068706  0.034536  0.037966  0.081585  0.064210\n",
       "506688            0.010919  0.009193  0.010288  0.006661  0.064205\n",
       "506689            0.020193  0.016995  0.015126  0.018594  0.064205\n",
       "506690            0.049331  0.013867  0.016733  0.003822  0.064205\n",
       "\n",
       "[506691 rows x 5 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.iloc[:,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN results are just for reference. They look suspicious so don't use them for now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=np.array([0.25,0.25,0.25,0.25,0.])\n",
    "pred_results['final']=np.sum(pred_results.iloc[:,:5].values * weights,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>LightGBM</th>\n",
       "      <th>CatBoost</th>\n",
       "      <th>ANN</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.637372</td>\n",
       "      <td>0.909110</td>\n",
       "      <td>0.915266</td>\n",
       "      <td>0.953025</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.853693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0.700002</td>\n",
       "      <td>0.866051</td>\n",
       "      <td>0.885622</td>\n",
       "      <td>0.906103</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.839445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0.882422</td>\n",
       "      <td>0.953324</td>\n",
       "      <td>0.949841</td>\n",
       "      <td>0.978518</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.941026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2940</th>\n",
       "      <td>0.892658</td>\n",
       "      <td>0.964252</td>\n",
       "      <td>0.826368</td>\n",
       "      <td>0.996162</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>0.919860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>0.926249</td>\n",
       "      <td>0.963710</td>\n",
       "      <td>0.884104</td>\n",
       "      <td>0.973096</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>0.936790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>0.935376</td>\n",
       "      <td>0.949243</td>\n",
       "      <td>0.880483</td>\n",
       "      <td>0.926641</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>0.922936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>0.954374</td>\n",
       "      <td>0.899294</td>\n",
       "      <td>0.863213</td>\n",
       "      <td>0.929913</td>\n",
       "      <td>0.064211</td>\n",
       "      <td>0.911699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063</th>\n",
       "      <td>0.540152</td>\n",
       "      <td>0.946795</td>\n",
       "      <td>0.860665</td>\n",
       "      <td>0.901152</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.812191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4067</th>\n",
       "      <td>0.543782</td>\n",
       "      <td>0.935166</td>\n",
       "      <td>0.827108</td>\n",
       "      <td>0.961252</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.816827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>0.636398</td>\n",
       "      <td>0.964908</td>\n",
       "      <td>0.900648</td>\n",
       "      <td>0.966869</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.867206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>0.790517</td>\n",
       "      <td>0.975680</td>\n",
       "      <td>0.938700</td>\n",
       "      <td>0.985410</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.922577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4079</th>\n",
       "      <td>0.870097</td>\n",
       "      <td>0.974726</td>\n",
       "      <td>0.925883</td>\n",
       "      <td>0.990201</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.940227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>0.885458</td>\n",
       "      <td>0.952386</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.972466</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.930744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4089</th>\n",
       "      <td>0.899366</td>\n",
       "      <td>0.949739</td>\n",
       "      <td>0.879439</td>\n",
       "      <td>0.974681</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.925806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4090</th>\n",
       "      <td>0.933357</td>\n",
       "      <td>0.972497</td>\n",
       "      <td>0.887026</td>\n",
       "      <td>0.983382</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.944065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4411</th>\n",
       "      <td>0.669425</td>\n",
       "      <td>0.913265</td>\n",
       "      <td>0.912661</td>\n",
       "      <td>0.930319</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.856418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4421</th>\n",
       "      <td>0.651541</td>\n",
       "      <td>0.923349</td>\n",
       "      <td>0.903510</td>\n",
       "      <td>0.885265</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.840916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430</th>\n",
       "      <td>0.766077</td>\n",
       "      <td>0.938073</td>\n",
       "      <td>0.912571</td>\n",
       "      <td>0.909916</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.881660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4440</th>\n",
       "      <td>0.807327</td>\n",
       "      <td>0.922159</td>\n",
       "      <td>0.906535</td>\n",
       "      <td>0.921940</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.889490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4765</th>\n",
       "      <td>0.800041</td>\n",
       "      <td>0.959183</td>\n",
       "      <td>0.922711</td>\n",
       "      <td>0.992307</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.918560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4769</th>\n",
       "      <td>0.854730</td>\n",
       "      <td>0.968128</td>\n",
       "      <td>0.928136</td>\n",
       "      <td>0.990460</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.935363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5199</th>\n",
       "      <td>0.605388</td>\n",
       "      <td>0.942030</td>\n",
       "      <td>0.872465</td>\n",
       "      <td>0.984783</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.851166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7124</th>\n",
       "      <td>0.906669</td>\n",
       "      <td>0.881742</td>\n",
       "      <td>0.865955</td>\n",
       "      <td>0.957546</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.902978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7130</th>\n",
       "      <td>0.730471</td>\n",
       "      <td>0.887618</td>\n",
       "      <td>0.840266</td>\n",
       "      <td>0.925253</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.845902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7134</th>\n",
       "      <td>0.866813</td>\n",
       "      <td>0.908095</td>\n",
       "      <td>0.857768</td>\n",
       "      <td>0.941231</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.893477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8936</th>\n",
       "      <td>0.887256</td>\n",
       "      <td>0.947675</td>\n",
       "      <td>0.871875</td>\n",
       "      <td>0.996098</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.925726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11374</th>\n",
       "      <td>0.520275</td>\n",
       "      <td>0.923972</td>\n",
       "      <td>0.893312</td>\n",
       "      <td>0.958279</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.823960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11626</th>\n",
       "      <td>0.843211</td>\n",
       "      <td>0.903845</td>\n",
       "      <td>0.761980</td>\n",
       "      <td>0.803072</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.828027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11629</th>\n",
       "      <td>0.891903</td>\n",
       "      <td>0.882218</td>\n",
       "      <td>0.788288</td>\n",
       "      <td>0.776088</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.834624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11649</th>\n",
       "      <td>0.877046</td>\n",
       "      <td>0.785561</td>\n",
       "      <td>0.725474</td>\n",
       "      <td>0.935490</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.830893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462416</th>\n",
       "      <td>0.638993</td>\n",
       "      <td>0.918460</td>\n",
       "      <td>0.864387</td>\n",
       "      <td>0.951845</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.843421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462417</th>\n",
       "      <td>0.650897</td>\n",
       "      <td>0.887341</td>\n",
       "      <td>0.888882</td>\n",
       "      <td>0.903794</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.832729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470461</th>\n",
       "      <td>0.517267</td>\n",
       "      <td>0.868923</td>\n",
       "      <td>0.905007</td>\n",
       "      <td>0.958542</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.812435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470659</th>\n",
       "      <td>0.527321</td>\n",
       "      <td>0.972437</td>\n",
       "      <td>0.926159</td>\n",
       "      <td>0.961455</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.846843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474565</th>\n",
       "      <td>0.516660</td>\n",
       "      <td>0.946319</td>\n",
       "      <td>0.882966</td>\n",
       "      <td>0.990086</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.834008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475457</th>\n",
       "      <td>0.587478</td>\n",
       "      <td>0.918828</td>\n",
       "      <td>0.892741</td>\n",
       "      <td>0.961662</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.840177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477317</th>\n",
       "      <td>0.557856</td>\n",
       "      <td>0.905902</td>\n",
       "      <td>0.810095</td>\n",
       "      <td>0.986517</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.815092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477327</th>\n",
       "      <td>0.835347</td>\n",
       "      <td>0.972921</td>\n",
       "      <td>0.901768</td>\n",
       "      <td>0.890677</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.900178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478473</th>\n",
       "      <td>0.503275</td>\n",
       "      <td>0.922224</td>\n",
       "      <td>0.905229</td>\n",
       "      <td>0.995015</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.831436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480020</th>\n",
       "      <td>0.735667</td>\n",
       "      <td>0.911920</td>\n",
       "      <td>0.865847</td>\n",
       "      <td>0.951494</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.866232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482545</th>\n",
       "      <td>0.913691</td>\n",
       "      <td>0.816058</td>\n",
       "      <td>0.744846</td>\n",
       "      <td>0.820593</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.823797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484056</th>\n",
       "      <td>0.554297</td>\n",
       "      <td>0.900022</td>\n",
       "      <td>0.820088</td>\n",
       "      <td>0.936376</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.802696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485051</th>\n",
       "      <td>0.713962</td>\n",
       "      <td>0.963494</td>\n",
       "      <td>0.953918</td>\n",
       "      <td>0.992666</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.906010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485061</th>\n",
       "      <td>0.673755</td>\n",
       "      <td>0.960188</td>\n",
       "      <td>0.949697</td>\n",
       "      <td>0.981598</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.891309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485101</th>\n",
       "      <td>0.751014</td>\n",
       "      <td>0.948558</td>\n",
       "      <td>0.909294</td>\n",
       "      <td>0.990534</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.899850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485257</th>\n",
       "      <td>0.838382</td>\n",
       "      <td>0.899781</td>\n",
       "      <td>0.910655</td>\n",
       "      <td>0.974976</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.905948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485297</th>\n",
       "      <td>0.902051</td>\n",
       "      <td>0.913167</td>\n",
       "      <td>0.909129</td>\n",
       "      <td>0.976106</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.925113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490375</th>\n",
       "      <td>0.882168</td>\n",
       "      <td>0.891627</td>\n",
       "      <td>0.908212</td>\n",
       "      <td>0.992219</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.918557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491296</th>\n",
       "      <td>0.544228</td>\n",
       "      <td>0.936610</td>\n",
       "      <td>0.870779</td>\n",
       "      <td>0.913272</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.816222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491678</th>\n",
       "      <td>0.868164</td>\n",
       "      <td>0.886269</td>\n",
       "      <td>0.867004</td>\n",
       "      <td>0.916663</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.884525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492086</th>\n",
       "      <td>0.662925</td>\n",
       "      <td>0.963499</td>\n",
       "      <td>0.949611</td>\n",
       "      <td>0.977260</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.888324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492130</th>\n",
       "      <td>0.362077</td>\n",
       "      <td>0.934252</td>\n",
       "      <td>0.927862</td>\n",
       "      <td>0.990276</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.803617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498466</th>\n",
       "      <td>0.921588</td>\n",
       "      <td>0.847202</td>\n",
       "      <td>0.818769</td>\n",
       "      <td>0.619335</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.801724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502030</th>\n",
       "      <td>0.911440</td>\n",
       "      <td>0.892204</td>\n",
       "      <td>0.723175</td>\n",
       "      <td>0.905479</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.858074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502739</th>\n",
       "      <td>0.614635</td>\n",
       "      <td>0.928879</td>\n",
       "      <td>0.869378</td>\n",
       "      <td>0.984658</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.849387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504960</th>\n",
       "      <td>0.871722</td>\n",
       "      <td>0.929160</td>\n",
       "      <td>0.862246</td>\n",
       "      <td>0.976737</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.909966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505697</th>\n",
       "      <td>0.700946</td>\n",
       "      <td>0.971936</td>\n",
       "      <td>0.938287</td>\n",
       "      <td>0.988992</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.900040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505698</th>\n",
       "      <td>0.893291</td>\n",
       "      <td>0.957156</td>\n",
       "      <td>0.915471</td>\n",
       "      <td>0.998620</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.941135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506467</th>\n",
       "      <td>0.709452</td>\n",
       "      <td>0.930126</td>\n",
       "      <td>0.938623</td>\n",
       "      <td>0.986203</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.891101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506472</th>\n",
       "      <td>0.774221</td>\n",
       "      <td>0.922360</td>\n",
       "      <td>0.907711</td>\n",
       "      <td>0.994107</td>\n",
       "      <td>0.064205</td>\n",
       "      <td>0.899600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1978 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LogisticRegression   XGBoost  LightGBM  CatBoost       ANN     final\n",
       "278               0.637372  0.909110  0.915266  0.953025  0.064205  0.853693\n",
       "279               0.700002  0.866051  0.885622  0.906103  0.064205  0.839445\n",
       "287               0.882422  0.953324  0.949841  0.978518  0.064205  0.941026\n",
       "2940              0.892658  0.964252  0.826368  0.996162  0.064209  0.919860\n",
       "2941              0.926249  0.963710  0.884104  0.973096  0.064209  0.936790\n",
       "...                    ...       ...       ...       ...       ...       ...\n",
       "504960            0.871722  0.929160  0.862246  0.976737  0.064205  0.909966\n",
       "505697            0.700946  0.971936  0.938287  0.988992  0.064205  0.900040\n",
       "505698            0.893291  0.957156  0.915471  0.998620  0.064205  0.941135\n",
       "506467            0.709452  0.930126  0.938623  0.986203  0.064205  0.891101\n",
       "506472            0.774221  0.922360  0.907711  0.994107  0.064205  0.899600\n",
       "\n",
       "[1978 rows x 6 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.loc[pred_results.final>0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3663549</td>\n",
       "      <td>0.016476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3663550</td>\n",
       "      <td>0.027784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3663551</td>\n",
       "      <td>0.037006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3663552</td>\n",
       "      <td>0.019701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3663553</td>\n",
       "      <td>0.018613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID   isFraud\n",
       "0        3663549  0.016476\n",
       "1        3663550  0.027784\n",
       "2        3663551  0.037006\n",
       "3        3663552  0.019701\n",
       "4        3663553  0.018613"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv')\n",
    "submission['isFraud'] = pred_results['final']\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_mark_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
